"""
FastAPI Server for SGK Scraper
"""
from fastapi import FastAPI, HTTPException, UploadFile, File, Form, BackgroundTasks
from fastapi.responses import JSONResponse
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel, Field
from typing import Dict, Any, Optional, List, Tuple
import uvicorn
from scrapers.kaysis_scraper import (
    scrape_kaysis_mevzuat,
    print_results_to_console,
    get_uploaded_documents,
    get_proxy_from_db,
    turkish_sentence_case,
    is_title_similar
)
import threading
import re
import os
from pathlib import Path
import json
import subprocess
import platform
from datetime import datetime

# curl_cffi import kontrol√º
try:
    from curl_cffi import requests
    from curl_cffi.requests import CurlMime
    CURL_CFFI_AVAILABLE = True
except ImportError:
    import requests
    CURL_CFFI_AVAILABLE = False
    CurlMime = None

from pdf_processor import PDFProcessor
from deepseek_analyzer import DeepSeekAnalyzer
from utils import download_pdf_from_url, create_output_directories, create_pdf_filename, validate_pdf_file
from datetime import datetime
from pymongo import MongoClient
from pymongo.errors import ConnectionFailure, PyMongoError
from bson import ObjectId
import urllib.parse
import unicodedata
import shutil
from dotenv import load_dotenv
from bs4 import BeautifulSoup
from urllib.parse import urlparse, urljoin

# .env dosyasƒ±nƒ± y√ºkle
load_dotenv()

# Stdout'u line-buffered yap (anlƒ±k log g√∂r√ºn√ºm√º i√ßin)
import sys
if sys.stdout.isatty():
    # Terminal'de √ßalƒ±≈üƒ±yorsa line buffering
    sys.stdout.reconfigure(line_buffering=True)
else:
    # Systemd/journalctl i√ßin unbuffered
    import os
    os.environ['PYTHONUNBUFFERED'] = '1'
    # sys.stdout'u flush etmek i√ßin wrapper
    class Unbuffered:
        def __init__(self, stream):
            self.stream = stream
        def write(self, data):
            self.stream.write(data)
            self.stream.flush()
        def __getattr__(self, attr):
            return getattr(self.stream, attr)
    sys.stdout = Unbuffered(sys.stdout)
    sys.stderr = Unbuffered(sys.stderr)

# Swagger/OpenAPI kategorileri
openapi_tags = [
    {
        "name": "SGK Scraper",
        "description": "SGK mevzuatlarƒ±nƒ± tarama, analiz ve y√ºkleme i≈ülemleri."
    },
    {
        "name": "e-Devlet Scraper",
        "description": "T√ºrkiye.gov.tr hizmet linklerini toplama ve kaydetme."
    },
    {
        "name": "Links",
        "description": "e-Devlet linkleri i√ßin listeleme, olu≈üturma, g√ºncelleme ve silme i≈ülemleri."
    },
    {
        "name": "Kurumlar",
        "description": "Kurum kayƒ±tlarƒ± i√ßin CRUD ve logo y√ºkleme i≈ülemleri."
    },
    {
        "name": "Kurum Duyuru",
        "description": "Kurum duyurularƒ± i√ßin CRUD i≈ülemleri."
    },
    {
        "name": "MongoDB",
        "description": "Metadata ve Content koleksiyonlarƒ± i√ßin y√∂netim endpointleri."
    },
    {
        "name": "Proxy",
        "description": "Proxy ayarlarƒ± i√ßin CRUD i≈ülemleri."
    },
    {
        "name": "Health",
        "description": "Servis saƒülƒ±k kontrol√º."
    }
]

app = FastAPI(
    title="SGK Scraper API",
    version="1.0.0",
    description="SGK ve e-Devlet entegrasyonlarƒ± i√ßin REST API",
    redoc_url=None,
    openapi_tags=openapi_tags
)

# CORS middleware ekle - T√ºm origin'lere izin ver
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # T√ºm origin'lere izin ver
    allow_credentials=False,  # allow_origins=["*"] ile birlikte True olamaz
    allow_methods=["GET", "POST", "PUT", "DELETE", "OPTIONS", "PATCH"],  # T√ºm HTTP metodlarƒ±na izin ver
    allow_headers=["*"],  # T√ºm header'lara izin ver
    expose_headers=["*"],  # T√ºm header'larƒ± expose et
    max_age=3600,  # Preflight cache s√ºresi (1 saat)
)

# Son tarama sonu√ßlarƒ±ndan id -> item e≈ülemesini tutmak i√ßin √∂nbellek
# { id: { "section_title": str, "baslik": str, "link": str } }
last_item_map: Dict[int, Dict[str, Any]] = {}


def _load_config() -> Optional[Dict[str, Any]]:
    """Config dosyasƒ±nƒ± y√ºkler"""
    try:
        with open('config.json', 'r', encoding='utf-8') as f:
            return json.load(f)
    except Exception:
        return None


def _get_mongo_collections():
    """MongoDB client ve ilgili koleksiyonlarƒ± d√∂ner (metadata, content)."""
    client = _get_mongodb_client()
    if not client:
        return None, None, None
    database_name = os.getenv("MONGODB_DATABASE", "mevzuatgpt")
    metadata_collection_name = os.getenv("MONGODB_METADATA_COLLECTION", "metadata")
    content_collection_name = os.getenv("MONGODB_CONTENT_COLLECTION", "content")
    db = client[database_name]
    return client, db[metadata_collection_name], db[content_collection_name]


def normalize_for_exact_match(s: str) -> str:
    """Tam e≈üle≈üme i√ßin metni normalize eder (T√ºrk√ße karakter ve bo≈üluk desteƒüi)"""
    if not s:
        return ""
    import unicodedata
    # Unicode normalizasyonu
    s = unicodedata.normalize('NFC', s)
    s = s.replace("i\u0307", "i")
    # T√ºrk√ße k√º√ß√ºk harfe √ßevirme
    s = s.replace('I', 'ƒ±').replace('ƒ∞', 'i').lower()
    # Fazla bo≈üluklarƒ± temizle ve trim et
    s = re.sub(r'\s+', ' ', s.strip())
    return s


def to_title(s: str) -> str:
    """T√ºrk√ße karakterleri dikkate alarak Title Case'e √ßevirir"""
    if not s:
        return ""
    import unicodedata
    # Unicode normalizasyonu
    s = unicodedata.normalize('NFC', s)
    s = s.replace("i\u0307", "i")
    # T√ºrk√ße k√º√ß√ºk harfe √ßevirme
    tmp = s.replace('I', 'ƒ±').replace('ƒ∞', 'i').lower()
    # Kelime kelime ba≈ü harf b√ºy√ºt
    words = re.split(r'(\s+)', tmp)
    titled_parts = []
    for w in words:
        if not w or w.isspace():
            titled_parts.append(w)
            continue
        first = w[0]
        rest = w[1:]
        if first == 'i':
            first_up = 'ƒ∞'
        elif first == 'ƒ±':
            first_up = 'I'
        else:
            first_up = first.upper()
        titled_parts.append(first_up + rest)
    return ''.join(titled_parts)


class ScrapeResponse(BaseModel):
    success: bool
    message: str
    data: Dict[str, Any] = {}


class PortalScanRequest(BaseModel):
    id: str = Field(..., description="Kurum MongoDB ObjectId")
    detsis: str = Field(..., description="DETSIS numarasƒ± (KAYSƒ∞S kurum ID'si)")
    type: str = Field(default="kaysis", description="Scraper tipi (varsayƒ±lan: kaysis)")

    model_config = {
        "json_schema_extra": {
            "example": {
                "id": "68bbf6df8ef4e8023c19641d",
                "detsis": "60521689",
                "type": "kaysis"
            }
        }
    }


class PortalScanWithDataRequest(BaseModel):
    id: Optional[str] = Field(default=None, description="Kurum MongoDB ObjectId (opsiyonel, kurum_id ile birlikte kullanƒ±labilir)")
    kurum_id: Optional[str] = Field(default=None, description="Kurum MongoDB ObjectId (opsiyonel, id ile birlikte kullanƒ±labilir)")
    detsis: Optional[str] = Field(default=None, description="DETSIS numarasƒ± (opsiyonel, MongoDB'den alƒ±nƒ±r)")
    type: str = Field(default="kaysis", description="Scraper tipi (varsayƒ±lan: kaysis)")
    sections: List[Dict[str, Any]] = Field(..., description="√ñnceden taranmƒ±≈ü mevzuat verileri (zorunlu, scraper √ßalƒ±≈ütƒ±rƒ±lmaz)")
    stats: Optional[Dict[str, Any]] = Field(default=None, description="√ñnceden taranmƒ±≈ü istatistikler (opsiyonel)")

    def __init__(self, **data):
        # Eƒüer 'data' wrapper'ƒ± varsa (generate-json response formatƒ±), i√ßindeki deƒüerleri √ßƒ±kar
        if 'data' in data and isinstance(data['data'], dict):
            data_wrapper = data.pop('data')
            # data i√ßindeki deƒüerleri ana seviyeye ta≈üƒ±
            for key, value in data_wrapper.items():
                if key not in data:  # Sadece yoksa ekle, varsa √ºzerine yazma
                    data[key] = value
        
        # id veya kurum_id'den birini normalize et
        if 'kurum_id' in data and 'id' not in data:
            data['id'] = data.pop('kurum_id')
        elif 'kurum_id' in data and 'id' in data:
            # ƒ∞kisi de varsa id'yi kullan, kurum_id'yi kaldƒ±r
            data.pop('kurum_id', None)
        super().__init__(**data)

    model_config = {
        "json_schema_extra": {
            "example": {
                "kurum_id": "68bbf6df8ef4e8023c19641d",
                "detsis": "60521689",
                "type": "kaysis",
                "sections": [
                    {
                        "section_title": "Kanunlar",
                        "items": [
                            {
                                "baslik": "√ñrnek Kanun",
                                "link": "https://kms.kaysis.gov.tr/Home/Goster/123"
                            }
                        ]
                    }
                ],
                "stats": {
                    "total_sections": 1,
                    "total_items": 1
                }
            }
        }
    }


class GenerateJsonRequest(BaseModel):
    id: Optional[str] = Field(default=None, description="Kurum MongoDB ObjectId (opsiyonel, kurum_id ile birlikte kullanƒ±labilir)")
    kurum_id: Optional[str] = Field(default=None, description="Kurum MongoDB ObjectId (opsiyonel, id ile birlikte kullanƒ±labilir)")
    type: str = Field(default="kaysis", description="Scraper tipi (varsayƒ±lan: kaysis)")

    def __init__(self, **data):
        # id veya kurum_id'den birini normalize et
        if 'kurum_id' in data and 'id' not in data:
            data['id'] = data.pop('kurum_id')
        elif 'kurum_id' in data and 'id' in data:
            # ƒ∞kisi de varsa id'yi kullan, kurum_id'yi kaldƒ±r
            data.pop('kurum_id', None)
        super().__init__(**data)

    model_config = {
        "json_schema_extra": {
            "example": {
                "kurum_id": "68bbf6df8ef4e8023c19641d",
                "type": "kaysis"
            }
        }
    }


class ProcessRequest(BaseModel):
    kurum_id: str = Field(..., description="Kurum MongoDB ObjectId")
    detsis: str = Field(..., description="DETSIS numarasƒ± (KAYSƒ∞S kurum ID'si)")
    type: str = Field(default="kaysis", description="Scraper tipi (varsayƒ±lan: kaysis)")
    link: str = Field(..., description="PDF indirme linki")
    mode: str = Field(default="t", description="ƒ∞≈ülem modu: 'm' (MevzuatGPT), 'p' (Portal), 't' (Tamamƒ±)")
    category: Optional[str] = Field(default=None, description="Belge kategorisi (opsiyonel)")
    document_name: Optional[str] = Field(default=None, description="Belge adƒ± (opsiyonel)")
    use_ocr: bool = Field(default=False, description="OCR kullanƒ±mƒ±: True ise t√ºm sayfalar OCR ile i≈ülenir, False ise OCR kullanƒ±lmaz (varsayƒ±lan: False)")

    model_config = {
        "json_schema_extra": {
            "example": {
                "kurum_id": "68bbf6df8ef4e8023c19641d",
                "detsis": "60521689",
                "type": "kaysis",
                "link": "https://kms.kaysis.gov.tr/Home/Goster/104890",
                "mode": "t",
                "category": "Kanunlar",
                "document_name": "T√ºrkiye cumhuriyeti h√ºk√ºmeti ile tunus cumhuriyeti h√ºk√ºmeti arasƒ±nda sosyal g√ºvenlik anla≈ümasƒ±nƒ±n onaylanmasƒ±nƒ±n uygun bulunduƒüuna dair kanun",
                "use_ocr": True
            }
        }
    }


class ProcessData(BaseModel):
    category: str
    institution: str
    document_name: str
    output_dir: Optional[str] = None
    sections_count: int
    upload_response: Optional[Dict[str, Any]] = None


class ProcessResponse(BaseModel):
    success: bool
    message: str
    data: Optional[ProcessData] = None


@app.get("/", tags=["Health"], summary="API k√∂k")
async def root():
    """API root endpoint"""
    return {
        "message": "SGK Scraper API",
        "version": "1.0.0",
        "endpoints": {
            "POST /api/mevzuatgpt/scrape": "Kurum mevzuatlarƒ±nƒ± tarar ve konsola yazdƒ±rƒ±r",
            "POST /api/mevzuatgpt/scrape-with-data": "Kurum mevzuatlarƒ±nƒ± tarar veya g√∂nderilen JSON verilerini kullanƒ±r",
            "POST /api/mevzuatgpt/generate-json": "Sadece tarama yapar ve JSON olu≈üturur (kar≈üƒ±la≈ütƒ±rma yapmaz)"
        }
    }


@app.post("/api/mevzuatgpt/scrape", response_model=ScrapeResponse, tags=["SGK Scraper"], summary="Kurum mevzuat tarama")
async def scrape_mevzuatgpt(req: PortalScanRequest):
    """
    Belirtilen kurumun mevzuatlarƒ±nƒ± tarar ve sonu√ßlarƒ± konsola yazdƒ±rƒ±r.
    type parametresi ile scraper tipi belirlenir (≈üu an i√ßin sadece 'kaysis' desteklenir).
    """
    try:
        print("\n" + "="*80)
        print(f"üöÄ API Endpoint'ten Kurum Mevzuat Tarama ƒ∞steƒüi Alƒ±ndƒ± (Kurum ID: {req.id}, Type: {req.type})")
        print("="*80)
        
        # Type kontrol√º
        if req.type.lower() != "kaysis":
            return ScrapeResponse(
                success=False,
                message=f"Desteklenmeyen scraper tipi: {req.type}. ≈ûu an i√ßin sadece 'kaysis' desteklenmektedir.",
                data={"error": "UNSUPPORTED_TYPE", "type": req.type}
            )
        
        # MongoDB'den kurum bilgisini √ßek
        kurum_adi = None
        try:
            client = _get_mongodb_client()
            if client:
                database_name = os.getenv("MONGODB_DATABASE", "mevzuatgpt")
                db = client[database_name]
                kurumlar_collection = db["kurumlar"]
                from bson import ObjectId
                kurum_doc = kurumlar_collection.find_one({"_id": ObjectId(req.id)})
                if kurum_doc:
                    kurum_adi = kurum_doc.get("kurum_adi", "Bilinmeyen Kurum")
                client.close()
        except Exception as e:
            print(f"‚ö†Ô∏è MongoDB'den kurum bilgisi alƒ±namadƒ±: {str(e)}")
            kurum_adi = "Bilinmeyen Kurum"
        
        print(f"üìã Kurum: {kurum_adi}")
        print(f"üî¢ DETSIS: {req.detsis}")
        
        # √ñnce API'den y√ºkl√º documents'larƒ± √ßek (√ßerez kullanmadan, direkt API)
        uploaded_docs = []
        # MongoDB'den portal'da bulunan pdf_adi'larƒ± √ßek
        portal_docs = []
        cfg = _load_config()
        if cfg:
            token = _login_with_config(cfg)
            if token:
                api_base_url = cfg.get("api_base_url")
                print(f"üì° API'den y√ºkl√º documents √ßekiliyor...")
                try:
                    uploaded_docs = get_uploaded_documents(api_base_url, token, use_streamlit=False)
                    print(f"‚úÖ {len(uploaded_docs)} document bulundu")
                    # Debug: ƒ∞lk birka√ß belgenin t√ºm alanlarƒ±nƒ± yazdƒ±r
                    if uploaded_docs:
                        print(f"üîç DEBUG - ƒ∞lk 3 belgenin t√ºm alanlarƒ±:")
                        for i, doc in enumerate(uploaded_docs[:3]):
                            print(f"   Belge {i+1}: {doc}")
                        # T√ºm olasƒ± alan isimlerini kontrol et
                        all_fields = set()
                        for doc in uploaded_docs[:10]:
                            all_fields.update(doc.keys())
                        print(f"üîç DEBUG - Belgelerde bulunan alan isimleri: {sorted(all_fields)}")
                except Exception as e:
                    print(f"‚ö†Ô∏è Documents √ßekme hatasƒ±: {str(e)}")
                    import traceback
                    traceback.print_exc()

        # MongoDB metadata.pdf_adi -> portal_docs
        try:
            client = _get_mongodb_client()
            if client:
                database_name = os.getenv("MONGODB_DATABASE", "mevzuatgpt")
                metadata_collection_name = os.getenv("MONGODB_METADATA_COLLECTION", "metadata")
                db = client[database_name]
                metadata_collection = db[metadata_collection_name]
                # Sadece pdf_adi alanƒ±nƒ± al
                cursor = metadata_collection.find({}, {"pdf_adi": 1})
                count = 0
                for doc in cursor:
                    val = (doc.get("pdf_adi") or "").strip()
                    if val:
                        portal_docs.append({"pdf_adi": val})
                        count += 1
                client.close()
                print(f"‚úÖ MongoDB'den {count} pdf_adi okundu (portal kar≈üƒ±la≈ütƒ±rmasƒ± i√ßin)")
        except Exception as e:
            print(f"‚ö†Ô∏è MongoDB portal listesi okunamadƒ±: {str(e)}")
        
        # KAYSƒ∞S scraper'ƒ± kullan
        if req.type.lower() == "kaysis":
            all_sections, stats = scrape_kaysis_mevzuat(detsis=req.detsis)
            print_results_to_console(all_sections, stats)
        
        # Response hazƒ±rla (benzersiz item id'leri, uploaded durumu ve b√∂l√ºm ba≈ülƒ±k temizleme)
        item_id_counter = 1
        response_sections = []
        # √ñnbelleƒüi sƒ±fƒ±rla
        global last_item_map
        last_item_map = {}
        for section in all_sections:
            raw_title = section['section_title']
            # Sonunda kalan sayƒ±larƒ± temizle (√∂rn: "Kanunlar4" -> "Kanunlar")
            clean_title = re.sub(r"\d+\s*$", "", raw_title).strip()
            items = section.get('items', [])
            items_with_ids = []
            for item in items:
                # Y√ºkleme durumunu belirle - tam e≈üle≈üme (normalize edilmi≈ü)
                item_baslik = item.get('baslik', '')
                item_normalized = normalize_for_exact_match(item_baslik)
                is_uploaded = False
                
                # API'den gelen belgelerle kar≈üƒ±la≈ütƒ±r (tam e≈üle≈üme)
                for doc in uploaded_docs:
                    belge_adi = doc.get("belge_adi", "")
                    if belge_adi:
                        belge_normalized = normalize_for_exact_match(belge_adi)
                        if item_normalized == belge_normalized:
                            is_uploaded = True
                            break
                
                # Portal (MongoDB metadata.pdf_adi kar≈üƒ±la≈ütƒ±rmasƒ±) - tam e≈üle≈üme
                is_in_portal = False
                for doc in portal_docs:
                    pdf_adi = doc.get("pdf_adi", "")
                    if pdf_adi:
                        pdf_normalized = normalize_for_exact_match(pdf_adi)
                        if item_normalized == pdf_normalized:
                            is_in_portal = True
                            break
                
                # Benzersiz id ver ve √∂nbelleƒüe yaz
                item_payload = {
                    "id": item_id_counter,
                    "mevzuatgpt": is_uploaded,
                    "portal": is_in_portal,
                    "baslik": item.get('baslik', ''),
                    "link": item.get('link', '')
                }
                items_with_ids.append(item_payload)

                # √ñnbelleƒüe kategori bilgisini de ekleyerek koy
                last_item_map[item_id_counter] = {
                    "section_title": clean_title,
                    "baslik": item_payload["baslik"],
                    "link": item_payload["link"]
                }
                item_id_counter += 1
            response_sections.append({
                "section_title": clean_title,
                "items_count": len(items_with_ids),
                "items": items_with_ids
            })
        
        # sections_stats'ƒ± is_title_similar ile yeniden hesapla
        sections_stats_clean = []
        for section in all_sections:
            raw_title = section['section_title']
            clean_title = re.sub(r"\d+\s*$", "", raw_title).strip()
            items = section.get('items', [])
            
            uploaded_count = 0
            not_uploaded_count = 0
            
            for item in items:
                item_baslik = item.get('baslik', '')
                item_normalized = normalize_for_exact_match(item_baslik)
                is_uploaded = False
                
                # API'den gelen belgelerle kar≈üƒ±la≈ütƒ±r (tam e≈üle≈üme)
                for doc in uploaded_docs:
                    belge_adi = doc.get("belge_adi", "")
                    if belge_adi:
                        belge_normalized = normalize_for_exact_match(belge_adi)
                        if item_normalized == belge_normalized:
                            is_uploaded = True
                            break
                
                if is_uploaded:
                    uploaded_count += 1
                else:
                    not_uploaded_count += 1
            
            sections_stats_clean.append({
                "section_title": clean_title,
                "total": len(items),
                "uploaded": uploaded_count,
                "not_uploaded": not_uploaded_count
            })
        
        response_data = {
            "total_sections": stats.get('total_sections', 0),
            "total_items": stats.get('total_items', 0),
            "uploaded_documents_count": stats.get('uploaded_documents_count', 0),
            "sections": response_sections,
            "sections_stats": sections_stats_clean
        }
        
        return ScrapeResponse(
            success=True,
            message=f"{kurum_adi} tarama i≈ülemi ba≈üarƒ±yla tamamlandƒ±. Sonu√ßlar konsola yazdƒ±rƒ±ldƒ±.",
            data=response_data
        )
        
    except Exception as e:
        print(f"‚ùå Hata olu≈ütu: {str(e)}")
        import traceback
        traceback.print_exc()
        raise HTTPException(
            status_code=500,
            detail=f"Scraping i≈ülemi sƒ±rasƒ±nda hata olu≈ütu: {str(e)}"
        )


@app.post("/api/mevzuatgpt/scrape-with-data", response_model=ScrapeResponse, tags=["SGK Scraper"], summary="JSON veri ile kar≈üƒ±la≈ütƒ±rma ve finalize")
async def scrape_mevzuatgpt_with_data(req: PortalScanWithDataRequest):
    """
    G√∂nderilen JSON verilerini kullanarak API/Elasticsearch kar≈üƒ±la≈ütƒ±rmasƒ± yapar ve finalize eder.
    Scraper √ßalƒ±≈ütƒ±rƒ±lmaz, sadece g√∂nderilen JSON verisi ile i≈ülem yapƒ±lƒ±r.
    Adƒ±mlar: 1) Kurum bilgisi, 2) API'den belgeler, 3) MongoDB'den belgeler, 4) Kar≈üƒ±la≈ütƒ±rma, 5) Finalize
    """
    try:
        print("\n" + "="*80)
        print(f"üöÄ JSON Veri ile Kar≈üƒ±la≈ütƒ±rma ƒ∞steƒüi Alƒ±ndƒ±")
        
        # id veya kurum_id kontrol√º
        kurum_id = req.id or getattr(req, 'kurum_id', None)
        if not kurum_id:
            return ScrapeResponse(
                success=False,
                message="Kurum ID (id veya kurum_id) g√∂nderilmedi.",
                data={"error": "KURUM_ID_REQUIRED"}
            )
        
        print(f"üìã Kurum ID: {kurum_id}, Type: {req.type}")
        
        # Sections kontrol√º - zorunlu
        if not req.sections or len(req.sections) == 0:
            return ScrapeResponse(
                success=False,
                message="JSON verisi (sections) g√∂nderilmedi. Bu endpoint sadece JSON verisi ile √ßalƒ±≈üƒ±r.",
                data={"error": "NO_SECTIONS_PROVIDED"}
            )
        
        print(f"üì¶ G√∂nderilen JSON verisi kullanƒ±lacak ({len(req.sections)} b√∂l√ºm)")
        print("="*80)
        
        # Type kontrol√º
        if req.type.lower() != "kaysis":
            return ScrapeResponse(
                success=False,
                message=f"Desteklenmeyen scraper tipi: {req.type}. ≈ûu an i√ßin sadece 'kaysis' desteklenmektedir.",
                data={"error": "UNSUPPORTED_TYPE", "type": req.type}
            )
        
        # ADIM 1,2,3: MongoDB'den kurum bilgisini √ßek ve mevcut belgeleri topla
        kurum_adi = None
        detsis = req.detsis  # √ñnce request'ten al
        
        try:
            client = _get_mongodb_client()
            if client:
                database_name = os.getenv("MONGODB_DATABASE", "mevzuatgpt")
                db = client[database_name]
                kurumlar_collection = db["kurumlar"]
                from bson import ObjectId
                kurum_doc = kurumlar_collection.find_one({"_id": ObjectId(kurum_id)})
                if kurum_doc:
                    kurum_adi = kurum_doc.get("kurum_adi", "Bilinmeyen Kurum")
                    # Eƒüer detsis request'te yoksa MongoDB'den al
                    if not detsis:
                        detsis = kurum_doc.get("detsis", "")
                client.close()
        except Exception as e:
            print(f"‚ö†Ô∏è MongoDB'den kurum bilgisi alƒ±namadƒ±: {str(e)}")
            kurum_adi = "Bilinmeyen Kurum"
        
        print(f"üìã Kurum: {kurum_adi}")
        print(f"üî¢ DETSIS: {detsis or 'Belirtilmedi'}")
        
        # ADIM 2: API'den y√ºkl√º documents'larƒ± √ßek (MevzuatGPT/Supabase)
        uploaded_docs = []
        cfg = _load_config()
        if cfg:
            token = _login_with_config(cfg)
            if token:
                api_base_url = cfg.get("api_base_url")
                print(f"üì° API'den y√ºkl√º documents √ßekiliyor (MevzuatGPT/Supabase)...")
                try:
                    uploaded_docs = get_uploaded_documents(api_base_url, token, use_streamlit=False)
                    print(f"‚úÖ {len(uploaded_docs)} document bulundu (MevzuatGPT/Supabase)")
                except Exception as e:
                    print(f"‚ö†Ô∏è Documents √ßekme hatasƒ±: {str(e)}")
                    uploaded_docs = []  # Hata durumunda bo≈ü liste
            else:
                print("‚ö†Ô∏è API'ye giri≈ü yapƒ±lamadƒ±, belge kontrol√º yapƒ±lamayacak")
                uploaded_docs = []
        else:
            print("‚ö†Ô∏è Config bulunamadƒ±, API belge kontrol√º yapƒ±lamayacak")
            uploaded_docs = []

        # ADIM 3: MongoDB metadata.pdf_adi -> portal_docs
        portal_docs = []
        try:
            client = _get_mongodb_client()
            if client:
                database_name = os.getenv("MONGODB_DATABASE", "mevzuatgpt")
                metadata_collection_name = os.getenv("MONGODB_METADATA_COLLECTION", "metadata")
                db = client[database_name]
                metadata_collection = db[metadata_collection_name]
                # Sadece pdf_adi alanƒ±nƒ± al
                cursor = metadata_collection.find({}, {"pdf_adi": 1})
                count = 0
                for doc in cursor:
                    val = (doc.get("pdf_adi") or "").strip()
                    if val:
                        portal_docs.append({"pdf_adi": val})
                        count += 1
                client.close()
                print(f"‚úÖ MongoDB'den {count} pdf_adi okundu (portal kar≈üƒ±la≈ütƒ±rmasƒ± i√ßin)")
        except Exception as e:
            print(f"‚ö†Ô∏è MongoDB portal listesi okunamadƒ±: {str(e)}")
        
        # ADIM 4: G√∂nderilen JSON verisini kullan (scraper yok)
        print("üì¶ G√∂nderilen JSON verisi kullanƒ±lƒ±yor (scraper √ßalƒ±≈ütƒ±rƒ±lmƒ±yor)...")
        all_sections = req.sections
        
        # Stats'ƒ± hesapla veya g√∂nderilen stats'ƒ± kullan
        if req.stats:
            stats = req.stats
        else:
            # Stats'ƒ± hesapla
            total_items = sum(len(section.get('items', [])) for section in all_sections)
            stats = {
                'total_sections': len(all_sections),
                'total_items': total_items,
                'uploaded_documents_count': len(uploaded_docs)
            }
        print(f"‚úÖ {len(all_sections)} b√∂l√ºm, {stats.get('total_items', 0)} mevzuat JSON'dan alƒ±ndƒ±")
        
        # ADIM 5,6: Response hazƒ±rla (benzersiz item id'leri, uploaded durumu ve b√∂l√ºm ba≈ülƒ±k temizleme)
        item_id_counter = 1
        response_sections = []
        # √ñnbelleƒüi sƒ±fƒ±rla
        global last_item_map
        last_item_map = {}
        for section in all_sections:
            raw_title = section.get('section_title', '')
            # Sonunda kalan sayƒ±larƒ± temizle (√∂rn: "Kanunlar4" -> "Kanunlar")
            clean_title = re.sub(r"\d+\s*$", "", raw_title).strip()
            items = section.get('items', [])
            items_with_ids = []
            for item in items:
                # Y√ºkleme durumunu belirle - tam e≈üle≈üme (normalize edilmi≈ü)
                item_baslik = item.get('baslik', '')
                if not item_baslik:
                    # Baslik yoksa atla
                    continue
                    
                item_normalized = normalize_for_exact_match(item_baslik)
                is_uploaded = False
                matched_doc_title = None
                matched_doc_field = None
                
                # MevzuatGPT/Supabase'den gelen belgelerle kar≈üƒ±la≈ütƒ±r
                if uploaded_docs:
                    # Birden fazla alan kontrol et (API'den d√∂nen belgelerde farklƒ± alan isimleri olabilir)
                    # SADECE TAM E≈ûLE≈ûME kullan (is_title_similar √ßok gev≈üek, yanlƒ±≈ü e≈üle≈ümelere neden oluyor)
                    for doc in uploaded_docs:
                        doc_titles = [
                            ("belge_adi", doc.get("belge_adi", "")),
                            ("title", doc.get("title", "")),
                            ("document_name", doc.get("document_name", "")),
                            ("filename", doc.get("filename", "")),
                            ("name", doc.get("name", ""))
                        ]
                        
                        for field_name, doc_title in doc_titles:
                            if doc_title:
                                # Sadece tam e≈üle≈üme kontrol√º (normalize_for_exact_match ile)
                                doc_normalized = normalize_for_exact_match(doc_title)
                                if item_normalized == doc_normalized:
                                    is_uploaded = True
                                    matched_doc_title = doc_title
                                    matched_doc_field = field_name
                                    break
                        
                        if is_uploaded:
                            break
                
                # Portal (MongoDB metadata.pdf_adi kar≈üƒ±la≈ütƒ±rmasƒ±) - tam e≈üle≈üme
                is_in_portal = False
                for doc in portal_docs:
                    pdf_adi = doc.get("pdf_adi", "")
                    if pdf_adi:
                        pdf_normalized = normalize_for_exact_match(pdf_adi)
                        if item_normalized == pdf_normalized:
                            is_in_portal = True
                            break
                
                # Benzersiz id ver ve √∂nbelleƒüe yaz
                item_payload = {
                    "id": item_id_counter,
                    "mevzuatgpt": is_uploaded,
                    "portal": is_in_portal,
                    "baslik": item.get('baslik', ''),
                    "link": item.get('link', '')
                }
                items_with_ids.append(item_payload)

                # √ñnbelleƒüe kategori bilgisini de ekleyerek koy
                last_item_map[item_id_counter] = {
                    "section_title": clean_title,
                    "baslik": item_payload["baslik"],
                    "link": item_payload["link"]
                }
                item_id_counter += 1
            response_sections.append({
                "section_title": clean_title,
                "items_count": len(items_with_ids),
                "items": items_with_ids
            })
        
        # sections_stats'ƒ± is_title_similar ile yeniden hesapla
        sections_stats_clean = []
        for section in all_sections:
            raw_title = section.get('section_title', '')
            clean_title = re.sub(r"\d+\s*$", "", raw_title).strip()
            items = section.get('items', [])
            
            uploaded_count = 0
            not_uploaded_count = 0
            
            for item in items:
                item_baslik = item.get('baslik', '')
                item_normalized = normalize_for_exact_match(item_baslik)
                is_uploaded = False
                
                # MevzuatGPT/Supabase'den gelen belgelerle kar≈üƒ±la≈ütƒ±r
                # SADECE TAM E≈ûLE≈ûME kullan (is_title_similar √ßok gev≈üek, yanlƒ±≈ü e≈üle≈ümelere neden oluyor)
                if uploaded_docs:
                    for doc in uploaded_docs:
                        doc_titles = [
                            ("belge_adi", doc.get("belge_adi", "")),
                            ("title", doc.get("title", "")),
                            ("document_name", doc.get("document_name", "")),
                            ("filename", doc.get("filename", "")),
                            ("name", doc.get("name", ""))
                        ]
                        
                        for field_name, doc_title in doc_titles:
                            if doc_title:
                                # Sadece tam e≈üle≈üme kontrol√º (normalize_for_exact_match ile)
                                doc_normalized = normalize_for_exact_match(doc_title)
                                if item_normalized == doc_normalized:
                                    is_uploaded = True
                                    break
                        
                        if is_uploaded:
                            break
                
                if is_uploaded:
                    uploaded_count += 1
                else:
                    not_uploaded_count += 1
            
            sections_stats_clean.append({
                "section_title": clean_title,
                "total": len(items),
                "uploaded": uploaded_count,
                "not_uploaded": not_uploaded_count
            })
        
        response_data = {
            "total_sections": stats.get('total_sections', 0),
            "total_items": stats.get('total_items', 0),
            "uploaded_documents_count": stats.get('uploaded_documents_count', len(uploaded_docs)),
            "sections": response_sections,
            "sections_stats": sections_stats_clean
        }
        
        # Nihai response'u JSON dosyasƒ±na kaydet
        try:
            import json
            from datetime import datetime
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"kar≈üƒ±la≈ütƒ±rma_sonu√ßlarƒ±_{kurum_id}_{timestamp}.json"
            filepath = os.path.join(os.getcwd(), filename)
            
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(response_data, f, ensure_ascii=False, indent=2)
            
            print(f"‚úÖ Kar≈üƒ±la≈ütƒ±rma sonu√ßlarƒ± kaydedildi: {filename}")
        except Exception as e:
            print(f"‚ö†Ô∏è JSON dosyasƒ±na kaydetme hatasƒ±: {str(e)}")
        
        return ScrapeResponse(
            success=True,
            message=f"{kurum_adi} tarama i≈ülemi ba≈üarƒ±yla tamamlandƒ±." + (" (JSON verisi kullanƒ±ldƒ±)" if req.sections else " (Siteden tarama yapƒ±ldƒ±)"),
            data=response_data
        )
        
    except Exception as e:
        print(f"‚ùå Hata olu≈ütu: {str(e)}")
        import traceback
        traceback.print_exc()
        raise HTTPException(
            status_code=500,
            detail=f"Scraping i≈ülemi sƒ±rasƒ±nda hata olu≈ütu: {str(e)}"
        )


@app.post("/api/mevzuatgpt/generate-json", response_model=ScrapeResponse, tags=["SGK Scraper"], summary="Sadece tarama yap ve JSON olu≈ütur")
async def generate_scrape_json(req: GenerateJsonRequest):
    """
    Sadece scraper ile siteye baƒülanƒ±r, tarama yapar ve toplanan verileri JSON formatƒ±nda d√∂nd√ºr√ºr.
    API baƒülantƒ±sƒ±, Elasticsearch kontrol√º, kar≈üƒ±la≈ütƒ±rma gibi i≈ülemler yapƒ±lmaz.
    Sadece saf tarama yapƒ±lƒ±r ve ham veriler d√∂ner.
    Kurum ID'si ile MongoDB'den detsis numarasƒ± bulunur ve kullanƒ±lƒ±r.
    """
    try:
        print("\n" + "="*80)
        
        # id veya kurum_id kontrol√º
        kurum_id = req.id or getattr(req, 'kurum_id', None)
        if not kurum_id:
            return ScrapeResponse(
                success=False,
                message="Kurum ID (id veya kurum_id) g√∂nderilmedi.",
                data={"error": "KURUM_ID_REQUIRED"}
            )
        
        print(f"üöÄ JSON Olu≈üturma ƒ∞steƒüi Alƒ±ndƒ± (Kurum ID: {kurum_id}, Type: {req.type})")
        print("="*80)
        
        # Type kontrol√º
        if req.type.lower() != "kaysis":
            return ScrapeResponse(
                success=False,
                message=f"Desteklenmeyen scraper tipi: {req.type}. ≈ûu an i√ßin sadece 'kaysis' desteklenmektedir.",
                data={"error": "UNSUPPORTED_TYPE", "type": req.type}
            )
        
        # MongoDB'den kurum bilgisini √ßek (sadece detsis i√ßin)
        detsis = None
        try:
            client = _get_mongodb_client()
            if client:
                database_name = os.getenv("MONGODB_DATABASE", "mevzuatgpt")
                db = client[database_name]
                kurumlar_collection = db["kurumlar"]
                from bson import ObjectId
                kurum_doc = kurumlar_collection.find_one({"_id": ObjectId(kurum_id)})
                if kurum_doc:
                    detsis = kurum_doc.get("detsis", "")
                client.close()
        except Exception as e:
            print(f"‚ö†Ô∏è MongoDB'den kurum bilgisi alƒ±namadƒ±: {str(e)}")
        
        if not detsis:
            return ScrapeResponse(
                success=False,
                message=f"Kurum bulunamadƒ± veya DETSIS numarasƒ± bulunamadƒ±. Kurum ID: {kurum_id}",
                data={"error": "KURUM_NOT_FOUND", "kurum_id": kurum_id}
            )
        
        print(f"üìã Kurum ID: {kurum_id}")
        print(f"üî¢ DETSIS: {detsis}")
        
        # Sadece tarama yap (API baƒülantƒ±sƒ± yok, sadece siteye baƒülan)
        print("üåê KAYSƒ∞S sitesinden tarama ba≈ülatƒ±lƒ±yor (sadece scraper, API/Elasticsearch yok)...")
        
        # KAYSƒ∞S URL'ini olu≈ütur
        url = f"https://kms.kaysis.gov.tr/Home/Kurum/{detsis}"
        print(f"üì° Site: {url}")
        
        # MongoDB'den g√ºncel proxy bilgilerini √ßek
        proxies = get_proxy_from_db()
        if proxies:
            print("üîê Proxy kullanƒ±lƒ±yor...")
        else:
            print("‚ö†Ô∏è Proxy bulunamadƒ±, direkt baƒülantƒ± deneniyor...")
        
        # Siteye baƒülan ve HTML'i parse et
        try:
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',
                'Accept-Language': 'tr-TR,tr;q=0.9,en-US;q=0.8,en;q=0.7',
                'Accept-Encoding': 'gzip, deflate, br',
                'Referer': 'https://www.google.com/',
                'Sec-Ch-Ua': '"Not_A Brand";v="8", "Chromium";v="120", "Google Chrome";v="120"',
                'Sec-Ch-Ua-Mobile': '?0',
                'Sec-Ch-Ua-Platform': '"Windows"',
                'Sec-Fetch-Dest': 'document',
                'Sec-Fetch-Mode': 'navigate',
                'Sec-Fetch-Site': 'cross-site',
                'Sec-Fetch-User': '?1',
                'Upgrade-Insecure-Requests': '1',
                'Connection': 'keep-alive',
                'Cache-Control': 'max-age=0'
            }
            
            # curl_cffi ile Chrome taklidi yap (eƒüer mevcut ise)
            if CURL_CFFI_AVAILABLE:
                response = requests.get(
                    url,
                    headers=headers,
                    timeout=1200,  # 20 dakika timeout
                    proxies=proxies,
                    impersonate="chrome110"  # Chrome 110 TLS fingerprint
                )
            else:
                response = requests.get(url, headers=headers, timeout=1200, proxies=proxies)
            
            if response.status_code != 200:
                print(f"‚ùå Siteye eri≈üilemedi: HTTP {response.status_code}")
                return ScrapeResponse(
                    success=False,
                    message=f"Siteye eri≈üilemedi: HTTP {response.status_code}",
                    data={"error": "SITE_ACCESS_FAILED", "status_code": response.status_code}
                )
            
            # HTML'i parse et
            soup = BeautifulSoup(response.content, 'html.parser')
            print("‚úÖ Site ba≈üarƒ±yla y√ºklendi!")
            
            print("üìã Accordion yapƒ±sƒ± aranƒ±yor...")
            
            # accordion2 div'ini bul
            accordion_div = soup.find('div', {'id': 'accordion2', 'class': 'panel-group'})
            
            if not accordion_div:
                print("‚ö†Ô∏è accordion2 div'i bulunamadƒ±!")
                return ScrapeResponse(
                    success=False,
                    message="Site yapƒ±sƒ± bulunamadƒ± (accordion2 div'i yok).",
                    data={"error": "STRUCTURE_NOT_FOUND"}
                )
            
            print("‚úÖ Accordion yapƒ±sƒ± bulundu!")
            print("üîç Ba≈ülƒ±klar ve i√ßerikler √ßekiliyor...")
            
            # Accordion i√ßindeki t√ºm panel'leri bul
            panels = accordion_div.find_all('div', class_='panel')
            
            if not panels:
                panels = accordion_div.find_all(['div'], class_=lambda x: x and 'panel' in str(x).lower())
            
            all_sections = []
            
            if panels:
                for panel in panels:
                    # Panel ba≈ülƒ±ƒüƒ±nƒ± bul
                    panel_heading = panel.find('div', class_=lambda x: x and 'heading' in str(x).lower())
                    if not panel_heading:
                        panel_heading = panel.find(['h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'a', 'span'], class_=lambda x: x and ('heading' in str(x).lower() or 'title' in str(x).lower()))
                    
                    heading_text = ""
                    if panel_heading:
                        # Ba≈ülƒ±k i√ßindeki badge/span sayacƒ±larƒ±nƒ± √ßƒ±kar
                        try:
                            for badge in panel_heading.find_all('span', class_=lambda c: c and 'badge' in c):
                                badge.decompose()
                        except Exception:
                            pass
                        heading_text = panel_heading.get_text(strip=True)
                        # Sonda kalan sayƒ±larƒ± da temizle (√∂rn: "Kanunlar4" -> "Kanunlar")
                        heading_text = re.sub(r"\d+\s*$", "", heading_text).strip()
                    
                    # Panel i√ßindeki linkleri ve i√ßerikleri bul
                    panel_body = panel.find('div', class_=lambda x: x and 'body' in str(x).lower())
                    if not panel_body:
                        panel_body = panel
                    
                    # Panel i√ßindeki t√ºm linkleri bul
                    links_in_panel = panel_body.find_all('a', href=True)
                    
                    items_in_section = []
                    for link in links_in_panel:
                        link_href = link.get('href', '')
                        
                        # Link i√ßinde badge span'i varsa atla
                        if link.find('span', class_='badge'):
                            continue
                        
                        # Link metnini al
                        link_text = link.get_text(strip=True)
                        
                        # Bo≈ü veya √ßok kƒ±sa metinleri atla
                        if not link_text or len(link_text.strip()) < 10:
                            continue
                        
                        # Sadece sayƒ±lardan olu≈üan metinleri atla
                        if re.match(r'^[\d\s.,]+$', link_text.strip()):
                            continue
                        
                        # Link URL'ini tamamla
                        if link_href.startswith('http'):
                            full_url = link_href
                        elif link_href.startswith('/'):
                            full_url = f"https://kms.kaysis.gov.tr{link_href}"
                        else:
                            full_url = f"{url}{link_href}"
                        
                        # Sadece /Home/Goster/ ile ba≈ülayan linkleri al
                        if not full_url or '/Home/Goster/' not in full_url:
                            continue
                        
                        # Metni formatla: yalnƒ±zca ba≈ülƒ±ƒüƒ±n ilk harfi b√ºy√ºk, diƒüerleri k√º√ß√ºk (T√ºrk√ße)
                        formatted_text = turkish_sentence_case(link_text)
                        formatted_text = re.sub(r'\d+$', '', formatted_text).strip()
                        original_text = link_text.strip()
                        
                        items_in_section.append({
                            'baslik': formatted_text,
                            'baslik_original': original_text,
                            'link': full_url
                        })
                    
                    if heading_text or items_in_section:
                        all_sections.append({
                            'section_title': heading_text or 'Ba≈ülƒ±ksƒ±z B√∂l√ºm',
                            'items': items_in_section
                        })
            
            print(f"‚úÖ {len(all_sections)} b√∂l√ºm bulundu")
            total_items = sum(len(section['items']) for section in all_sections)
            print(f"üìä Toplam {total_items} mevzuat bulundu")
            
            if not all_sections:
                return ScrapeResponse(
                    success=False,
                    message="Tarama ba≈üarƒ±sƒ±z veya sonu√ß bulunamadƒ±.",
                    data={"error": "SCRAPE_FAILED", "message": "Hi√ß b√∂l√ºm bulunamadƒ±"}
                )
            
            # Stats olu≈ütur (sadece temel bilgiler, API kar≈üƒ±la≈ütƒ±rmasƒ± yok)
            stats = {
                'total_sections': len(all_sections),
                'total_items': total_items
            }
            
            # JSON formatƒ±nƒ± hazƒ±rla (ham veriler, kar≈üƒ±la≈ütƒ±rma yok)
            json_data = {
                "kurum_id": kurum_id,
                "detsis": detsis,
                "type": req.type,
                "sections": all_sections,
                "stats": stats
            }
            
            print(f"‚úÖ JSON olu≈üturuldu: {len(all_sections)} b√∂l√ºm, {total_items} mevzuat")
            
            return ScrapeResponse(
                success=True,
                message=f"Tarama tamamlandƒ± ve JSON olu≈üturuldu.",
                data=json_data
            )
            
        except requests.exceptions.RequestException as e:
            print(f"‚ùå Baƒülantƒ± hatasƒ±: {str(e)}")
            return ScrapeResponse(
                success=False,
                message=f"Baƒülantƒ± hatasƒ±: {str(e)}",
                data={"error": "CONNECTION_ERROR", "message": str(e)}
            )
        except Exception as e:
            print(f"‚ùå Tarama hatasƒ±: {str(e)}")
            import traceback
            traceback.print_exc()
            return ScrapeResponse(
                success=False,
                message=f"Tarama hatasƒ±: {str(e)}",
                data={"error": "SCRAPE_ERROR", "message": str(e)}
            )
        
    except Exception as e:
        print(f"‚ùå Hata olu≈ütu: {str(e)}")
        import traceback
        traceback.print_exc()
        raise HTTPException(
            status_code=500,
            detail=f"JSON olu≈üturma i≈ülemi sƒ±rasƒ±nda hata olu≈ütu: {str(e)}"
        )


@app.post("/api/kurum/portal-scan", response_model=ScrapeResponse, tags=["SGK Scraper"], summary="Kurum portal tarama (MongoDB kontrol√º)")
async def scrape_kurum_portal(req: PortalScanRequest):
    """
    Belirtilen kurumun mevzuatlarƒ±nƒ± tarar ve MongoDB metadata koleksiyonundaki kayƒ±tlarla kar≈üƒ±la≈ütƒ±rƒ±r.
    Portal durumunu (true/false) d√∂ner.
    type parametresi ile scraper tipi belirlenir (≈üu an i√ßin sadece 'kaysis' desteklenir).
    """
    try:
        print("\n" + "="*80)
        print(f"üöÄ API Endpoint'ten Kurum Portal Tarama ƒ∞steƒüi Alƒ±ndƒ± (Kurum ID: {req.id}, Type: {req.type})")
        print("="*80)
        
        # Type kontrol√º
        if req.type.lower() != "kaysis":
            return ScrapeResponse(
                success=False,
                message=f"Desteklenmeyen scraper tipi: {req.type}. ≈ûu an i√ßin sadece 'kaysis' desteklenmektedir.",
                data={"error": "UNSUPPORTED_TYPE", "type": req.type}
            )
        
        # MongoDB'den kurum bilgisini √ßek
        kurum_adi = None
        try:
            client = _get_mongodb_client()
            if client:
                database_name = os.getenv("MONGODB_DATABASE", "mevzuatgpt")
                db = client[database_name]
                kurumlar_collection = db["kurumlar"]
                from bson import ObjectId
                kurum_doc = kurumlar_collection.find_one({"_id": ObjectId(req.id)})
                if kurum_doc:
                    kurum_adi = kurum_doc.get("kurum_adi", "Bilinmeyen Kurum")
                client.close()
        except Exception as e:
            print(f"‚ö†Ô∏è MongoDB'den kurum bilgisi alƒ±namadƒ±: {str(e)}")
            kurum_adi = "Bilinmeyen Kurum"
        
        print(f"üìã Kurum: {kurum_adi}")
        print(f"üî¢ DETSIS: {req.detsis}")
        
        # MongoDB'den portal'da bulunan pdf_adi'larƒ± √ßek
        portal_title_set = set()
        try:
            client = _get_mongodb_client()
            if client:
                database_name = os.getenv("MONGODB_DATABASE", "mevzuatgpt")
                metadata_collection_name = os.getenv("MONGODB_METADATA_COLLECTION", "metadata")
                db = client[database_name]
                metadata_collection = db[metadata_collection_name]
                # Sadece pdf_adi alanƒ±nƒ± al
                cursor = metadata_collection.find({}, {"pdf_adi": 1})
                count = 0
                for doc in cursor:
                    val = (doc.get("pdf_adi") or "").strip()
                    if val:
                        portal_title_set.add(to_title(val))
                        count += 1
                client.close()
                print(f"‚úÖ MongoDB'den {count} pdf_adi okundu (portal kar≈üƒ±la≈ütƒ±rmasƒ± i√ßin)")
        except Exception as e:
            print(f"‚ö†Ô∏è MongoDB portal listesi okunamadƒ±: {str(e)}")
        
        # KAYSƒ∞S scraper'ƒ± kullan
        if req.type.lower() == "kaysis":
            all_sections, stats = scrape_kaysis_mevzuat(detsis=req.detsis)
            print_results_to_console(all_sections, stats)
        
        # Response hazƒ±rla (benzersiz item id'leri, portal durumu ve b√∂l√ºm ba≈ülƒ±k temizleme)
        item_id_counter = 1
        response_sections = []
        # √ñnbelleƒüi sƒ±fƒ±rla
        global last_item_map
        last_item_map = {}
        for section in all_sections:
            raw_title = section['section_title']
            # Sonunda kalan sayƒ±larƒ± temizle (√∂rn: "Kanunlar4" -> "Kanunlar")
            clean_title = re.sub(r"\d+\s*$", "", raw_title).strip()
            items = section.get('items', [])
            items_with_ids = []
            for item in items:
                # Portal (MongoDB metadata.pdf_adi kar≈üƒ±la≈ütƒ±rmasƒ±) - %100 e≈üitlik
                item_title_tc = to_title(item.get('baslik', ''))
                is_in_portal = (item_title_tc in portal_title_set)
                
                # Benzersiz id ver ve √∂nbelleƒüe yaz
                item_payload = {
                    "id": item_id_counter,
                    "portal": is_in_portal,
                    "baslik": item.get('baslik', ''),
                    "link": item.get('link', '')
                }
                items_with_ids.append(item_payload)

                # √ñnbelleƒüe kategori bilgisini de ekleyerek koy
                last_item_map[item_id_counter] = {
                    "section_title": clean_title,
                    "baslik": item_payload["baslik"],
                    "link": item_payload["link"]
                }
                item_id_counter += 1
            response_sections.append({
                "section_title": clean_title,
                "items_count": len(items_with_ids),
                "items": items_with_ids
            })
        
        # sections_stats'ƒ± portal_title_set ile hesapla
        sections_stats_clean = []
        for section in all_sections:
            raw_title = section['section_title']
            clean_title = re.sub(r"\d+\s*$", "", raw_title).strip()
            items = section.get('items', [])
            
            portal_count = 0
            not_portal_count = 0
            
            for item in items:
                item_title_tc = to_title(item.get('baslik', ''))
                is_in_portal = (item_title_tc in portal_title_set)
                if is_in_portal:
                    portal_count += 1
                else:
                    not_portal_count += 1
            
            sections_stats_clean.append({
                "section_title": clean_title,
                "total": len(items),
                "portal": portal_count,
                "not_portal": not_portal_count
            })
        
        response_data = {
            "total_sections": stats.get('total_sections', 0),
            "total_items": stats.get('total_items', 0),
            "portal_documents_count": len(portal_title_set),
            "sections": response_sections,
            "sections_stats": sections_stats_clean
        }
        
        return ScrapeResponse(
            success=True,
            message=f"{kurum_adi} portal tarama i≈ülemi ba≈üarƒ±yla tamamlandƒ±. Sonu√ßlar konsola yazdƒ±rƒ±ldƒ±.",
            data=response_data
        )
        
    except Exception as e:
        print(f"‚ùå Hata olu≈ütu: {str(e)}")
        import traceback
        traceback.print_exc()
        raise HTTPException(
            status_code=500,
            detail=f"Portal tarama i≈ülemi sƒ±rasƒ±nda hata olu≈ütu: {str(e)}"
        )


@app.get("/health", tags=["Health"], summary="Saƒülƒ±k kontrol√º")
async def health_check():
    """
    Detaylƒ± saƒülƒ±k kontrol√º endpoint'i.
    Servis durumu, MongoDB baƒülantƒ±sƒ± ve sistem bilgilerini kontrol eder.
    """
    health_status = {
        "status": "healthy",
        "service": "SGK Scraper API",
        "timestamp": datetime.now().isoformat(),
        "checks": {}
    }
    
    # 1. MongoDB baƒülantƒ± kontrol√º
    try:
        client = _get_mongodb_client()
        if client:
            client.admin.command('ping')
            client.close()
            health_status["checks"]["mongodb"] = {
                "status": "healthy",
                "message": "MongoDB baƒülantƒ±sƒ± ba≈üarƒ±lƒ±"
            }
        else:
            health_status["checks"]["mongodb"] = {
                "status": "unhealthy",
                "message": "MongoDB baƒülantƒ±sƒ± kurulamadƒ±"
            }
            health_status["status"] = "degraded"
    except Exception as e:
        health_status["checks"]["mongodb"] = {
            "status": "unhealthy",
            "message": f"MongoDB baƒülantƒ± hatasƒ±: {str(e)}"
        }
        health_status["status"] = "degraded"
    
    # 2. Systemd servis durumu kontrol√º
    try:
        service_name = "pdfanalyzerrag"
        
        # systemctl komutunu farklƒ± path'lerde ara (√∂nce en yaygƒ±n path'ler)
        systemctl_paths = ["/usr/bin/systemctl", "/bin/systemctl", "systemctl"]
        systemctl_cmd = None
        
        for path in systemctl_paths:
            try:
                if path == "systemctl":
                    # PATH'te ara
                    result = subprocess.run(
                        ["which", "systemctl"],
                        capture_output=True,
                        timeout=2
                    )
                    if result.returncode == 0:
                        systemctl_cmd = result.stdout.strip().decode('utf-8') if result.stdout else "systemctl"
                        break
                else:
                    # Direkt path'i kontrol et
                    result = subprocess.run(
                        ["test", "-f", path],
                        capture_output=True,
                        timeout=2
                    )
                    if result.returncode == 0:
                        systemctl_cmd = path
                        break
            except Exception:
                continue
        
        if systemctl_cmd:
            result = subprocess.run(
                [systemctl_cmd, "is-active", service_name],
                capture_output=True,
                text=True,
                timeout=5
            )
            if result.returncode == 0:
                service_status = result.stdout.strip()
                health_status["checks"]["systemd_service"] = {
                    "status": "healthy" if service_status == "active" else "unhealthy",
                    "message": f"Servis durumu: {service_status}",
                    "service_name": service_name
                }
                if service_status != "active":
                    health_status["status"] = "unhealthy"
            else:
                health_status["checks"]["systemd_service"] = {
                    "status": "unknown",
                    "message": f"Servis durumu kontrol edilemedi: {result.stderr.strip() if result.stderr else 'Servis bulunamadƒ± veya eri≈üilemedi'}"
                }
        else:
            health_status["checks"]["systemd_service"] = {
                "status": "not_available",
                "message": "systemctl komutu bulunamadƒ± (systemd mevcut deƒüil veya PATH'te yok)",
                "note": "Bu sistemde systemd servis y√∂netimi kullanƒ±lamƒ±yor olabilir"
            }
    except (subprocess.TimeoutExpired, FileNotFoundError, Exception) as e:
        health_status["checks"]["systemd_service"] = {
            "status": "not_available",
            "message": f"Systemd servis kontrol√º yapƒ±lamadƒ±: {str(e)}",
            "note": "Sistem systemd kullanmƒ±yor olabilir veya yetki sorunu olabilir"
        }
    
    # 3. curl_cffi kontrol√º
    health_status["checks"]["curl_cffi"] = {
        "status": "available" if CURL_CFFI_AVAILABLE else "unavailable",
        "message": "curl_cffi mevcut" if CURL_CFFI_AVAILABLE else "curl_cffi kurulu deƒüil (standart requests kullanƒ±lƒ±yor)"
    }
    
    # 4. Sistem bilgileri
    health_status["system"] = {
        "platform": platform.system(),
        "platform_release": platform.release(),
        "python_version": platform.python_version()
    }
    
    return health_status


@app.get("/api/health/logs", tags=["Health"], summary="Servis loglarƒ±nƒ± getir")
async def get_service_logs(lines: int = 100):
    """
    Systemd servis loglarƒ±nƒ± getirir.
    
    Args:
        lines: G√∂sterilecek log satƒ±rƒ± sayƒ±sƒ± (varsayƒ±lan: 100, maksimum: 1000)
    
    Returns:
        Servis loglarƒ± ve metadata
    """
    try:
        # Satƒ±r sayƒ±sƒ±nƒ± sƒ±nƒ±rla
        lines = max(1, min(lines, 1000))
        
        service_name = "pdfanalyzerrag"
        
        # journalctl komutunu farklƒ± path'lerde ara (√∂nce en yaygƒ±n path'ler)
        journalctl_paths = ["/usr/bin/journalctl", "/bin/journalctl", "journalctl"]
        journalctl_cmd = None
        
        for path in journalctl_paths:
            try:
                if path == "journalctl":
                    # PATH'te ara
                    result = subprocess.run(
                        ["which", "journalctl"],
                        capture_output=True,
                        timeout=2
                    )
                    if result.returncode == 0:
                        journalctl_cmd = result.stdout.strip().decode('utf-8') if result.stdout else "journalctl"
                        break
                else:
                    # Direkt path'i kontrol et
                    result = subprocess.run(
                        ["test", "-f", path],
                        capture_output=True,
                        timeout=2
                    )
                    if result.returncode == 0:
                        journalctl_cmd = path
                        break
            except Exception:
                continue
        
        if not journalctl_cmd:
            return {
                "success": False,
                "service_name": service_name,
                "error": "journalctl komutu bulunamadƒ± (systemd mevcut deƒüil)",
                "timestamp": datetime.now().isoformat(),
                "logs": [],
                "raw_logs": "",
                "note": "Bu sistemde systemd log y√∂netimi kullanƒ±lamƒ±yor"
            }
        
        # journalctl komutunu √ßalƒ±≈ütƒ±r
        result = subprocess.run(
            [journalctl_cmd, "-u", service_name, "-n", str(lines), "--no-pager"],
            capture_output=True,
            text=True,
            timeout=10
        )
        
        if result.returncode == 0:
            logs = result.stdout.strip()
            log_lines = logs.split('\n') if logs else []
            
            return {
                "success": True,
                "service_name": service_name,
                "lines_requested": lines,
                "lines_returned": len(log_lines),
                "timestamp": datetime.now().isoformat(),
                "logs": log_lines,
                "raw_logs": logs
            }
        else:
            # journalctl komutu ba≈üarƒ±sƒ±z oldu, alternatif y√∂ntem dene
            error_msg = result.stderr.strip() if result.stderr else "Bilinmeyen hata"
            
            # systemctl status komutunu dene
            systemctl_paths = ["/usr/bin/systemctl", "/bin/systemctl", "systemctl"]
            systemctl_cmd = None
            
            for path in systemctl_paths:
                try:
                    if path == "systemctl":
                        # PATH'te ara
                        test_result = subprocess.run(
                            ["which", "systemctl"],
                            capture_output=True,
                            timeout=2
                        )
                        if test_result.returncode == 0:
                            systemctl_cmd = test_result.stdout.strip().decode('utf-8') if test_result.stdout else "systemctl"
                            break
                    else:
                        # Direkt path'i kontrol et
                        test_result = subprocess.run(
                            ["test", "-f", path],
                            capture_output=True,
                            timeout=2
                        )
                        if test_result.returncode == 0:
                            systemctl_cmd = path
                            break
                except Exception:
                    continue
            
            if systemctl_cmd:
                try:
                    status_result = subprocess.run(
                        [systemctl_cmd, "status", service_name, "--no-pager", "-n", str(lines)],
                        capture_output=True,
                        text=True,
                        timeout=10
                    )
                    if status_result.returncode == 0:
                        logs = status_result.stdout.strip()
                        log_lines = logs.split('\n') if logs else []
                        return {
                            "success": True,
                            "service_name": service_name,
                            "lines_requested": lines,
                            "lines_returned": len(log_lines),
                            "timestamp": datetime.now().isoformat(),
                            "logs": log_lines,
                            "raw_logs": logs,
                            "note": "journalctl kullanƒ±lamadƒ±, systemctl status kullanƒ±ldƒ±"
                        }
                except Exception:
                    pass
            
            return {
                "success": False,
                "service_name": service_name,
                "error": f"Loglar alƒ±namadƒ±: {error_msg}",
                "timestamp": datetime.now().isoformat(),
                "logs": [],
                "raw_logs": ""
            }
            
    except subprocess.TimeoutExpired:
        return {
            "success": False,
            "error": "Log alma i≈ülemi zaman a≈üƒ±mƒ±na uƒüradƒ±",
            "timestamp": datetime.now().isoformat(),
            "logs": [],
            "raw_logs": ""
        }
    except FileNotFoundError:
        return {
            "success": False,
            "error": "journalctl komutu bulunamadƒ± (systemd mevcut deƒüil)",
            "timestamp": datetime.now().isoformat(),
            "logs": [],
            "raw_logs": ""
        }
    except Exception as e:
        return {
            "success": False,
            "error": f"Beklenmeyen hata: {str(e)}",
            "timestamp": datetime.now().isoformat(),
            "logs": [],
            "raw_logs": ""
        }


@app.get("/api/health/status", tags=["Health"], summary="Servis durumu detaylƒ± bilgi")
async def get_service_status():
    """
    Systemd servis durumunu detaylƒ± olarak getirir.
    
    Returns:
        Servis durumu, aktif s√ºre, son restart zamanƒ± vb.
    """
    try:
        service_name = "pdfanalyzerrag"
        
        # systemctl komutunu farklƒ± path'lerde ara (√∂nce en yaygƒ±n path'ler)
        systemctl_paths = ["/usr/bin/systemctl", "/bin/systemctl", "systemctl"]
        systemctl_cmd = None
        
        for path in systemctl_paths:
            try:
                if path == "systemctl":
                    # PATH'te ara
                    test_result = subprocess.run(
                        ["which", "systemctl"],
                        capture_output=True,
                        timeout=2
                    )
                    if test_result.returncode == 0:
                        systemctl_cmd = test_result.stdout.strip().decode('utf-8') if test_result.stdout else "systemctl"
                        break
                else:
                    # Direkt path'i kontrol et
                    test_result = subprocess.run(
                        ["test", "-f", path],
                        capture_output=True,
                        timeout=2
                    )
                    if test_result.returncode == 0:
                        systemctl_cmd = path
                        break
            except Exception:
                continue
        
        if not systemctl_cmd:
            return {
                "success": False,
                "error": "systemctl komutu bulunamadƒ± (systemd mevcut deƒüil)",
                "timestamp": datetime.now().isoformat(),
                "note": "Bu sistemde systemd servis y√∂netimi kullanƒ±lamƒ±yor"
            }
        
        # systemctl status komutunu √ßalƒ±≈ütƒ±r
        result = subprocess.run(
            [systemctl_cmd, "status", service_name, "--no-pager"],
            capture_output=True,
            text=True,
            timeout=10
        )
        
        status_info = {
            "success": True,
            "service_name": service_name,
            "timestamp": datetime.now().isoformat(),
            "status_output": result.stdout.strip() if result.returncode == 0 else None,
            "error": result.stderr.strip() if result.stderr and result.returncode != 0 else None
        }
        
        # systemctl show komutu ile daha detaylƒ± bilgi al
        try:
            show_result = subprocess.run(
                [systemctl_cmd, "show", service_name, "--no-pager"],
                capture_output=True,
                text=True,
                timeout=10
            )
            if show_result.returncode == 0:
                # Key-value √ßiftlerini parse et
                details = {}
                for line in show_result.stdout.strip().split('\n'):
                    if '=' in line:
                        key, value = line.split('=', 1)
                        details[key] = value
                status_info["details"] = details
        except Exception:
            pass
        
        return status_info
        
    except subprocess.TimeoutExpired:
        return {
            "success": False,
            "error": "Servis durumu kontrol√º zaman a≈üƒ±mƒ±na uƒüradƒ±",
            "timestamp": datetime.now().isoformat()
        }
    except FileNotFoundError:
        return {
            "success": False,
            "error": "systemctl komutu bulunamadƒ± (systemd mevcut deƒüil)",
            "timestamp": datetime.now().isoformat()
        }
    except Exception as e:
        return {
            "success": False,
            "error": f"Beklenmeyen hata: {str(e)}",
            "timestamp": datetime.now().isoformat()
        }


# ========================
# MongoDB Admin Endpoints
# ========================

@app.get("/api/mongo/metadata/{id}", tags=["MongoDB"], summary="Metadata getir")
async def get_metadata(id: str):
    try:
        client, metadata_col, content_col = _get_mongo_collections()
        if not client:
            raise HTTPException(status_code=500, detail="MongoDB baƒülantƒ±sƒ± kurulamadƒ±")
        try:
            doc = metadata_col.find_one({"_id": ObjectId(id)})
        except Exception:
            client.close()
            raise HTTPException(status_code=400, detail="Ge√ßersiz metadata _id")
        if not doc:
            client.close()
            raise HTTPException(status_code=404, detail="Metadata bulunamadƒ±")
        doc["_id"] = str(doc["_id"])
        client.close()
        return {"success": True, "data": doc}
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Hata: {str(e)}")


@app.put("/api/mongo/metadata/{id}", tags=["MongoDB"], summary="Metadata g√ºncelle")
async def update_metadata(id: str, body: Dict[str, Any]):
    try:
        client, metadata_col, content_col = _get_mongo_collections()
        if not client:
            raise HTTPException(status_code=500, detail="MongoDB baƒülantƒ±sƒ± kurulamadƒ±")
        # G√ºvenli g√ºncelleme (bo≈ü/null deƒüerleri set etmeyelim)
        update_data: Dict[str, Any] = {}
        for k, v in (body or {}).items():
            if v is not None:
                update_data[k] = v
        if not update_data:
            client.close()
            return {"success": True, "message": "G√ºncellenecek alan yok"}
        try:
            res = metadata_col.update_one({"_id": ObjectId(id)}, {"$set": update_data})
        except Exception:
            client.close()
            raise HTTPException(status_code=400, detail="Ge√ßersiz metadata _id")
        client.close()
        if res.matched_count == 0:
            raise HTTPException(status_code=404, detail="Metadata bulunamadƒ±")
        return {"success": True, "modified": res.modified_count}
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Hata: {str(e)}")


@app.get("/api/mongo/content/by-metadata/{metadata_id}", tags=["MongoDB"], summary="Content getir (metadata)")
async def get_content_by_metadata(metadata_id: str):
    try:
        client, metadata_col, content_col = _get_mongo_collections()
        if not client:
            raise HTTPException(status_code=500, detail="MongoDB baƒülantƒ±sƒ± kurulamadƒ±")
        try:
            doc = content_col.find_one({"metadata_id": ObjectId(metadata_id)})
        except Exception:
            client.close()
            raise HTTPException(status_code=400, detail="Ge√ßersiz metadata_id")
        if not doc:
            client.close()
            raise HTTPException(status_code=404, detail="Content bulunamadƒ±")
        doc["_id"] = str(doc["_id"])
        doc["metadata_id"] = str(doc["metadata_id"])
        client.close()
        return {"success": True, "data": doc}
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Hata: {str(e)}")


@app.put("/api/mongo/content/by-metadata/{metadata_id}", tags=["MongoDB"], summary="Content g√ºncelle (metadata)")
async def update_content_by_metadata(metadata_id: str, body: Dict[str, Any]):
    try:
        client, metadata_col, content_col = _get_mongo_collections()
        if not client:
            raise HTTPException(status_code=500, detail="MongoDB baƒülantƒ±sƒ± kurulamadƒ±")
        new_content = (body or {}).get("icerik")
        if new_content is None:
            client.close()
            raise HTTPException(status_code=400, detail="Body i√ßinde 'icerik' alanƒ± gerekli")
        try:
            res = content_col.update_one(
                {"metadata_id": ObjectId(metadata_id)},
                {"$set": {"icerik": new_content}}
            )
        except Exception:
            client.close()
            raise HTTPException(status_code=400, detail="Ge√ßersiz metadata_id")
        client.close()
        if res.matched_count == 0:
            raise HTTPException(status_code=404, detail="Content bulunamadƒ±")
        return {"success": True, "modified": res.modified_count}
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Hata: {str(e)}")


@app.delete("/api/mongo/metadata/{id}", tags=["MongoDB"], summary="Portal i√ßeriƒüini sil (Metadata, Content ve Bunny.net PDF)")
async def delete_portal_content(id: str):
    """
    Portal i√ßeriƒüini tamamen siler:
    1. MongoDB metadata kaydƒ±nƒ± siler
    2. MongoDB content kaydƒ±nƒ± siler (metadata_id ile ili≈ükili)
    3. Bunny.net'teki PDF dosyasƒ±nƒ± siler (pdf_url'den)
    
    NOT: Bu i≈ülem sadece portal i√ßin ge√ßerlidir, MevzuatGPT'yi etkilemez.
    """
    try:
        client, metadata_col, content_col = _get_mongo_collections()
        if not client:
            raise HTTPException(status_code=500, detail="MongoDB baƒülantƒ±sƒ± kurulamadƒ±")
        
        try:
            # √ñnce metadata kaydƒ±nƒ± bul
            metadata_doc = metadata_col.find_one({"_id": ObjectId(id)})
            if not metadata_doc:
                client.close()
                raise HTTPException(status_code=404, detail="Metadata bulunamadƒ±")
            
            # pdf_url'i al (Bunny.net'ten silmek i√ßin)
            pdf_url = metadata_doc.get("pdf_url", "")
            
            print(f"üóëÔ∏è Portal i√ßeriƒüi siliniyor: metadata_id={id}")
            print(f"üìÑ PDF URL: {pdf_url}")
            
            # 1. Content kaydƒ±nƒ± sil (metadata_id ile ili≈ükili)
            content_result = content_col.delete_one({"metadata_id": ObjectId(id)})
            if content_result.deleted_count > 0:
                print(f"‚úÖ Content kaydƒ± silindi: {content_result.deleted_count} kayƒ±t")
            else:
                print("‚ö†Ô∏è Content kaydƒ± bulunamadƒ± (zaten silinmi≈ü olabilir)")
            
            # 2. Metadata kaydƒ±nƒ± sil
            metadata_result = metadata_col.delete_one({"_id": ObjectId(id)})
            if metadata_result.deleted_count == 0:
                client.close()
                raise HTTPException(status_code=404, detail="Metadata silinemedi (kayƒ±t bulunamadƒ±)")
            
            print(f"‚úÖ Metadata kaydƒ± silindi: {metadata_result.deleted_count} kayƒ±t")
            
            # 3. Bunny.net'ten PDF'i sil
            bunny_deleted = False
            if pdf_url:
                bunny_deleted = _delete_from_bunny(pdf_url)
            else:
                print("‚ö†Ô∏è PDF URL bulunamadƒ±, Bunny.net silme i≈ülemi atlandƒ±")
            
            client.close()
            
            # Sonu√ß mesajƒ±
            result_message = f"Portal i√ßeriƒüi ba≈üarƒ±yla silindi. Metadata: ‚úÖ, Content: ‚úÖ"
            if pdf_url:
                if bunny_deleted:
                    result_message += ", Bunny.net PDF: ‚úÖ"
                else:
                    result_message += ", Bunny.net PDF: ‚ö†Ô∏è (silme ba≈üarƒ±sƒ±z veya dosya bulunamadƒ±)"
            
            return {
                "success": True,
                "message": result_message,
                "deleted": {
                    "metadata": metadata_result.deleted_count,
                    "content": content_result.deleted_count,
                    "bunny_pdf": bunny_deleted
                }
            }
            
        except HTTPException:
            client.close()
            raise
        except Exception as e:
            client.close()
            if "invalid" in str(e).lower() or "objectid" in str(e).lower():
                raise HTTPException(status_code=400, detail="Ge√ßersiz metadata _id")
            raise HTTPException(status_code=500, detail=f"Silme i≈ülemi sƒ±rasƒ±nda hata: {str(e)}")
            
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Hata: {str(e)}")


@app.get("/api/mongo/metadata", tags=["MongoDB"], summary="Metadata listele")
async def list_metadata(limit: int = 100, offset: int = 0):
    """T√ºm metadata kayƒ±tlarƒ±nƒ± listeler (varsayƒ±lan limit 100)."""
    try:
        client, metadata_col, content_col = _get_mongo_collections()
        if not client:
            raise HTTPException(status_code=500, detail="MongoDB baƒülantƒ±sƒ± kurulamadƒ±")
        # G√ºvenli limit aralƒ±ƒüƒ±
        if limit <= 0:
            limit = 100
        if limit > 1000:
            limit = 1000
        if offset < 0:
            offset = 0
        total = metadata_col.count_documents({})
        cursor = metadata_col.find({}).skip(offset).limit(limit).sort("olusturulma_tarihi", -1)
        items = []
        for doc in cursor:
            doc["_id"] = str(doc["_id"])
            items.append(doc)
        client.close()
        return {"success": True, "total": total, "limit": limit, "offset": offset, "data": items}
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Hata: {str(e)}")


# ========================
# Kurumlar CRUD Endpoints
# ========================

def _get_kurumlar_collection():
    client = _get_mongodb_client()
    if not client:
        return None, None
    database_name = os.getenv("MONGODB_DATABASE", "mevzuatgpt")
    db = client[database_name]
    return client, db["kurumlar"]


def _get_kurum_duyuru_collection():
    client = _get_mongodb_client()
    if not client:
        return None, None
    database_name = os.getenv("MONGODB_DATABASE", "mevzuatgpt")
    db = client[database_name]
    return client, db["kurum_duyuru"]

def _get_links_collection():
    client = _get_mongodb_client()
    if not client:
        return None, None
    database_name = os.getenv("MONGODB_DATABASE", "mevzuatgpt")
    db = client[database_name]
    return client, db["links"]


@app.get("/api/mongo/kurumlar", tags=["Kurumlar"], summary="Kurumlarƒ± listele")
async def list_kurumlar(limit: int = 100, offset: int = 0):
    try:
        client, col = _get_kurumlar_collection()
        if not client:
            raise HTTPException(status_code=500, detail="MongoDB baƒülantƒ±sƒ± kurulamadƒ±")
        if limit <= 0:
            limit = 100
        if limit > 1000:
            limit = 1000
        if offset < 0:
            offset = 0
        total = col.count_documents({})
        cursor = col.find({}).skip(offset).limit(limit).sort("olusturulma_tarihi", -1)
        items = []
        for d in cursor:
            d["_id"] = str(d["_id"])
            items.append(d)
        client.close()
        return {"success": True, "total": total, "limit": limit, "offset": offset, "data": items}
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Hata: {str(e)}")


# ==============================
# e-Devlet Link Scraper Endpoint
# ==============================

def _is_valid_url(url: str) -> bool:
    try:
        parsed = urlparse(url)
        return bool(parsed.scheme and parsed.netloc)
    except Exception:
        return False


def _is_safe_edevlet_url(url: str) -> bool:
    try:
        parsed = urlparse(url)
        if parsed.scheme not in ["http", "https"]:
            return False
        hostname = parsed.hostname or ""
        allowed_domains = [
            'turkiye.gov.tr',
            'www.turkiye.gov.tr',
            'gov.tr',
            'e-devlet.gov.tr'
        ]
        for d in allowed_domains:
            if hostname == d or hostname.endswith('.' + d):
                return True
        return False
    except Exception:
        return False


def _extract_links_from_page(base_url: str, html: bytes) -> List[Dict[str, str]]:
    soup = BeautifulSoup(html, 'html.parser')

    # √ñncelik verilen selekt√∂rler
    priority_selectors = [
        'a.integratedService[href]:not([href=""])',
        'a[data-description][href]:not([href=""])'
    ]
    general_selectors = [
        '.service-item a',
        '.link-item a',
        '.menu-item a',
        'li a[href]:not([href="#"]):not([href=""])',
        '.card a',
        '.services-list a',
        '.category-list a',
        '.service-card a',
        'a[href*="/hizmet"]'
    ]

    containers = []
    for selector in priority_selectors:
        containers.extend(soup.select(selector))
    if len(containers) < 5:
        for selector in general_selectors:
            containers.extend(soup.select(selector))

    # Tekille≈ütir
    seen = set()
    unique = []
    for el in containers:
        href = el.get('href', '')
        if href and href not in seen:
            seen.add(href)
            unique.append(el)

    results: List[Dict[str, str]] = []
    seen_urls_result = set()
    for el in unique:
        href = el.get('href', '').strip()
        if not href:
            continue
        full_url = urljoin(base_url, href)
        if full_url in seen_urls_result:
            continue

        # Ba≈ülƒ±k
        title = el.get_text(strip=True) or el.get('title', '').strip() or el.get('alt', '') or el.get('aria-label', '')
        if not title:
            # √úst ba≈ülƒ±klarƒ± dene
            parent = el.parent
            while parent and not title:
                if parent.name in ['h1', 'h2', 'h3', 'h4', 'h5', 'h6']:
                    title = parent.get_text(strip=True)
                    break
                parent = parent.parent
        title = (title or "Ba≈ülƒ±k bulunamadƒ±")[:200]

        # A√ßƒ±klama
        description = el.get('data-description', '').strip()
        if not description:
            parent = el.parent
            if parent:
                siblings = parent.find_all(['p', 'span', 'div'], class_=lambda x: x and ('desc' in x.lower() or 'summary' in x.lower()))
                for s in siblings:
                    txt = s.get_text(strip=True)
                    if txt and len(txt) > 10:
                        description = txt
                        break
        if not description:
            next_elements = el.find_next_siblings(['p', 'div', 'span'])
            for ne in next_elements[:3]:
                txt = ne.get_text(strip=True)
                if txt and 20 <= len(txt) < 500:
                    description = txt
                    break
        description = (description or "A√ßƒ±klama bulunamadƒ±")[:500]

        # Filtreler
        if not _is_valid_url(full_url):
            continue
        lower_url = full_url.lower()
        skip_patterns = ['javascript:', 'mailto:', 'tel:', '#', '.pdf', '.doc', '.docx', '.xls', '.xlsx', 'facebook.com', 'twitter.com', 'instagram.com', 'youtube.com']
        if any(p in lower_url for p in skip_patterns):
            continue
        if not title or len(title.strip()) < 3:
            continue
        if 'turkiye.gov.tr' in lower_url:
            # Daha kƒ±sa ba≈ülƒ±klara izin ver
            if len(title.strip()) < 8:
                continue

        results.append({
            "baslik": title,
            "aciklama": description,
            "url": full_url
        })
        seen_urls_result.add(full_url)

    return results


@app.post("/api/mongo/edevlet/scrape", tags=["e-Devlet Scraper"], summary="e-Devlet linkleri topla ve kaydet")
async def scrape_edevlet_links(body: Dict[str, Any]):
    """
    Verilen e-Devlet/T√ºrkiye.gov.tr sayfasƒ±ndan hizmet linklerini toplayƒ±p `links` koleksiyonuna kaydeder.
    Beklenen body: {"kurum_id": "ObjectId string", "url": "https://www.turkiye.gov.tr/..."}
    """
    try:
        client, col = _get_links_collection()
        if not client:
            raise HTTPException(status_code=500, detail="MongoDB baƒülantƒ±sƒ± kurulamadƒ±")

        kurum_id = (body or {}).get("kurum_id")
        url = (body or {}).get("url")
        if not kurum_id:
            client.close()
            raise HTTPException(status_code=400, detail="'kurum_id' zorunlu")
        if not url:
            client.close()
            raise HTTPException(status_code=400, detail="'url' zorunlu")

        # kurum_id doƒürula
        try:
            kurum_oid = ObjectId(str(kurum_id).strip())
        except Exception:
            client.close()
            raise HTTPException(status_code=400, detail="'kurum_id' ge√ßersiz ObjectId")

        # URL g√ºvenlik ve format kontrolleri
        if not _is_valid_url(url):
            client.close()
            raise HTTPException(status_code=400, detail="Ge√ßersiz URL formatƒ±")
        if not _is_safe_edevlet_url(url):
            client.close()
            raise HTTPException(status_code=400, detail="Bu URL izin verilen domainlerde deƒüil")

        # E-devlet scraper'ƒ±nda proxy kullanƒ±lƒ±yor
        proxies = get_proxy_from_db()
        if proxies:
            print("üîê E-devlet scraper'ƒ±nda proxy kullanƒ±lƒ±yor...")
        else:
            print("‚ö†Ô∏è Proxy bulunamadƒ±, direkt baƒülantƒ± deneniyor...")
        
        # Sayfayƒ± √ßek - Ger√ßek bir Chrome tarayƒ±cƒ±sƒ±nƒ±n g√∂nderdiƒüi t√ºm header'lar
        try:
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',
                'Accept-Language': 'tr-TR,tr;q=0.9,en-US;q=0.8,en;q=0.7',
                'Accept-Encoding': 'gzip, deflate, br',
                'Referer': 'https://www.google.com/',
                'Sec-Ch-Ua': '"Not_A Brand";v="8", "Chromium";v="120", "Google Chrome";v="120"',
                'Sec-Ch-Ua-Mobile': '?0',
                'Sec-Ch-Ua-Platform': '"Windows"',
                'Sec-Fetch-Dest': 'document',
                'Sec-Fetch-Mode': 'navigate',
                'Sec-Fetch-Site': 'cross-site',
                'Sec-Fetch-User': '?1',
                'Upgrade-Insecure-Requests': '1',
                'Connection': 'keep-alive',
                'Cache-Control': 'max-age=0'
            }
            
            # curl_cffi ile Chrome taklidi yap (eƒüer mevcut ise)
            if CURL_CFFI_AVAILABLE:
                resp = requests.get(
                    url,
                    headers=headers,
                    timeout=15,
                    proxies=proxies,
                    impersonate="chrome110"  # Chrome 110 TLS fingerprint
                )
            else:
                resp = requests.get(url, headers=headers, timeout=15, proxies=proxies)
            resp.raise_for_status()
        except Exception as e:
            client.close()
            raise HTTPException(status_code=502, detail=f"HTTP hatasƒ±: {str(e)}")

        links = _extract_links_from_page(url, resp.content)

        if not links:
            client.close()
            return {"success": True, "inserted_count": 0, "data": []}

        # Dok√ºmanlarƒ± hazƒ±rla
        now_iso = datetime.now().isoformat()
        docs = []
        for item in links:
            docs.append({
                "baslik": item.get("baslik", ""),
                "aciklama": item.get("aciklama", ""),
                "url": item.get("url", ""),
                "kurum_id": kurum_oid,
                "created_at": now_iso
            })

        # Ekle (toplu)
        try:
            res = col.insert_many(docs, ordered=False)
            inserted_count = len(res.inserted_ids)
        except Exception as e:
            client.close()
            raise HTTPException(status_code=500, detail=f"MongoDB ekleme hatasƒ±: {str(e)}")

        # JSON uyumlu d√∂n√º≈ü (ObjectId d√∂n√º≈üt√ºr)
        def _to_jsonable(o):
            if isinstance(o, ObjectId):
                return str(o)
            if isinstance(o, dict):
                return {k: _to_jsonable(v) for k, v in o.items()}
            if isinstance(o, list):
                return [_to_jsonable(x) for x in o]
            return o

        all_data = _to_jsonable(docs)
        
        client.close()
        return {"success": True, "inserted_count": inserted_count, "data": all_data}
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Hata: {str(e)}")


# ==============================
# Links Koleksiyonu CRUD Endpoints
# ==============================

@app.get("/api/mongo/links", tags=["Links"], summary="Linkleri listele")
async def list_links(limit: int = 100, offset: int = 0):
    try:
        client, col = _get_links_collection()
        if not client:
            raise HTTPException(status_code=500, detail="MongoDB baƒülantƒ±sƒ± kurulamadƒ±")
        if limit <= 0:
            limit = 100
        if limit > 1000:
            limit = 1000
        if offset < 0:
            offset = 0
        total = col.count_documents({})
        cursor = col.find({}).skip(offset).limit(limit).sort("_id", -1)
        items = []
        for d in cursor:
            d["_id"] = str(d["_id"]) 
            if "kurum_id" in d and isinstance(d["kurum_id"], ObjectId):
                d["kurum_id"] = str(d["kurum_id"]) 
            items.append(d)
        client.close()
        return {"success": True, "total": total, "limit": limit, "offset": offset, "data": items}
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Hata: {str(e)}")


@app.post("/api/mongo/links", tags=["Links"], summary="Link olu≈ütur")
async def create_link(body: Dict[str, Any]):
    try:
        client, col = _get_links_collection()
        if not client:
            raise HTTPException(status_code=500, detail="MongoDB baƒülantƒ±sƒ± kurulamadƒ±")
        data = body or {}
        baslik = (data.get("baslik") or "").strip()
        aciklama = (data.get("aciklama") or "").strip()
        url = (data.get("url") or "").strip()
        kurum_id = (data.get("kurum_id") or "").strip()

        if not baslik or not url or not kurum_id:
            client.close()
            raise HTTPException(status_code=400, detail="'baslik', 'url' ve 'kurum_id' zorunludur")
        if not _is_valid_url(url):
            client.close()
            raise HTTPException(status_code=400, detail="Ge√ßersiz URL formatƒ±")
        try:
            kurum_oid = ObjectId(kurum_id)
        except Exception:
            client.close()
            raise HTTPException(status_code=400, detail="'kurum_id' ge√ßersiz ObjectId")

        doc = {
            "baslik": baslik,
            "aciklama": aciklama,
            "url": url,
            "kurum_id": kurum_oid,
            "created_at": datetime.now().isoformat()
        }
        res = col.insert_one(doc)
        new_id = str(res.inserted_id)
        client.close()
        return {"success": True, "id": new_id}
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Hata: {str(e)}")


@app.get("/api/mongo/links/{id}", tags=["Links"], summary="Link getir")
async def get_link(id: str):
    try:
        client, col = _get_links_collection()
        if not client:
            raise HTTPException(status_code=500, detail="MongoDB baƒülantƒ±sƒ± kurulamadƒ±")
        try:
            d = col.find_one({"_id": ObjectId(id)})
        except Exception:
            client.close()
            raise HTTPException(status_code=400, detail="Ge√ßersiz link id")
        if not d:
            client.close()
            raise HTTPException(status_code=404, detail="Kayƒ±t bulunamadƒ±")
        d["_id"] = str(d["_id"]) 
        if "kurum_id" in d and isinstance(d["kurum_id"], ObjectId):
            d["kurum_id"] = str(d["kurum_id"]) 
        client.close()
        return {"success": True, "data": d}
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Hata: {str(e)}")


@app.put("/api/mongo/links/{id}", tags=["Links"], summary="Link g√ºncelle")
async def update_link(id: str, body: Dict[str, Any]):
    try:
        client, col = _get_links_collection()
        if not client:
            raise HTTPException(status_code=500, detail="MongoDB baƒülantƒ±sƒ± kurulamadƒ±")
        update_data: Dict[str, Any] = {}
        data = body or {}

        if "baslik" in data and data["baslik"] is not None:
            update_data["baslik"] = str(data["baslik"]).strip()
        if "aciklama" in data and data["aciklama"] is not None:
            update_data["aciklama"] = str(data["aciklama"]).strip()
        if "url" in data and data["url"] is not None:
            link_url = str(data["url"]).strip()
            if not _is_valid_url(link_url):
                client.close()
                raise HTTPException(status_code=400, detail="Ge√ßersiz URL formatƒ±")
            update_data["url"] = link_url
        if "kurum_id" in data and data["kurum_id"] is not None:
            try:
                update_data["kurum_id"] = ObjectId(str(data["kurum_id"]).strip())
            except Exception:
                client.close()
                raise HTTPException(status_code=400, detail="'kurum_id' ge√ßersiz ObjectId")

        if not update_data:
            client.close()
            return {"success": True, "modified": 0, "message": "G√ºncellenecek alan yok"}

        try:
            res = col.update_one({"_id": ObjectId(id)}, {"$set": update_data})
        except Exception:
            client.close()
            raise HTTPException(status_code=400, detail="Ge√ßersiz link id")
        client.close()
        if res.matched_count == 0:
            raise HTTPException(status_code=404, detail="Kayƒ±t bulunamadƒ±")
        return {"success": True, "modified": res.modified_count}
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Hata: {str(e)}")


@app.delete("/api/mongo/links/{id}", tags=["Links"], summary="Link sil")
async def delete_link(id: str):
    try:
        client, col = _get_links_collection()
        if not client:
            raise HTTPException(status_code=500, detail="MongoDB baƒülantƒ±sƒ± kurulamadƒ±")
        try:
            res = col.delete_one({"_id": ObjectId(id)})
        except Exception:
            client.close()
            raise HTTPException(status_code=400, detail="Ge√ßersiz link id")
        client.close()
        if res.deleted_count == 0:
            raise HTTPException(status_code=404, detail="Kayƒ±t bulunamadƒ±")
        return {"success": True, "deleted": res.deleted_count}
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Hata: {str(e)}")


@app.delete("/api/mongo/links/by-kurum/{kurum_id}", tags=["Links"], summary="Kurumdaki t√ºm linkleri sil")
async def delete_links_by_kurum(kurum_id: str):
    """
    Verilen kurum_id i√ßin links koleksiyonundaki T√úM kayƒ±tlarƒ± siler.
    """
    try:
        client, col = _get_links_collection()
        if not client:
            raise HTTPException(status_code=500, detail="MongoDB baƒülantƒ±sƒ± kurulamadƒ±")
        try:
            kurum_oid = ObjectId(kurum_id)
        except Exception:
            client.close()
            raise HTTPException(status_code=400, detail="'kurum_id' ge√ßersiz ObjectId")
        res = col.delete_many({"kurum_id": kurum_oid})
        deleted = res.deleted_count if res else 0
        client.close()
        return {"success": True, "deleted_count": deleted}
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Hata: {str(e)}")

# Kurum Duyuru CRUD Endpoints
# ==============================

@app.get("/api/mongo/kurum-duyuru", tags=["Kurum Duyuru"], summary="Kurum duyurularƒ± listele")
async def list_kurum_duyuru(limit: int = 100, offset: int = 0):
    try:
        client, col = _get_kurum_duyuru_collection()
        if not client:
            raise HTTPException(status_code=500, detail="MongoDB baƒülantƒ±sƒ± kurulamadƒ±")
        if limit <= 0:
            limit = 100
        if limit > 1000:
            limit = 1000
        if offset < 0:
            offset = 0
        total = col.count_documents({})
        cursor = col.find({}).skip(offset).limit(limit).sort("_id", -1)
        items = []
        for d in cursor:
            d["_id"] = str(d["_id"])
            if "kurum_id" in d and isinstance(d["kurum_id"], ObjectId):
                d["kurum_id"] = str(d["kurum_id"])
            items.append(d)
        client.close()
        return {"success": True, "total": total, "limit": limit, "offset": offset, "data": items}
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Hata: {str(e)}")


@app.post("/api/mongo/kurum-duyuru", tags=["Kurum Duyuru"], summary="Kurum duyurusu olu≈ütur")
async def create_kurum_duyuru(body: Dict[str, Any]):
    try:
        client, col = _get_kurum_duyuru_collection()
        if not client:
            raise HTTPException(status_code=500, detail="MongoDB baƒülantƒ±sƒ± kurulamadƒ±")
        data = body or {}
        kurum_id = (data.get("kurum_id") or "").strip()
        duyuru_linki = (data.get("duyuru_linki") or "").strip()
        if not kurum_id:
            client.close()
            raise HTTPException(status_code=400, detail="'kurum_id' zorunlu")
        if not duyuru_linki:
            client.close()
            raise HTTPException(status_code=400, detail="'duyuru_linki' zorunlu")
        # Basit URL kontrol√º
        if not re.match(r"^https?://", duyuru_linki):
            client.close()
            raise HTTPException(status_code=400, detail="'duyuru_linki' ge√ßerli bir URL olmalƒ±")
        # kurum_id ObjectId'e d√∂n√º≈üt√ºr
        try:
            kurum_oid = ObjectId(kurum_id)
        except Exception:
            client.close()
            raise HTTPException(status_code=400, detail="'kurum_id' ge√ßersiz ObjectId")
        doc = {
            "kurum_id": kurum_oid,
            "duyuru_linki": duyuru_linki,
            "olusturulma_tarihi": datetime.now().isoformat()
        }
        res = col.insert_one(doc)
        new_id = str(res.inserted_id)
        client.close()
        return {"success": True, "id": new_id}
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Hata: {str(e)}")


@app.get("/api/mongo/kurum-duyuru/{id}", tags=["Kurum Duyuru"], summary="Kurum duyurusu getir")
async def get_kurum_duyuru(id: str):
    try:
        client, col = _get_kurum_duyuru_collection()
        if not client:
            raise HTTPException(status_code=500, detail="MongoDB baƒülantƒ±sƒ± kurulamadƒ±")
        try:
            d = col.find_one({"_id": ObjectId(id)})
        except Exception:
            client.close()
            raise HTTPException(status_code=400, detail="Ge√ßersiz duyuru id")
        if not d:
            client.close()
            raise HTTPException(status_code=404, detail="Duyuru bulunamadƒ±")
        d["_id"] = str(d["_id"])
        if "kurum_id" in d and isinstance(d["kurum_id"], ObjectId):
            d["kurum_id"] = str(d["kurum_id"])
        client.close()
        return {"success": True, "data": d}
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Hata: {str(e)}")


@app.put("/api/mongo/kurum-duyuru/{id}", tags=["Kurum Duyuru"], summary="Kurum duyurusu g√ºncelle")
async def update_kurum_duyuru(id: str, body: Dict[str, Any]):
    try:
        client, col = _get_kurum_duyuru_collection()
        if not client:
            raise HTTPException(status_code=500, detail="MongoDB baƒülantƒ±sƒ± kurulamadƒ±")
        update_data: Dict[str, Any] = {}
        data = body or {}
        if "kurum_id" in data and data["kurum_id"] is not None:
            try:
                update_data["kurum_id"] = ObjectId(str(data["kurum_id"]).strip())
            except Exception:
                client.close()
                raise HTTPException(status_code=400, detail="'kurum_id' ge√ßersiz ObjectId")
        if "duyuru_linki" in data and data["duyuru_linki"] is not None:
            link = str(data["duyuru_linki"]).strip()
            if not link:
                client.close()
                raise HTTPException(status_code=400, detail="'duyuru_linki' bo≈ü olamaz")
            if not re.match(r"^https?://", link):
                client.close()
                raise HTTPException(status_code=400, detail="'duyuru_linki' ge√ßerli bir URL olmalƒ±")
            update_data["duyuru_linki"] = link
        if not update_data:
            client.close()
            return {"success": True, "modified": 0, "message": "G√ºncellenecek alan yok"}
        try:
            res = col.update_one({"_id": ObjectId(id)}, {"$set": update_data})
        except Exception:
            client.close()
            raise HTTPException(status_code=400, detail="Ge√ßersiz duyuru id")
        client.close()
        if res.matched_count == 0:
            raise HTTPException(status_code=404, detail="Duyuru bulunamadƒ±")
        return {"success": True, "modified": res.modified_count}
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Hata: {str(e)}")


@app.delete("/api/mongo/kurum-duyuru/{id}", tags=["Kurum Duyuru"], summary="Kurum duyurusu sil")
async def delete_kurum_duyuru(id: str):
    try:
        client, col = _get_kurum_duyuru_collection()
        if not client:
            raise HTTPException(status_code=500, detail="MongoDB baƒülantƒ±sƒ± kurulamadƒ±")
        try:
            res = col.delete_one({"_id": ObjectId(id)})
        except Exception:
            client.close()
            raise HTTPException(status_code=400, detail="Ge√ßersiz duyuru id")
        client.close()
        if res.deleted_count == 0:
            raise HTTPException(status_code=404, detail="Duyuru bulunamadƒ±")
        return {"success": True, "deleted": res.deleted_count}
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Hata: {str(e)}")


# ==============================
# Proxy Koleksiyonu Yardƒ±mcƒ± Fonksiyonlarƒ±
# ==============================

def _get_proxy_collection():
    """Proxy koleksiyonunu d√∂ner"""
    client = _get_mongodb_client()
    if not client:
        return None, None
    database_name = os.getenv("MONGODB_DATABASE", "mevzuatgpt")
    db = client[database_name]
    return client, db["proxies"]


def get_proxy_from_db() -> Optional[Dict[str, str]]:
    """
    MongoDB'den aktif proxy bilgilerini √ßeker.
    Returns: {'http': 'http://user:pass@host:port', 'https': 'http://user:pass@host:port'} veya None
    """
    try:
        client, col = _get_proxy_collection()
        if not client:
            return None
        
        # Aktif proxy'yi bul (is_active=True olan ilk kayƒ±t)
        proxy_doc = col.find_one({"is_active": True}, sort=[("created_at", -1)])
        client.close()
        
        if not proxy_doc:
            return None
        
        host = proxy_doc.get("host", "").strip()
        port = proxy_doc.get("port", "").strip()
        username = proxy_doc.get("username", "").strip()
        password = proxy_doc.get("password", "").strip()
        
        if not host or not port:
            return None
        
        # Proxy URL'ini olu≈ütur
        if username and password:
            proxy_auth = f"{username}:{password}"
            proxy_url = f"{proxy_auth}@{host}:{port}"
        else:
            proxy_url = f"{host}:{port}"
        
        return {
            'http': f'http://{proxy_url}',
            'https': f'http://{proxy_url}'
        }
    except Exception as e:
        print(f"‚ö†Ô∏è Proxy bilgisi √ßekilemedi: {str(e)}")
        return None


# ==============================
# Proxy Koleksiyonu CRUD Endpoints
# ==============================

@app.get("/api/mongo/proxies", tags=["Proxy"], summary="Proxy listele")
async def list_proxies(limit: int = 100, offset: int = 0):
    try:
        client, col = _get_proxy_collection()
        if not client:
            raise HTTPException(status_code=500, detail="MongoDB baƒülantƒ±sƒ± kurulamadƒ±")
        
        # Password'u gizle
        cursor = col.find().sort("created_at", -1).skip(offset).limit(limit)
        proxies = []
        for doc in cursor:
            proxy_data = {
                "id": str(doc["_id"]),
                "host": doc.get("host", ""),
                "port": doc.get("port", ""),
                "username": doc.get("username", ""),
                "password": "***" if doc.get("password") else "",  # Password'u gizle
                "is_active": doc.get("is_active", False),
                "created_at": doc.get("created_at", ""),
                "updated_at": doc.get("updated_at", "")
            }
            proxies.append(proxy_data)
        
        total = col.count_documents({})
        client.close()
        return {"success": True, "total": total, "data": proxies}
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Hata: {str(e)}")


@app.post("/api/mongo/proxies", tags=["Proxy"], summary="Proxy olu≈ütur")
async def create_proxy(body: Dict[str, Any]):
    try:
        client, col = _get_proxy_collection()
        if not client:
            raise HTTPException(status_code=500, detail="MongoDB baƒülantƒ±sƒ± kurulamadƒ±")
        
        data = body or {}
        host = (data.get("host") or "").strip()
        port = (data.get("port") or "").strip()
        username = (data.get("username") or "").strip()
        password = (data.get("password") or "").strip()
        is_active = data.get("is_active", True)
        
        if not host or not port:
            client.close()
            raise HTTPException(status_code=400, detail="'host' ve 'port' zorunludur")
        
        # Port'un sayƒ±sal olup olmadƒ±ƒüƒ±nƒ± kontrol et
        try:
            port_int = int(port)
            if port_int < 1 or port_int > 65535:
                client.close()
                raise HTTPException(status_code=400, detail="Port 1-65535 arasƒ±nda olmalƒ±dƒ±r")
        except ValueError:
            client.close()
            raise HTTPException(status_code=400, detail="Port ge√ßerli bir sayƒ± olmalƒ±dƒ±r")
        
        # Eƒüer yeni proxy aktif yapƒ±lƒ±yorsa, diƒüer aktif proxy'leri pasif yap
        if is_active:
            col.update_many({"is_active": True}, {"$set": {"is_active": False, "updated_at": datetime.now().isoformat()}})
        
        doc = {
            "host": host,
            "port": port,
            "username": username,
            "password": password,
            "is_active": is_active,
            "created_at": datetime.now().isoformat(),
            "updated_at": datetime.now().isoformat()
        }
        
        res = col.insert_one(doc)
        new_id = str(res.inserted_id)
        client.close()
        return {"success": True, "id": new_id}
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Hata: {str(e)}")


@app.get("/api/mongo/proxies/{id}", tags=["Proxy"], summary="Proxy getir")
async def get_proxy(id: str):
    try:
        client, col = _get_proxy_collection()
        if not client:
            raise HTTPException(status_code=500, detail="MongoDB baƒülantƒ±sƒ± kurulamadƒ±")
        
        try:
            doc = col.find_one({"_id": ObjectId(id)})
        except Exception:
            client.close()
            raise HTTPException(status_code=400, detail="Ge√ßersiz proxy id")
        
        client.close()
        if not doc:
            raise HTTPException(status_code=404, detail="Proxy bulunamadƒ±")
        
        return {
            "success": True,
            "data": {
                "id": str(doc["_id"]),
                "host": doc.get("host", ""),
                "port": doc.get("port", ""),
                "username": doc.get("username", ""),
                "password": "***" if doc.get("password") else "",  # Password'u gizle
                "is_active": doc.get("is_active", False),
                "created_at": doc.get("created_at", ""),
                "updated_at": doc.get("updated_at", "")
            }
        }
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Hata: {str(e)}")


@app.put("/api/mongo/proxies/{id}", tags=["Proxy"], summary="Proxy g√ºncelle")
async def update_proxy(id: str, body: Dict[str, Any]):
    try:
        client, col = _get_proxy_collection()
        if not client:
            raise HTTPException(status_code=500, detail="MongoDB baƒülantƒ±sƒ± kurulamadƒ±")
        
        data = body or {}
        update_data = {"updated_at": datetime.now().isoformat()}
        
        if "host" in data:
            host = (data.get("host") or "").strip()
            if not host:
                client.close()
                raise HTTPException(status_code=400, detail="'host' bo≈ü olamaz")
            update_data["host"] = host
        
        if "port" in data:
            port = (data.get("port") or "").strip()
            if not port:
                client.close()
                raise HTTPException(status_code=400, detail="'port' bo≈ü olamaz")
            try:
                port_int = int(port)
                if port_int < 1 or port_int > 65535:
                    client.close()
                    raise HTTPException(status_code=400, detail="Port 1-65535 arasƒ±nda olmalƒ±dƒ±r")
            except ValueError:
                client.close()
                raise HTTPException(status_code=400, detail="Port ge√ßerli bir sayƒ± olmalƒ±dƒ±r")
            update_data["port"] = port
        
        if "username" in data:
            update_data["username"] = (data.get("username") or "").strip()
        
        if "password" in data:
            update_data["password"] = (data.get("password") or "").strip()
        
        if "is_active" in data:
            is_active = data.get("is_active", False)
            # Eƒüer proxy aktif yapƒ±lƒ±yorsa, diƒüer aktif proxy'leri pasif yap
            if is_active:
                col.update_many(
                    {"is_active": True, "_id": {"$ne": ObjectId(id)}},
                    {"$set": {"is_active": False, "updated_at": datetime.now().isoformat()}}
                )
            update_data["is_active"] = is_active
        
        if not update_data or len(update_data) == 1:  # Sadece updated_at varsa
            client.close()
            return {"success": True, "modified": 0, "message": "G√ºncellenecek alan yok"}
        
        try:
            res = col.update_one({"_id": ObjectId(id)}, {"$set": update_data})
        except Exception:
            client.close()
            raise HTTPException(status_code=400, detail="Ge√ßersiz proxy id")
        
        client.close()
        if res.matched_count == 0:
            raise HTTPException(status_code=404, detail="Proxy bulunamadƒ±")
        return {"success": True, "modified": res.modified_count}
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Hata: {str(e)}")


@app.delete("/api/mongo/proxies/{id}", tags=["Proxy"], summary="Proxy sil")
async def delete_proxy(id: str):
    try:
        client, col = _get_proxy_collection()
        if not client:
            raise HTTPException(status_code=500, detail="MongoDB baƒülantƒ±sƒ± kurulamadƒ±")
        
        try:
            res = col.delete_one({"_id": ObjectId(id)})
        except Exception:
            client.close()
            raise HTTPException(status_code=400, detail="Ge√ßersiz proxy id")
        
        client.close()
        if res.deleted_count == 0:
            raise HTTPException(status_code=404, detail="Proxy bulunamadƒ±")
        return {"success": True, "deleted": res.deleted_count}
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Hata: {str(e)}")


@app.post("/api/mongo/proxies/test", tags=["Proxy"], summary="Proxy baƒülantƒ± testi (KAYSƒ∞S)")
async def test_proxy_connection(body: Dict[str, Any]):
    """
    Proxy baƒülantƒ±sƒ±nƒ± KAYSƒ∞S sitesine test eder.
    curl_cffi kullanarak Chrome tarayƒ±cƒ±sƒ±nƒ± taklit eder ve WAF engellemelerini a≈üar.
    
    Args:
        body: {"id": "proxy_id", "detsis": "22620739"} (detsis opsiyonel, varsayƒ±lan: 22620739 - SGK)
    
    Returns:
        Test sonu√ßlarƒ± (IP bilgisi, baƒülantƒ± durumu, hata mesajlarƒ±)
    """
    try:
        # Body'den proxy ID'yi al
        if not body or not body.get("id"):
            raise HTTPException(status_code=400, detail="Body'de 'id' alanƒ± zorunludur")
        
        proxy_id = str(body.get("id")).strip()
        if not proxy_id:
            raise HTTPException(status_code=400, detail="Proxy ID bo≈ü olamaz")
        
        # Proxy bilgilerini MongoDB'den √ßek
        client, col = _get_proxy_collection()
        if not client:
            raise HTTPException(status_code=500, detail="MongoDB baƒülantƒ±sƒ± kurulamadƒ±")
        
        try:
            proxy_doc = col.find_one({"_id": ObjectId(proxy_id)})
        except Exception:
            client.close()
            raise HTTPException(status_code=400, detail="Ge√ßersiz proxy id formatƒ±")
        
        client.close()
        
        if not proxy_doc:
            raise HTTPException(status_code=404, detail=f"Proxy bulunamadƒ± (ID: {proxy_id})")
        
        # Proxy bilgilerini hazƒ±rla
        host = proxy_doc.get("host", "").strip()
        port = proxy_doc.get("port", "").strip()
        username = proxy_doc.get("username", "").strip()
        password = proxy_doc.get("password", "").strip()
        
        if not host or not port:
            raise HTTPException(status_code=400, detail="Proxy bilgileri eksik (host veya port)")
        
        # Proxy URL'ini olu≈ütur
        if username and password:
            proxy_auth = f"{username}:{password}"
            proxy_url = f"{proxy_auth}@{host}:{port}"
        else:
            proxy_url = f"{host}:{port}"
        
        proxies = {
            'http': f'http://{proxy_url}',
            'https': f'http://{proxy_url}'
        }
        
        # DETSIS numarasƒ±nƒ± al (varsayƒ±lan: 22620739 - SGK)
        detsis = "22620739"
        if body.get("detsis"):
            detsis = str(body.get("detsis")).strip()
        
        test_url = f"https://kms.kaysis.gov.tr/Home/Kurum/{detsis}"
        
        result = {
            "success": False,
            "proxy_id": proxy_id,
            "proxy_host": host,
            "proxy_port": port,
            "test_url": test_url,
            "detsis": detsis,
            "ip_info": None,
            "connection_status": None,
            "http_status": None,
            "response_size": None,
            "error": None,
            "curl_cffi_available": CURL_CFFI_AVAILABLE
        }
        
        # 1. IP kontrol√º
        try:
            print(f"üåç Proxy IP adresi kontrol ediliyor... (Proxy ID: {proxy_id})")
            ip_response = requests.get(
                'https://ipv4.icanhazip.com',
                proxies=proxies,
                timeout=10,
                impersonate="chrome110" if CURL_CFFI_AVAILABLE else None
            )
            ip_address = ip_response.text.strip()
            
            # IP lokasyon bilgisini al
            try:
                geo_response = requests.get(
                    f'http://ip-api.com/json/{ip_address}?fields=status,country,countryCode,city,query',
                    proxies=proxies,
                    timeout=10,
                    impersonate="chrome110" if CURL_CFFI_AVAILABLE else None
                )
                geo_data = geo_response.json()
                
                if geo_data.get('status') == 'success':
                    result["ip_info"] = {
                        "ip": ip_address,
                        "country": geo_data.get('country', 'Bilinmiyor'),
                        "country_code": geo_data.get('countryCode', 'Bilinmiyor'),
                        "city": geo_data.get('city', 'Bilinmiyor'),
                        "is_turkey": geo_data.get('countryCode') == 'TR'
                    }
                else:
                    result["ip_info"] = {"ip": ip_address}
            except Exception as e:
                result["ip_info"] = {"ip": ip_address, "error": str(e)}
        except Exception as e:
            result["ip_info"] = {"error": f"IP kontrol√º ba≈üarƒ±sƒ±z: {str(e)}"}
        
        # 2. KAYSƒ∞S baƒülantƒ± testi
        try:
            print(f"üåê KAYSƒ∞S sitesine baƒülanƒ±lƒ±yor... (Proxy ID: {proxy_id})")
            
            # Ger√ßek bir Chrome tarayƒ±cƒ±sƒ±nƒ±n g√∂nderdiƒüi t√ºm header'lar
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',
                'Accept-Language': 'tr-TR,tr;q=0.9,en-US;q=0.8,en;q=0.7',
                'Accept-Encoding': 'gzip, deflate, br',
                'Referer': 'https://www.google.com/',
                'Sec-Ch-Ua': '"Not_A Brand";v="8", "Chromium";v="120", "Google Chrome";v="120"',
                'Sec-Ch-Ua-Mobile': '?0',
                'Sec-Ch-Ua-Platform': '"Windows"',
                'Sec-Fetch-Dest': 'document',
                'Sec-Fetch-Mode': 'navigate',
                'Sec-Fetch-Site': 'cross-site',
                'Sec-Fetch-User': '?1',
                'Upgrade-Insecure-Requests': '1',
                'Connection': 'keep-alive',
                'Cache-Control': 'max-age=0'
            }
            
            # curl_cffi ile Chrome taklidi yap (eƒüer mevcut ise)
            if CURL_CFFI_AVAILABLE:
                response = requests.get(
                    test_url,
                    headers=headers,
                    proxies=proxies,
                    timeout=1200,  # 20 dakika timeout
                    impersonate="chrome110"  # Chrome 110 TLS fingerprint
                )
            else:
                response = requests.get(test_url, headers=headers, timeout=1200, proxies=proxies)  # 20 dakika timeout
            
            result["http_status"] = response.status_code
            result["response_size"] = len(response.content)
            
            if response.status_code == 200:
                result["success"] = True
                result["connection_status"] = "success"
                
                # HTML i√ßeriƒüinde ba≈üarƒ±lƒ± y√ºkleme i≈üaretleri kontrol et
                content = response.text.lower()
                if 'accordion' in content or 'panel' in content or 'kurum' in content:
                    result["content_check"] = "KAYSƒ∞S yapƒ±sƒ± tespit edildi"
                else:
                    result["content_check"] = "Sayfa y√ºklendi ancak beklenen i√ßerik bulunamadƒ±"
            else:
                result["connection_status"] = "failed"
                result["error"] = f"HTTP {response.status_code}: {response.text[:200] if response.text else 'Bo≈ü yanƒ±t'}"
                
        except requests.exceptions.ProxyError as e:
            result["connection_status"] = "proxy_error"
            result["error"] = f"Proxy hatasƒ±: {str(e)}"
        except requests.exceptions.Timeout:
            result["connection_status"] = "timeout"
            result["error"] = "Zaman a≈üƒ±mƒ±: Baƒülantƒ± 30 saniye i√ßinde tamamlanamadƒ±"
        except requests.exceptions.ConnectionError as e:
            result["connection_status"] = "connection_error"
            result["error"] = f"Baƒülantƒ± hatasƒ±: {str(e)}"
        except Exception as e:
            result["connection_status"] = "error"
            result["error"] = f"Beklenmeyen hata: {str(e)}"
        
        return result
        
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Test sƒ±rasƒ±nda hata: {str(e)}")

@app.post("/api/mongo/kurumlar", tags=["Kurumlar"], summary="Kurum olu≈ütur")
async def create_kurum(
    kurum_adi: str = Form(...),
    aciklama: Optional[str] = Form(None),
    detsis: Optional[str] = Form(None),
    logo: Optional[UploadFile] = File(None)
):
    """
    Yeni kurum olu≈üturur (multipart/form-data).
    - kurum_adi: Zorunlu
    - aciklama: Opsiyonel
    - detsis: Opsiyonel (DETSIS numarasƒ±)
    - logo: Opsiyonel (PNG, JPG, JPEG, SVG, GIF, WEBP)
    """
    try:
        client, col = _get_kurumlar_collection()
        if not client:
            raise HTTPException(status_code=500, detail="MongoDB baƒülantƒ±sƒ± kurulamadƒ±")
        
        if not kurum_adi or not str(kurum_adi).strip():
            client.close()
            raise HTTPException(status_code=400, detail="'kurum_adi' zorunlu")
        
        # Logo varsa y√ºkle
        logo_url = None
        if logo:
            # Dosya formatƒ±nƒ± kontrol et
            allowed_extensions = {'.png', '.jpg', '.jpeg', '.svg', '.gif', '.webp'}
            file_extension = Path(logo.filename or '').suffix.lower()
            
            if file_extension not in allowed_extensions:
                client.close()
                raise HTTPException(
                    status_code=400,
                    detail=f"Desteklenmeyen dosya formatƒ±. ƒ∞zin verilen formatlar: {', '.join(allowed_extensions)}"
                )
            
            # Content type'ƒ± belirle
            content_type_map = {
                '.png': 'image/png',
                '.jpg': 'image/jpeg',
                '.jpeg': 'image/jpeg',
                '.svg': 'image/svg+xml',
                '.gif': 'image/gif',
                '.webp': 'image/webp'
            }
            content_type = content_type_map.get(file_extension, logo.content_type or 'image/png')
            
            # Dosya i√ßeriƒüini oku
            file_data = await logo.read()
            
            # Dosya adƒ±nƒ± olu≈ütur
            safe_filename = _transliterate_turkish(kurum_adi)
            safe_filename = re.sub(r'[^a-zA-Z0-9\s-]', '', safe_filename).strip()
            safe_filename = re.sub(r'\s+', '_', safe_filename)
            safe_filename = re.sub(r'_+', '_', safe_filename)
            
            # Ge√ßici ID olu≈ütur (hen√ºz MongoDB'de yok)
            temp_id = str(ObjectId())
            logo_filename = f"{safe_filename}_{temp_id}{file_extension}"
            
            # Bunny.net'e y√ºkle
            logo_url = _upload_logo_to_bunny(file_data, logo_filename, content_type)
            
            if not logo_url:
                client.close()
                raise HTTPException(status_code=500, detail="Logo Bunny.net'e y√ºklenemedi")
        
        # MongoDB'ye kaydet
        data = {
            "kurum_adi": kurum_adi.strip(),
            "olusturulma_tarihi": datetime.now().isoformat()
        }
        
        if aciklama:
            data["aciklama"] = aciklama.strip()
        
        if detsis:
            data["detsis"] = detsis.strip()
        
        if logo_url:
            data["kurum_logo"] = logo_url
        
        res = col.insert_one(data)
        new_id = str(res.inserted_id)
        client.close()
        
        return {
            "success": True,
            "id": new_id,
            "logo_url": logo_url
        }
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Hata: {str(e)}")


@app.get("/api/mongo/kurumlar/{id}", tags=["Kurumlar"], summary="Kurum getir")
async def get_kurum(id: str):
    try:
        client, col = _get_kurumlar_collection()
        if not client:
            raise HTTPException(status_code=500, detail="MongoDB baƒülantƒ±sƒ± kurulamadƒ±")
        try:
            d = col.find_one({"_id": ObjectId(id)})
        except Exception:
            client.close()
            raise HTTPException(status_code=400, detail="Ge√ßersiz kurum id")
        if not d:
            client.close()
            raise HTTPException(status_code=404, detail="Kurum bulunamadƒ±")
        d["_id"] = str(d["_id"])
        client.close()
        return {"success": True, "data": d}
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Hata: {str(e)}")


@app.put("/api/mongo/kurumlar/{id}", tags=["Kurumlar"], summary="Kurum g√ºncelle (logo destekli)")
async def update_kurum(
    id: str,
    kurum_adi: Optional[str] = Form(None),
    aciklama: Optional[str] = Form(None),
    detsis: Optional[str] = Form(None),
    logo: Optional[UploadFile] = File(None)
):
    """
    Kurum bilgilerini g√ºnceller (multipart/form-data).
    - kurum_adi: Opsiyonel (g√∂nderilirse g√ºncellenir)
    - aciklama: Opsiyonel (g√∂nderilirse g√ºncellenir)
    - detsis: Opsiyonel (g√∂nderilirse g√ºncellenir - DETSIS numarasƒ±)
    - logo: Opsiyonel (g√∂nderilirse y√ºklenir ve g√ºncellenir) (PNG, JPG, JPEG, SVG, GIF, WEBP)
    """
    try:
        client, col = _get_kurumlar_collection()
        if not client:
            raise HTTPException(status_code=500, detail="MongoDB baƒülantƒ±sƒ± kurulamadƒ±")
        
        # Kurum var mƒ± kontrol et
        try:
            kurum = col.find_one({"_id": ObjectId(id)})
        except Exception:
            client.close()
            raise HTTPException(status_code=400, detail="Ge√ßersiz kurum id")
        
        if not kurum:
            client.close()
            raise HTTPException(status_code=404, detail="Kurum bulunamadƒ±")
        
        update_data: Dict[str, Any] = {}
        
        # Logo varsa y√ºkle
        if logo:
            # Dosya formatƒ±nƒ± kontrol et
            allowed_extensions = {'.png', '.jpg', '.jpeg', '.svg', '.gif', '.webp'}
            file_extension = Path(logo.filename or '').suffix.lower()
            
            if file_extension not in allowed_extensions:
                client.close()
                raise HTTPException(
                    status_code=400,
                    detail=f"Desteklenmeyen dosya formatƒ±. ƒ∞zin verilen formatlar: {', '.join(allowed_extensions)}"
                )
            
            # Content type'ƒ± belirle
            content_type_map = {
                '.png': 'image/png',
                '.jpg': 'image/jpeg',
                '.jpeg': 'image/jpeg',
                '.svg': 'image/svg+xml',
                '.gif': 'image/gif',
                '.webp': 'image/webp'
            }
            content_type = content_type_map.get(file_extension, logo.content_type or 'image/png')
            
            # Dosya i√ßeriƒüini oku
            file_data = await logo.read()
            
            # Dosya adƒ±nƒ± olu≈ütur (kurum adƒ±ndan veya mevcut kurum adƒ±ndan)
            kurum_adi_for_filename = kurum_adi.strip() if kurum_adi else kurum.get('kurum_adi', 'kurum')
            safe_filename = _transliterate_turkish(kurum_adi_for_filename)
            safe_filename = re.sub(r'[^a-zA-Z0-9\s-]', '', safe_filename).strip()
            safe_filename = re.sub(r'\s+', '_', safe_filename)
            safe_filename = re.sub(r'_+', '_', safe_filename)
            logo_filename = f"{safe_filename}_{id}{file_extension}"
            
            # Bunny.net'e y√ºkle
            logo_url = _upload_logo_to_bunny(file_data, logo_filename, content_type)
            
            if not logo_url:
                client.close()
                raise HTTPException(status_code=500, detail="Logo Bunny.net'e y√ºklenemedi")
            
            update_data["kurum_logo"] = logo_url
        
        # Diƒüer alanlarƒ± g√ºncelle
        if kurum_adi is not None:
            update_data["kurum_adi"] = kurum_adi.strip()
        
        if aciklama is not None:
            update_data["aciklama"] = aciklama.strip()
        
        if detsis is not None:
            update_data["detsis"] = detsis.strip()
        
        if not update_data:
            client.close()
            return {"success": True, "modified": 0, "message": "G√ºncellenecek alan yok"}
        
        # MongoDB'yi g√ºncelle
        try:
            res = col.update_one({"_id": ObjectId(id)}, {"$set": update_data})
        except Exception:
            client.close()
            raise HTTPException(status_code=400, detail="Ge√ßersiz kurum id")
        
        client.close()
        
        if res.matched_count == 0:
            raise HTTPException(status_code=404, detail="Kurum bulunamadƒ±")
        
        return {
            "success": True,
            "modified": res.modified_count,
            "logo_url": update_data.get("kurum_logo") if "kurum_logo" in update_data else None
        }
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Hata: {str(e)}")


@app.delete("/api/mongo/kurumlar/{id}", tags=["Kurumlar"], summary="Kurum sil")
async def delete_kurum(id: str):
    try:
        client, col = _get_kurumlar_collection()
        if not client:
            raise HTTPException(status_code=500, detail="MongoDB baƒülantƒ±sƒ± kurulamadƒ±")
        try:
            res = col.delete_one({"_id": ObjectId(id)})
        except Exception:
            client.close()
            raise HTTPException(status_code=400, detail="Ge√ßersiz kurum id")
        client.close()
        if res.deleted_count == 0:
            raise HTTPException(status_code=404, detail="Kurum bulunamadƒ±")
        return {"success": True, "deleted": res.deleted_count}
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Hata: {str(e)}")


def _get_deepseek_api_key() -> Optional[str]:
    # 1) Env
    env_key = os.getenv("DEEPSEEK_API_KEY", "").strip()
    if env_key:
        return env_key
    # 2) config.json
    cfg = _load_config()
    if cfg:
        cfg_key = (cfg.get("deepseek_api_key") or "").strip()
        if cfg_key:
            return cfg_key
    # 3) Replit local state (opsiyonel)
    try:
        replit_state = Path(".local/state/replit/agent/filesystem/filesystem_state.json")
        if replit_state.exists():
            content = replit_state.read_text(encoding='utf-8', errors='ignore')
            # Basit bir anahtar aramasƒ±
            import re as _re
            m = _re.search(r"sk-[A-Za-z0-9]{32,}", content)
            if m:
                return m.group(0)
    except Exception:
        pass
    return None


def _login_with_config(cfg: Dict[str, Any]) -> Optional[str]:
    try:
        api_base_url = cfg.get("api_base_url")
        email = cfg.get("admin_email")
        password = cfg.get("admin_password")
        if not all([api_base_url, email, password]):
            return None
        # API isteklerinde proxy kullanƒ±lmƒ±yor
        
        login_url = f"{api_base_url.rstrip('/')}/api/auth/login"
        resp = requests.post(login_url, headers={"Content-Type": "application/json"}, json={
            "email": email,
            "password": password
        }, timeout=1200)  # 20 dakika timeout (MevzuatGPT y√ºkleme s√ºrecinin par√ßasƒ±)
        if resp.status_code == 200:
            data = resp.json()
            return data.get("access_token")
        return None
    except Exception:
        return None


def _transliterate_turkish(text: str) -> str:
    """T√ºrk√ße karakterleri ƒ∞ngilizce kar≈üƒ±lƒ±klarƒ±na √ßevirir (kaldƒ±rmaz)"""
    if not text:
        return ""
    
    # T√ºrk√ße karakterleri ƒ∞ngilizce kar≈üƒ±lƒ±klarƒ±na √ßevir
    char_map = {
        '√ß': 'c', 'ƒü': 'g', 'ƒ±': 'i', '√∂': 'o', '≈ü': 's', '√º': 'u',
        '√á': 'C', 'ƒû': 'G', 'ƒ∞': 'I', '√ñ': 'O', '≈û': 'S', '√ú': 'U'
    }
    
    result = text
    for tr_char, en_char in char_map.items():
        result = result.replace(tr_char, en_char)
    
    return result


def _create_url_slug(text: str) -> str:
    """URL-friendly slug olu≈üturur (alt tire ile, sƒ±nƒ±rsƒ±z)"""
    if not text:
        return "pdf_document"
    
    # T√ºrk√ße karakterleri ƒ∞ngilizce kar≈üƒ±lƒ±klarƒ±na √ßevir
    slug = _transliterate_turkish(text)
    
    # Unicode normalize
    slug = unicodedata.normalize('NFKD', slug)
    
    # K√º√ß√ºk harf yap
    slug = slug.lower()
    
    # Sadece harfler, rakamlar ve bo≈üluk
    slug = re.sub(r'[^a-z0-9\s]', '', slug)
    
    # √áoklu bo≈üluklarƒ± alt tire ile deƒüi≈ütir
    slug = re.sub(r'\s+', '_', slug)
    
    # √áoklu alt tireleri tek alt tire yap
    slug = re.sub(r'_+', '_', slug)
    
    # Ba≈üƒ±ndaki ve sonundaki alt tireleri kaldƒ±r
    slug = slug.strip('_')
    
    # Kƒ±saltma yok, tam uzunluk
    
    return slug or "pdf_document"


def _upload_to_bunny(pdf_path: str, filename: str) -> Optional[str]:
    """PDF'i Bunny.net'e y√ºkler ve public URL d√∂ner"""
    try:
        print(f"üì§ [Bunny.net Upload] Ba≈ülatƒ±lƒ±yor...")
        print(f"   üìÑ Dosya: {pdf_path}")
        print(f"   üìù Filename: {filename}")
        
        api_key = os.getenv("BUNNY_STORAGE_API_KEY")
        storage_zone = os.getenv("BUNNY_STORAGE_ZONE", "mevzuatgpt")
        storage_region = os.getenv("BUNNY_STORAGE_REGION", "storage.bunnycdn.com")
        storage_endpoint = os.getenv("BUNNY_STORAGE_ENDPOINT", "https://cdn.mevzuatgpt.org")
        storage_folder = os.getenv("BUNNY_STORAGE_FOLDER", "portal")
        
        print(f"   üåê Storage Zone: {storage_zone}")
        print(f"   üåê Storage Region: {storage_region}")
        print(f"   üìÇ Storage Folder: {storage_folder}")
        
        if not api_key:
            print("‚ùå [Bunny.net Upload] API anahtarƒ± bulunamadƒ±")
            return None
        
        # PDF dosyasƒ±nƒ± oku
        print(f"   üìñ PDF dosyasƒ± okunuyor...")
        with open(pdf_path, 'rb') as f:
            pdf_data = f.read()
        file_size = len(pdf_data)
        file_size_mb = round(file_size / (1024 * 1024), 2)
        print(f"   ‚úÖ Dosya okundu: {file_size:,} bytes ({file_size_mb} MB)")
        
        # URL-safe filename
        safe_filename = urllib.parse.quote(filename)
        upload_url = f"https://{storage_region}/{storage_zone}/{storage_folder}/{safe_filename}"
        print(f"   üåê Upload URL: {upload_url}")
        
        headers = {
            'AccessKey': api_key,
            'Content-Type': 'application/pdf',
            'User-Agent': 'SGK-Scraper-API/1.0'
        }
        
        print(f"   üöÄ Bunny.net'e y√ºkleme ba≈ülatƒ±lƒ±yor...")
        print(f"   ‚è±Ô∏è Timeout: 1200 saniye (20 dakika)")
        response = requests.put(upload_url, headers=headers, data=pdf_data, timeout=1200)  # 20 dakika timeout
        
        print(f"   üì° Response alƒ±ndƒ±")
        print(f"   üìä Status Code: {response.status_code}")
        print(f"   üìã Response headers: {dict(response.headers)}")
        
        if response.status_code == 201:
            public_url = f"{storage_endpoint}/{storage_folder}/{safe_filename}"
            print(f"‚úÖ [Bunny.net Upload] Ba≈üarƒ±lƒ±!")
            print(f"   üîó Public URL: {public_url}")
            return public_url
        else:
            print(f"‚ùå [Bunny.net Upload] Ba≈üarƒ±sƒ±z!")
            print(f"   üìù Response body (ilk 500 karakter): {response.text[:500]}")
            if len(response.text) > 500:
                print(f"      ... (toplam {len(response.text)} karakter)")
            return None
            
    except requests.exceptions.Timeout:
        print(f"‚ùå [Bunny.net Upload] Zaman a≈üƒ±mƒ± (20 dakika)")
        return None
    except requests.exceptions.RequestException as e:
        print(f"‚ùå [Bunny.net Upload] Aƒü hatasƒ±: {str(e)}")
        return None
    except Exception as e:
        print(f"‚ùå [Bunny.net Upload] Beklenmeyen hata: {str(e)}")
        import traceback
        print(f"   üìã Traceback: {traceback.format_exc()}")
        return None


def _upload_logo_to_bunny(file_data: bytes, filename: str, content_type: str) -> Optional[str]:
    """Logo/resim dosyasƒ±nƒ± Bunny.net'e y√ºkler ve public URL d√∂ner (referans koddaki mantƒ±k)"""
    try:
        api_key = os.getenv("BUNNY_STORAGE_API_KEY")
        storage_zone = os.getenv("BUNNY_STORAGE_ZONE", "mevzuatgpt")
        storage_region = os.getenv("BUNNY_STORAGE_REGION", "storage.bunnycdn.com")
        storage_endpoint = os.getenv("BUNNY_STORAGE_ENDPOINT", "https://cdn.mevzuatgpt.org")
        storage_folder = os.getenv("BUNNY_STORAGE_FOLDER", "portal")
        
        if not api_key:
            print("Bunny.net API anahtarƒ± bulunamadƒ±")
            return None
        
        # URL-safe filename
        safe_filename = urllib.parse.quote(filename)
        upload_url = f"https://{storage_region}/{storage_zone}/{storage_folder}/{safe_filename}"
        
        print(f"Logo y√ºkleniyor: {upload_url}")
        
        headers = {
            'AccessKey': api_key,
            'Content-Type': content_type,
            'User-Agent': 'SGK-Scraper-API/1.0'
        }
        
        # Upload file
        response = requests.put(upload_url, headers=headers, data=file_data, timeout=1200)  # 20 dakika timeout
        
        if response.status_code == 201:
            # Return public URL
            public_url = f"{storage_endpoint}/{storage_folder}/{safe_filename}"
            print("Logo ba≈üarƒ±yla Bunny.net'e y√ºklendi")
            return public_url
        else:
            print(f"Logo y√ºkleme hatasƒ±: {response.status_code} - {response.text}")
            return None
            
    except requests.exceptions.Timeout:
        print("Logo y√ºkleme zaman a≈üƒ±mƒ±na uƒüradƒ±")
        return None
    except requests.exceptions.RequestException as e:
        print(f"Logo y√ºkleme aƒü hatasƒ±: {str(e)}")
        return None
    except Exception as e:
        print(f"Beklenmeyen logo y√ºkleme hatasƒ±: {str(e)}")
        return None


def _delete_from_bunny(pdf_url: str) -> bool:
    """Bunny.net'ten PDF dosyasƒ±nƒ± siler"""
    try:
        if not pdf_url or not pdf_url.strip():
            print("‚ö†Ô∏è PDF URL bo≈ü, silme i≈ülemi atlandƒ±")
            return False
        
        api_key = os.getenv("BUNNY_STORAGE_API_KEY")
        storage_zone = os.getenv("BUNNY_STORAGE_ZONE", "mevzuatgpt")
        storage_region = os.getenv("BUNNY_STORAGE_REGION", "storage.bunnycdn.com")
        storage_endpoint = os.getenv("BUNNY_STORAGE_ENDPOINT", "https://cdn.mevzuatgpt.org")
        storage_folder = os.getenv("BUNNY_STORAGE_FOLDER", "portal")
        
        if not api_key:
            print("‚ö†Ô∏è Bunny.net API anahtarƒ± bulunamadƒ±, silme i≈ülemi atlandƒ±")
            return False
        
        # PDF URL'den dosya adƒ±nƒ± √ßƒ±kar
        # Format: https://cdn.mevzuatgpt.org/portal/filename.pdf
        # veya: https://cdn.mevzuatgpt.org/portal/filename%20with%20spaces.pdf
        try:
            # URL'den dosya adƒ±nƒ± al
            if storage_endpoint in pdf_url:
                # Endpoint'ten sonraki kƒ±smƒ± al
                file_path = pdf_url.split(storage_endpoint, 1)[1]
                # Ba≈üƒ±ndaki /portal/ kƒ±smƒ±nƒ± kaldƒ±r
                if file_path.startswith(f"/{storage_folder}/"):
                    filename = file_path[len(f"/{storage_folder}/"):]
                else:
                    filename = file_path.lstrip("/")
            else:
                # Farklƒ± format olabilir, direkt dosya adƒ±nƒ± √ßƒ±kar
                filename = os.path.basename(pdf_url)
            
            if not filename:
                print(f"‚ö†Ô∏è PDF URL'den dosya adƒ± √ßƒ±karƒ±lamadƒ±: {pdf_url}")
                return False
            
            # URL decode yap (eƒüer encoded ise)
            filename = urllib.parse.unquote(filename)
            
            # URL-safe filename (tekrar encode et)
            safe_filename = urllib.parse.quote(filename)
            
            # Delete URL olu≈ütur
            delete_url = f"https://{storage_region}/{storage_zone}/{storage_folder}/{safe_filename}"
            
            headers = {
                'AccessKey': api_key,
                'User-Agent': 'SGK-Scraper-API/1.0'
            }
            
            print(f"üóëÔ∏è Bunny.net'ten siliniyor: {filename}")
            response = requests.delete(delete_url, headers=headers, timeout=30)
            
            if response.status_code == 200 or response.status_code == 204:
                print(f"‚úÖ PDF Bunny.net'ten ba≈üarƒ±yla silindi: {filename}")
                return True
            elif response.status_code == 404:
                print(f"‚ö†Ô∏è PDF Bunny.net'te bulunamadƒ± (zaten silinmi≈ü olabilir): {filename}")
                return True  # Zaten yoksa ba≈üarƒ±lƒ± say
            else:
                print(f"‚ö†Ô∏è Bunny.net silme hatasƒ±: {response.status_code} - {response.text}")
                return False
                
        except Exception as parse_error:
            print(f"‚ö†Ô∏è PDF URL parse hatasƒ±: {str(parse_error)}")
            return False
            
    except Exception as e:
        print(f"‚ö†Ô∏è Bunny.net silme hatasƒ±: {str(e)}")
        return False


def _get_mongodb_client() -> Optional[MongoClient]:
    """MongoDB baƒülantƒ±sƒ± olu≈üturur"""
    try:
        connection_string = os.getenv("MONGODB_CONNECTION_STRING")
        if not connection_string:
            print("MongoDB baƒülantƒ± dizesi bulunamadƒ±")
            return None
        
        client = MongoClient(connection_string, serverSelectionTimeoutMS=5000)
        # Test connection
        client.admin.command('ping')
        return client
    except Exception as e:
        print(f"MongoDB baƒülantƒ± hatasƒ±: {str(e)}")
        return None


def _check_document_name_exists(belge_adi: str, mode: str) -> Tuple[bool, bool, Optional[str]]:
    """
    Belge adƒ±nƒ±n hem Supabase (MevzuatGPT API) hem de MongoDB (Portal) √ºzerinde 
    daha √∂nce y√ºklenip y√ºklenmediƒüini kontrol eder.
    
    Args:
        belge_adi: Kontrol edilecek belge adƒ±
        mode: ƒ∞≈ülem modu ('m': MevzuatGPT, 'p': Portal, 't': Tamamƒ±)
    
    Returns:
        (exists_in_mevzuatgpt, exists_in_portal, error_message) tuple:
        - exists_in_mevzuatgpt: True ise MevzuatGPT'de mevcut
        - exists_in_portal: True ise Portal'da mevcut
        - error_message: Hata mesajƒ± (varsa)
    """
    exists_in_mevzuatgpt = False
    exists_in_portal = False
    
    try:
        print("=" * 80)
        print("üîç BELGE ADI KONTROL√ú")
        print("=" * 80)
        print(f"   üìÑ Kontrol edilen belge adƒ±: {belge_adi}")
        print(f"   üîß ƒ∞≈ülem modu: {mode.upper()}")
        
        belge_normalized = normalize_for_exact_match(belge_adi)
        print(f"   üî§ Normalize edilmi≈ü ad: {belge_normalized}")
        
        # MevzuatGPT (Supabase/API) kontrol√º - 'm' ve 't' modlarƒ± i√ßin
        if mode in ["m", "t"]:
            print("\n   üì° [1/2] MevzuatGPT (Supabase) kontrol√º yapƒ±lƒ±yor...")
            print(f"   üåê Endpoint: /api/admin/documents")
            try:
                cfg = _load_config()
                if cfg:
                    token = _login_with_config(cfg)
                    if token:
                        api_base_url = cfg.get("api_base_url")
                        uploaded_docs = get_uploaded_documents(api_base_url, token, use_streamlit=False)
                        print(f"   üìä API'den {len(uploaded_docs)} belge √ßekildi")
                        
                        for doc in uploaded_docs:
                            # Birden fazla alan kontrol et (API'den d√∂nen belgelerde farklƒ± alan isimleri olabilir)
                            doc_titles = [
                                doc.get("belge_adi", ""),
                                doc.get("document_name", ""),
                                doc.get("title", ""),
                                doc.get("filename", ""),
                                doc.get("name", "")
                            ]
                            
                            for doc_title in doc_titles:
                                if doc_title:
                                    doc_normalized = normalize_for_exact_match(doc_title)
                                    if belge_normalized == doc_normalized:
                                        exists_in_mevzuatgpt = True
                                        print(f"   ‚úÖ MevzuatGPT'de bulundu: '{doc_title}'")
                                        break
                            
                            if exists_in_mevzuatgpt:
                                break
                        
                        if not exists_in_mevzuatgpt:
                            print(f"   ‚ùå MevzuatGPT'de bulunamadƒ± ({len(uploaded_docs)} belge kontrol edildi)")
                    else:
                        print("   ‚ö†Ô∏è MevzuatGPT login ba≈üarƒ±sƒ±z, kontrol atlandƒ±")
                else:
                    print("   ‚ö†Ô∏è Config bulunamadƒ±, MevzuatGPT kontrol√º atlandƒ±")
            except Exception as e:
                print(f"   ‚ö†Ô∏è MevzuatGPT kontrol√º sƒ±rasƒ±nda hata: {str(e)}")
                import traceback
                print(f"   üìã Traceback: {traceback.format_exc()}")
                # Hata olsa bile devam et, sadece uyarƒ± ver
        
        # Portal (MongoDB) kontrol√º - 'p' ve 't' modlarƒ± i√ßin
        if mode in ["p", "t"]:
            print("\n   üóÑÔ∏è [2/2] Portal (MongoDB) kontrol√º yapƒ±lƒ±yor...")
            try:
                client = _get_mongodb_client()
                if client:
                    database_name = os.getenv("MONGODB_DATABASE", "mevzuatgpt")
                    metadata_collection_name = os.getenv("MONGODB_METADATA_COLLECTION", "metadata")
                    db = client[database_name]
                    metadata_collection = db[metadata_collection_name]
                    
                    # MongoDB'den t√ºm pdf_adi'leri √ßek ve kontrol et
                    cursor = metadata_collection.find({}, {"pdf_adi": 1})
                    count = 0
                    for doc in cursor:
                        pdf_adi = doc.get("pdf_adi", "")
                        if pdf_adi:
                            pdf_normalized = normalize_for_exact_match(pdf_adi)
                            if belge_normalized == pdf_normalized:
                                exists_in_portal = True
                                print(f"   ‚úÖ Portal'da bulundu: {pdf_adi}")
                                break
                        count += 1
                    
                    client.close()
                    if not exists_in_portal:
                        print(f"   ‚ùå Portal'da bulunamadƒ± ({count} belge kontrol edildi)")
                else:
                    print("   ‚ö†Ô∏è MongoDB baƒülantƒ±sƒ± kurulamadƒ±, Portal kontrol√º atlandƒ±")
            except Exception as e:
                print(f"   ‚ö†Ô∏è Portal kontrol√º sƒ±rasƒ±nda hata: {str(e)}")
                # Hata olsa bile devam et, sadece uyarƒ± ver
        
        # Sonu√ß √∂zeti
        print("\n   üìä Kontrol Sonu√ßlarƒ±:")
        print(f"      - MevzuatGPT: {'‚úÖ Mevcut' if exists_in_mevzuatgpt else '‚ùå Yok'}")
        print(f"      - Portal: {'‚úÖ Mevcut' if exists_in_portal else '‚ùå Yok'}")
        
        # Her ikisinde de varsa hata mesajƒ± olu≈ütur
        if exists_in_mevzuatgpt and exists_in_portal:
            error_msg = f"Bu belge adƒ± ('{belge_adi}') hem MevzuatGPT'de hem de Portal'da zaten mevcut. Y√ºkleme yapƒ±lmayacak."
            print(f"\n   ‚ùå {error_msg}")
            return exists_in_mevzuatgpt, exists_in_portal, error_msg
        
        print("\n   ‚úÖ Belge adƒ± kontrol√º tamamlandƒ±")
        return exists_in_mevzuatgpt, exists_in_portal, None
        
    except Exception as e:
        print(f"   ‚ùå Belge adƒ± kontrol√º sƒ±rasƒ±nda beklenmeyen hata: {str(e)}")
        # Hata durumunda g√ºvenli tarafta kal, kontrol√º ge√ß
        return False, False, None


def _save_to_mongodb(metadata: Dict[str, Any], content: str) -> Optional[str]:
    """Metadata ve content'i MongoDB'ye kaydeder, metadata_id d√∂ner"""
    try:
        print(f"üíæ [MongoDB Save] Ba≈ülatƒ±lƒ±yor...")
        
        client = _get_mongodb_client()
        if not client:
            print("‚ùå [MongoDB Save] MongoDB client bulunamadƒ±")
            return None
        
        database_name = os.getenv("MONGODB_DATABASE", "mevzuatgpt")
        metadata_collection_name = os.getenv("MONGODB_METADATA_COLLECTION", "metadata")
        content_collection_name = os.getenv("MONGODB_CONTENT_COLLECTION", "content")
        
        print(f"   üóÑÔ∏è Database: {database_name}")
        print(f"   üìã Metadata Collection: {metadata_collection_name}")
        print(f"   üìÑ Content Collection: {content_collection_name}")
        
        db = client[database_name]
        metadata_collection = db[metadata_collection_name]
        content_collection = db[content_collection_name]
        
        # Metadata kaydet
        print(f"   üìù Metadata temizleniyor...")
        clean_metadata = {}
        for key, value in metadata.items():
            if value is not None and value != '':
                clean_metadata[key] = value
        
        print(f"   üìä Metadata keys: {list(clean_metadata.keys())}")
        print(f"   üìÑ PDF Adƒ±: {clean_metadata.get('pdf_adi', 'N/A')}")
        print(f"   üè¢ Kurum ID: {clean_metadata.get('kurum_id', 'N/A')}")
        print(f"   üìÇ Belge T√ºr√º: {clean_metadata.get('belge_turu', 'N/A')}")
        print(f"   üìä Sayfa Sayƒ±sƒ±: {clean_metadata.get('sayfa_sayisi', 'N/A')}")
        print(f"   üíæ Dosya Boyutu: {clean_metadata.get('dosya_boyutu_mb', 'N/A')} MB")
        
        clean_metadata['olusturulma_tarihi'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        
        print(f"   üíæ Metadata MongoDB'ye kaydediliyor...")
        metadata_result = metadata_collection.insert_one(clean_metadata)
        metadata_id = str(metadata_result.inserted_id)
        print(f"   ‚úÖ Metadata kaydedildi: metadata_id={metadata_id}")
        
        # Content kaydet
        content_length = len(content)
        content_length_kb = round(content_length / 1024, 2)
        print(f"   üìÑ Content hazƒ±rlanƒ±yor...")
        print(f"      üìä Content uzunluƒüu: {content_length:,} karakter ({content_length_kb} KB)")
        
        content_doc = {
            'metadata_id': ObjectId(metadata_id),
            'icerik': content,
            'olusturulma_tarihi': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        }
        
        print(f"   üíæ Content MongoDB'ye kaydediliyor...")
        content_result = content_collection.insert_one(content_doc)
        content_id = str(content_result.inserted_id)
        print(f"   ‚úÖ Content kaydedildi: content_id={content_id}")
        
        client.close()
        print(f"‚úÖ [MongoDB Save] Ba≈üarƒ±lƒ±! metadata_id={metadata_id}")
        return metadata_id
        
    except Exception as e:
        print(f"‚ùå [MongoDB Save] Hata: {str(e)}")
        import traceback
        print(f"   üìã Traceback: {traceback.format_exc()}")
        return None


def _extract_pdf_text_markdown(pdf_path: str) -> Optional[str]:
    """PDF'den markdown formatƒ±nda metin √ßƒ±karƒ±r (OCR desteƒüi ile)"""
    try:
        import pdfplumber
        from io import BytesIO
        
        extracted_text = ""
        total_pages = 0
        
        # √ñnce PDF yapƒ±sƒ±nƒ± analiz et (daha doƒüru tespit i√ßin)
        processor = PDFProcessor()
        pdf_structure = processor.analyze_pdf_structure(pdf_path)
        total_pages = pdf_structure.get('total_pages', 0)
        text_coverage = pdf_structure.get('text_coverage', 0.0)
        has_text = pdf_structure.get('has_text', False)
        needs_ocr = pdf_structure.get('needs_ocr', False)
        
        # Resim formatƒ± kontrol√º: Eƒüer PDF resim formatƒ±ndaysa direkt OCR ile ba≈üla
        # %30 e≈üiƒüi: Metin kapsamƒ± d√º≈ü√ºkse kalite zayƒ±f olabilir, OCR daha iyi sonu√ß verebilir
        # Ayrƒ±ca, eƒüer metin varsa ama √ßok azsa (sadece ba≈ülƒ±klar), OCR gerekli
        # Ortalama sayfa ba≈üƒ±na metin miktarƒ±nƒ± kontrol et
        avg_text_per_page = 0
        if total_pages > 0:
            # Hƒ±zlƒ± kontrol: ƒ∞lk 3 sayfadan ortalama metin miktarƒ±nƒ± hesapla
            import pdfplumber
            from io import BytesIO
            with open(pdf_path, 'rb') as f:
                pdf_bytes = f.read()
            pdf_file_obj = BytesIO(pdf_bytes)
            with pdfplumber.open(pdf_file_obj) as pdf:
                quick_check_pages = min(3, total_pages)
                quick_total_text = 0
                for page_num in range(quick_check_pages):
                    try:
                        page = pdf.pages[page_num]
                        page_text = page.extract_text()
                        if page_text:
                            quick_total_text += len(page_text.strip())
                    except Exception:
                        pass
                avg_text_per_page = quick_total_text / quick_check_pages if quick_check_pages > 0 else 0
        
        # Eƒüer ortalama sayfa ba≈üƒ±na metin 300 karakterden azsa, muhtemelen sadece ba≈ülƒ±klar var
        is_image_pdf = not has_text or text_coverage < 0.3 or needs_ocr or (has_text and avg_text_per_page < 300)
        
        if is_image_pdf:
            print(f"üì∏ PDF resim formatƒ±nda tespit edildi (kapsam: %{text_coverage*100:.1f}). OCR ile t√ºm {total_pages} sayfa i≈üleniyor (sƒ±nƒ±rlama olmadan)...")
            try:
                if processor._check_ocr_available():
                    # Direkt OCR ile t√ºm sayfalarƒ± i≈üle (sƒ±nƒ±rlama yok)
                    print(f"üîÑ OCR ba≈ülatƒ±lƒ±yor: {total_pages} sayfa i≈ülenecek...")
                    ocr_text = processor.extract_text_from_pages(pdf_path, 1, total_pages, use_ocr=True)
                    if ocr_text and len(ocr_text.strip()) > 0:
                        extracted_text = _format_text_as_markdown(ocr_text)
                        ocr_char_count = len(ocr_text)
                        ocr_line_count = len([line for line in ocr_text.split('\n') if line.strip()])
                        print(f"‚úÖ OCR tamamlandƒ±: {total_pages} sayfa i≈ülendi, {ocr_char_count:,} karakter, {ocr_line_count:,} satƒ±r √ßƒ±karƒ±ldƒ±")
                        return extracted_text.strip()
                    else:
                        print("‚ö†Ô∏è OCR ile metin √ßƒ±karƒ±lamadƒ±")
                else:
                    print("‚ö†Ô∏è OCR k√ºt√ºphaneleri kurulu deƒüil veya Poppler/RapidOCR eksik")
                    print("‚ö†Ô∏è Kurulum i√ßin: 'apt-get install poppler-utils' (Linux)")
                    print("‚ö†Ô∏è Python paketi: 'pip install rapidocr-onnxruntime'")
            except Exception as ocr_error:
                error_msg = str(ocr_error)
                print(f"‚ùå OCR hatasƒ±: {error_msg}")
                if "poppler" in error_msg.lower() or "pdftoppm" in error_msg.lower():
                    print("‚ùå Poppler kurulu deƒüil! 'apt-get install poppler-utils' komutunu √ßalƒ±≈ütƒ±rƒ±n.")
                elif "rapidocr" in error_msg.lower() or "rapid" in error_msg.lower():
                    print("‚ùå RapidOCR kurulu deƒüil! 'pip install rapidocr-onnxruntime' komutunu √ßalƒ±≈ütƒ±rƒ±n.")
                import traceback
                traceback.print_exc()
                return None
        
        # Normal metin √ßƒ±karma: PDF'de yeterli metin var
        with open(pdf_path, 'rb') as f:
            pdf_bytes = f.read()
        
        pdf_file_obj = BytesIO(pdf_bytes)
        
        with pdfplumber.open(pdf_file_obj) as pdf:
            if total_pages == 0:
                total_pages = len(pdf.pages)
            
            # Hƒ±zlƒ± kontrol: ƒ∞lk 3 sayfadan metin √ßƒ±kar
            quick_check_pages = min(3, total_pages)
            total_text_length = 0
            pages_with_text = 0
            
            for page_num in range(quick_check_pages):
                try:
                    page = pdf.pages[page_num]
                    page_text = page.extract_text()
                    if page_text and len(page_text.strip()) > 10:
                        total_text_length += len(page_text.strip())
                        pages_with_text += 1
                except Exception:
                    pass
            
            # Metin kapsamƒ±nƒ± hesapla (ilk 3 sayfadan)
            quick_coverage = pages_with_text / quick_check_pages if quick_check_pages > 0 else 0.0
            
            # Resim formatƒ± kontrol√º: Eƒüer ilk 3 sayfada hi√ß metin yoksa, √ßok az metin varsa 
            # veya metin kapsamƒ± %30'dan azsa veya toplam metin √ßok azsa direkt OCR ile t√ºm sayfalarƒ± i≈üle
            # %30 e≈üiƒüi: Metin kapsamƒ± d√º≈ü√ºkse kalite zayƒ±f olabilir, OCR daha iyi sonu√ß verebilir
            # Ayrƒ±ca, eƒüer metin varsa ama √ßok azsa (1000 karakterden az), bu da resim formatƒ± olabilir
            should_use_ocr_directly = (
                pages_with_text == 0 or 
                (pages_with_text < 2 and total_text_length < 500) or
                quick_coverage < 0.3 or
                (pages_with_text > 0 and total_text_length < 1000)  # Metin var ama √ßok az
            )
            
            if should_use_ocr_directly:
                print(f"üì∏ PDF resim formatƒ±nda tespit edildi (ilk {quick_check_pages} sayfada kapsam: %{quick_coverage*100:.1f}, metin: {pages_with_text}/{quick_check_pages} sayfa). OCR ile t√ºm {total_pages} sayfa i≈üleniyor (sƒ±nƒ±rlama olmadan)...")
                try:
                    processor = PDFProcessor()
                    if processor._check_ocr_available():
                        # Direkt OCR ile t√ºm sayfalarƒ± i≈üle (sƒ±nƒ±rlama yok)
                        print(f"üîÑ OCR ba≈ülatƒ±lƒ±yor: {total_pages} sayfa i≈ülenecek...")
                        ocr_text = processor.extract_text_from_pages(pdf_path, 1, total_pages, use_ocr=True)
                        if ocr_text and len(ocr_text.strip()) > 0:
                            extracted_text = _format_text_as_markdown(ocr_text)
                            ocr_char_count = len(ocr_text)
                            ocr_line_count = len([line for line in ocr_text.split('\n') if line.strip()])
                            print(f"‚úÖ OCR tamamlandƒ±: {total_pages} sayfa i≈ülendi, {ocr_char_count:,} karakter, {ocr_line_count:,} satƒ±r √ßƒ±karƒ±ldƒ±")
                            return extracted_text.strip()
                        else:
                            print("‚ö†Ô∏è OCR ile metin √ßƒ±karƒ±lamadƒ±")
                    else:
                        print("‚ö†Ô∏è OCR k√ºt√ºphaneleri kurulu deƒüil veya Poppler/RapidOCR eksik")
                        print("‚ö†Ô∏è Kurulum i√ßin: 'apt-get install poppler-utils' (Linux)")
                        print("‚ö†Ô∏è Python paketi: 'pip install rapidocr-onnxruntime'")
                except Exception as ocr_error:
                    error_msg = str(ocr_error)
                    print(f"‚ùå OCR hatasƒ±: {error_msg}")
                    if "poppler" in error_msg.lower() or "pdftoppm" in error_msg.lower():
                        print("‚ùå Poppler kurulu deƒüil! 'apt-get install poppler-utils' komutunu √ßalƒ±≈ütƒ±rƒ±n.")
                    elif "rapidocr" in error_msg.lower() or "rapid" in error_msg.lower():
                        print("‚ùå RapidOCR kurulu deƒüil! 'pip install rapidocr-onnxruntime' komutunu √ßalƒ±≈ütƒ±rƒ±n.")
                    import traceback
                    traceback.print_exc()
                    return None
            
            # Normal metin √ßƒ±karma: T√ºm sayfalarƒ± i≈üle (metin kapsamƒ± yeterliyse)
            total_text_length = 0
            pages_with_text = 0
            
            for page_num, page in enumerate(pdf.pages, 1):
                try:
                    page_text = page.extract_text()
                    if page_text and len(page_text.strip()) > 10:
                        # Basit markdown formatƒ±
                        formatted_text = _format_text_as_markdown(page_text)
                        extracted_text += formatted_text + "\n\n"
                        total_text_length += len(page_text.strip())
                        pages_with_text += 1
                    else:
                        # Metin yoksa OCR ile dene
                        processor = PDFProcessor()
                        if processor._check_ocr_available():
                            try:
                                ocr_text = processor._extract_text_with_ocr(pdf_path, page_num - 1)
                                if ocr_text and len(ocr_text.strip()) > 0:
                                    formatted_text = _format_text_as_markdown(ocr_text)
                                    extracted_text += formatted_text + "\n\n"
                                    total_text_length += len(ocr_text.strip())
                                    pages_with_text += 1
                            except Exception:
                                pass
                except Exception as page_error:
                    # Sayfa hatasƒ± varsa OCR ile dene
                    processor = PDFProcessor()
                    if processor._check_ocr_available():
                        try:
                            ocr_text = processor._extract_text_with_ocr(pdf_path, page_num - 1)
                            if ocr_text and len(ocr_text.strip()) > 0:
                                formatted_text = _format_text_as_markdown(ocr_text)
                                extracted_text += formatted_text + "\n\n"
                        except Exception:
                            pass
                    continue
        
            # Metin kapsamƒ±nƒ± kontrol et: Eƒüer %30'dan az sayfa metin i√ßeriyorsa veya toplam metin √ßok azsa OCR kullan
            # %30 e≈üiƒüi: Metin kapsamƒ± d√º≈ü√ºkse kalite zayƒ±f olabilir, OCR daha iyi sonu√ß verebilir
            text_coverage = pages_with_text / total_pages if total_pages > 0 else 0.0
            should_use_ocr = text_coverage < 0.3 or total_text_length < 1000
        
        # Eƒüer metin yetersizse OCR ile t√ºm sayfalarƒ± i≈üle
        if should_use_ocr and total_pages > 0:
            print(f"üì∏ PDF'de metin bulunamadƒ± veya yetersiz (kapsam: %{text_coverage*100:.1f}, toplam: {total_text_length} karakter), OCR ile t√ºm {total_pages} sayfa i≈üleniyor...")
            try:
                processor = PDFProcessor()
                if processor._check_ocr_available():
                    # T√ºm sayfalar i√ßin OCR yap (use_ocr=True ile zorunlu OCR)
                    # end_page dahil olacak ≈üekilde total_pages kullan
                    print(f"üîÑ OCR ba≈ülatƒ±lƒ±yor: {total_pages} sayfa i≈ülenecek...")
                    ocr_text = processor.extract_text_from_pages(pdf_path, 1, total_pages, use_ocr=True)
                    if ocr_text and len(ocr_text.strip()) > 100:
                        extracted_text = _format_text_as_markdown(ocr_text)
                        ocr_char_count = len(ocr_text)
                        ocr_line_count = len([line for line in ocr_text.split('\n') if line.strip()])
                        print(f"‚úÖ OCR tamamlandƒ±: {total_pages} sayfa i≈ülendi, {ocr_char_count:,} karakter, {ocr_line_count:,} satƒ±r √ßƒ±karƒ±ldƒ±")
                    else:
                        print("‚ö†Ô∏è OCR ile metin √ßƒ±karƒ±lamadƒ± veya √ßok az metin √ßƒ±karƒ±ldƒ±")
                        if ocr_text:
                            print(f"‚ö†Ô∏è √áƒ±karƒ±lan metin uzunluƒüu: {len(ocr_text)} karakter (√ßok kƒ±sa)")
                else:
                    print("‚ö†Ô∏è OCR k√ºt√ºphaneleri kurulu deƒüil veya Poppler/RapidOCR eksik")
                    print("‚ö†Ô∏è Kurulum i√ßin: 'apt-get install poppler-utils' (Linux)")
                    print("‚ö†Ô∏è Python paketi: 'pip install rapidocr-onnxruntime'")
            except Exception as ocr_error:
                error_msg = str(ocr_error)
                print(f"‚ùå OCR hatasƒ±: {error_msg}")
                # Poppler veya RapidOCR eksikse √∂zel mesaj
                if "poppler" in error_msg.lower() or "pdftoppm" in error_msg.lower():
                    print("‚ùå Poppler kurulu deƒüil! 'apt-get install poppler-utils' komutunu √ßalƒ±≈ütƒ±rƒ±n.")
                elif "rapidocr" in error_msg.lower() or "rapid" in error_msg.lower():
                    print("‚ùå RapidOCR kurulu deƒüil! 'pip install rapidocr-onnxruntime' komutunu √ßalƒ±≈ütƒ±rƒ±n.")
                import traceback
                traceback.print_exc()
        
        return extracted_text.strip() if extracted_text.strip() else None
        
    except Exception as e:
        print(f"PDF metin √ßƒ±karma hatasƒ±: {str(e)}")
        import traceback
        traceback.print_exc()
        return None


def _format_text_as_markdown(text: str) -> str:
    """Metni markdown formatƒ±na √ßevirir"""
    try:
        if not text:
            return ""
        
        lines = text.split('\n')
        formatted_lines = []
        
        for line in lines:
            line = line.strip()
            if not line:
                continue
            
            # Sayfa numaralarƒ±nƒ± atla
            if re.match(r'^\d+$', line) or re.match(r'^sayfa\s+\d+', line.lower()):
                continue
            
            # Ana ba≈ülƒ±klar (b√ºy√ºk harf, 10+ karakter)
            if line.isupper() and len(line) > 10 and not re.match(r'^\d+', line):
                formatted_lines.append(f"\n## {line.title()}\n")
            
            # Madde ba≈ülƒ±klarƒ±
            elif re.match(r'^MADDE\s+\d+', line, re.IGNORECASE):
                formatted_lines.append(f"\n### {line.title()}\n")
            
            # B√∂l√ºm ba≈ülƒ±klarƒ±
            elif re.match(r'^B√ñL√úM\s+[IVX\d]+', line, re.IGNORECASE):
                formatted_lines.append(f"\n## {line.title()}\n")
            
            # Alt ba≈ülƒ±klar (numaralƒ±)
            elif re.match(r'^\d+\.\s+[A-Z√ú√áƒûIƒ∞√ñ≈û]', line):
                formatted_lines.append(f"\n**{line}**\n")
            
            # Normal paragraflar
            else:
                if len(line) > 50:
                    formatted_lines.append(f"{line}\n")
                else:
                    formatted_lines.append(f"**{line}**\n")
        
        return '\n'.join(formatted_lines)
        
    except Exception:
        return text


def _analyze_and_prepare_headless(pdf_path: str, pdf_base_name: str, api_key: Optional[str], use_ocr: bool = False) -> Dict[str, Any]:
    """Streamlit'e baƒülƒ± olmadan analiz ve metadata √ºretimini yapar.
    
    Args:
        pdf_path: PDF dosya yolu
        pdf_base_name: PDF dosya adƒ± (base)
        api_key: DeepSeek API anahtarƒ± (zorunlu - b√∂l√ºmleme i√ßin gerekli)
        use_ocr: OCR kullanƒ±mƒ± (True: zorunlu OCR, False: OCR kullanma, varsayƒ±lan: False)
    """
    print("=" * 80)
    print("üîç [A≈ûAMA 0.1] PDF ANALƒ∞Zƒ∞ BA≈ûLATILIYOR")
    print("=" * 80)
    
    # DeepSeek API anahtarƒ± zorunlu
    if not api_key:
        raise HTTPException(
            status_code=500,
            detail="DeepSeek API anahtarƒ± bulunamadƒ±. B√∂l√ºmleme i√ßin DeepSeek API anahtarƒ± zorunludur."
        )
    print("‚úÖ [A≈ûAMA 0.1] DeepSeek API anahtarƒ± bulundu")
    
    processor = PDFProcessor()
    
    # OCR kullanƒ±mƒ± kontrol√º
    if use_ocr is True:
        print("üì∏ [A≈ûAMA 0.1] OCR kullanƒ±mƒ±: Aktif (kullanƒ±cƒ± tarafƒ±ndan belirlendi)")
        # OCR kullanƒ±lacaksa √∂nce kontrol et
        if not processor._check_ocr_available():
            raise HTTPException(
                status_code=500,
                detail="OCR kullanƒ±mƒ± isteniyor ancak RapidOCR kurulu deƒüil. L√ºtfen 'pip install rapidocr-onnxruntime' komutunu √ßalƒ±≈ütƒ±rƒ±n."
            )
        # use_ocr=True ise sadece total_pages i√ßin minimal analiz yap (metin kontrol√º yapma)
        pdf_structure = processor.analyze_pdf_structure(pdf_path, skip_text_analysis=True)
        total_pages = pdf_structure['total_pages']
        print(f"   üìÑ Toplam sayfa: {total_pages}")
        print(f"   üì∏ T√ºm {total_pages} sayfa OCR ile i≈ülenecek")
    else:
        print("üìÑ [A≈ûAMA 0.1] OCR kullanƒ±mƒ±: Pasif (normal metin √ßƒ±karma)")
        # OCR kullanƒ±lmayacak, normal analiz yap
        pdf_structure = processor.analyze_pdf_structure(pdf_path)
        total_pages = pdf_structure['total_pages']
        print(f"   üìÑ Toplam sayfa: {total_pages}")
    
    print("=" * 80)
    print("üîç [A≈ûAMA 0.2] PDF B√ñL√úMLEME (DeepSeek API ile)")
    print("=" * 80)
    
    # Her zaman DeepSeek API ile b√∂l√ºmleme yap
    analyzer = DeepSeekAnalyzer(api_key)
    print("‚úÖ [A≈ûAMA 0.2] DeepSeek Analyzer olu≈üturuldu")
    
    try:
        print("   üîÑ Intelligent sections olu≈üturuluyor...")
        sections = processor.create_intelligent_sections(pdf_path, total_pages, analyzer, use_ocr=use_ocr)
        if use_ocr:
            cache_size = processor.get_ocr_cache_size()
            print(f"   üíæ OCR cache: {cache_size} sayfa √∂nbelleƒüe alƒ±ndƒ±")
        print(f"‚úÖ [A≈ûAMA 0.2] {len(sections)} b√∂l√ºm olu≈üturuldu (DeepSeek API ile)")
    except Exception as e:
        print(f"‚ùå [A≈ûAMA 0.2] Intelligent sections hatasƒ±: {str(e)}")
        import traceback
        print(f"   üìã Traceback: {traceback.format_exc()}")
        raise HTTPException(
            status_code=500,
            detail=f"DeepSeek API ile b√∂l√ºmleme ba≈üarƒ±sƒ±z: {str(e)}"
        )

    print("=" * 80)
    print("üîç [A≈ûAMA 0.3] METADATA √úRETƒ∞Mƒ∞ (DeepSeek API ile)")
    print("=" * 80)

    metadata_list: List[Dict[str, Any]] = []
    
    if use_ocr:
        cache_size = processor.get_ocr_cache_size()
        print(f"üì∏ OCR modu aktif: Metin √ßƒ±karma cache'den yapƒ±lacak ({cache_size} sayfa √∂nbellekte)")
    
    for i, section in enumerate(sections):
        print(f"   üìé [{i+1}/{len(sections)}] B√∂l√ºm metadata √ºretiliyor...")
        print(f"      üìÑ Sayfa aralƒ±ƒüƒ±: {section['start_page']}-{section['end_page']}")
        
        section_text = processor.extract_text_from_pages(pdf_path, section['start_page'], section['end_page'], use_ocr=use_ocr)
        
        if section_text.strip():
            print(f"      üìù Metin √ßƒ±karƒ±ldƒ±: {len(section_text)} karakter")
            print(f"      ü§ñ DeepSeek API ile analiz yapƒ±lƒ±yor...")
            try:
                analysis = analyzer.analyze_section_content(section_text)
                title = analysis.get('title', f'B√∂l√ºm {i + 1}')
                description = analysis.get('description', 'Bu b√∂l√ºm i√ßin a√ßƒ±klama olu≈üturulamadƒ±.')
                keywords = analysis.get('keywords', f'b√∂l√ºm {i + 1}')
                print(f"      ‚úÖ Metadata √ºretildi: {title}")
            except Exception as e:
                print(f"      ‚ö†Ô∏è DeepSeek API analiz hatasƒ±: {str(e)}")
                title = f"B√∂l√ºm {i + 1}"
                description = "Bu b√∂l√ºm i√ßin otomatik a√ßƒ±klama olu≈üturulamadƒ±."
                keywords = f"b√∂l√ºm {i + 1}"
        else:
            print(f"      ‚ö†Ô∏è B√∂l√ºmde metin bulunamadƒ±")
            title = f"B√∂l√ºm {i + 1}"
            description = "Bu b√∂l√ºm i√ßin otomatik a√ßƒ±klama olu≈üturulamadƒ±."
            keywords = f"b√∂l√ºm {i + 1}"

        output_filename = create_pdf_filename(pdf_base_name, i + 1, section['start_page'], section['end_page'], title)
        metadata_list.append({
            "output_filename": output_filename,
            "start_page": section['start_page'],
            "end_page": section['end_page'],
            "title": title,
            "description": description,
            "keywords": keywords
        })
        print(f"      ‚úÖ B√∂l√ºm {i+1} tamamlandƒ±")

    print(f"‚úÖ [A≈ûAMA 0.3] {len(metadata_list)} b√∂l√ºm i√ßin metadata √ºretildi")
    print("=" * 80)

    return {"sections": sections, "metadata_list": metadata_list, "total_pages": total_pages}


def _split_pdfs(pdf_path: str, sections: List[Dict[str, int]], metadata_list: List[Dict[str, Any]]) -> str:
    """PDF'leri b√∂l√ºmlere ayƒ±rƒ±r ve chunk'lar olu≈üturur"""
    print(f"   üìÇ PDF dosyasƒ±: {pdf_path}")
    print(f"   üìä Toplam b√∂l√ºm: {len(sections)}")
    
    output_dir = create_output_directories()
    print(f"   üìÅ Output dizini olu≈üturuldu: {output_dir}")
    
    from pypdf import PdfReader, PdfWriter
    with open(pdf_path, 'rb') as source:
        reader = PdfReader(source)
        total_pages = len(reader.pages)
        print(f"   üìÑ Kaynak PDF sayfa sayƒ±sƒ±: {total_pages}")
        
        for i, (section, metadata) in enumerate(zip(sections, metadata_list), 1):
            start_page = section['start_page']
            end_page = section['end_page']
            output_filename = metadata.get('output_filename', f'section_{i}.pdf')
            
            print(f"   üìé [{i}/{len(sections)}] B√∂l√ºm i≈üleniyor: {output_filename}")
            print(f"      üìÑ Sayfa aralƒ±ƒüƒ±: {start_page}-{end_page}")
            
            writer = PdfWriter()
            pages_added = 0
            for page_num in range(start_page - 1, end_page):
                if page_num < len(reader.pages):
                    writer.add_page(reader.pages[page_num])
                    pages_added += 1
            
            print(f"      ‚úÖ {pages_added} sayfa eklendi")
            
            out_path = Path(output_dir) / output_filename
            try:
                with open(out_path, 'wb') as f:
                    writer.write(f)
                    file_size = out_path.stat().st_size
                    print(f"      üíæ Dosya kaydedildi: {file_size:,} bytes")
            except Exception as e:
                print(f"      ‚ùå Dosya kaydetme hatasƒ±: {str(e)}")
                raise
    
    # JSON metadata dosyasƒ± da kaydedilsin
    json_path = Path(output_dir) / "pdf_sections_metadata.json"
    print(f"   üìã Metadata JSON dosyasƒ± kaydediliyor: {json_path}")
    try:
        with open(json_path, 'w', encoding='utf-8') as jf:
            json.dump({"pdf_sections": metadata_list}, jf, ensure_ascii=False, indent=2)
            json_size = json_path.stat().st_size
            print(f"   ‚úÖ Metadata JSON kaydedildi: {json_size:,} bytes")
    except Exception as e:
        print(f"   ‚ö†Ô∏è Metadata JSON kaydetme hatasƒ±: {str(e)}")
    
    return output_dir


def _upload_bulk(cfg: Dict[str, Any], token: str, output_dir: str, category: str, institution: str, belge_adi: str, metadata_list: List[Dict[str, Any]]) -> Optional[Dict[str, Any]]:
    """MevzuatGPT'ye bulk upload yapar"""
    try:
        print(f"üîß [MevzuatGPT Upload] Ba≈ülatƒ±lƒ±yor...")
        print(f"   üìÇ Output dizini: {output_dir}")
        print(f"   üìã Kategori: {category}")
        print(f"   üè¢ Kurum: {institution}")
        print(f"   üìÑ Belge: {belge_adi}")
        print(f"   üìä Metadata sayƒ±sƒ±: {len(metadata_list)}")
        
        api_base_url = cfg.get("api_base_url")
        if not api_base_url:
            print("‚ùå [MevzuatGPT Upload] API base URL bulunamadƒ±!")
            return None
        
        upload_url = f"{api_base_url.rstrip('/')}/api/admin/documents/bulk-upload"
        print(f"üåê [MevzuatGPT Upload] Upload URL: {upload_url}")
        
        # PDF dosyalarƒ±nƒ± bul
        print(f"üìÅ [MevzuatGPT Upload] PDF dosyalarƒ± aranƒ±yor: {output_dir}")
        pdf_files = list(sorted(Path(output_dir).glob('*.pdf')))
        print(f"   üìÑ Bulunan PDF sayƒ±sƒ±: {len(pdf_files)}")
        
        if len(pdf_files) == 0:
            print("‚ùå [MevzuatGPT Upload] Y√ºklenecek PDF dosyasƒ± bulunamadƒ±!")
            return None
        
        # PDF dosyalarƒ±nƒ± oku ve i√ßeriklerini al
        files_content = []
        for i, pdf_file in enumerate(pdf_files, 1):
            print(f"   üìé [{i}/{len(pdf_files)}] PDF dosyasƒ± hazƒ±rlanƒ±yor: {pdf_file.name}")
            try:
                with open(pdf_file, 'rb') as f:
                    file_content = f.read()
                    file_size = len(file_content)
                    files_content.append((pdf_file.name, file_content, 'application/pdf'))
                    print(f"      ‚úÖ Dosya okundu: {file_size:,} bytes")
            except Exception as e:
                print(f"   ‚ö†Ô∏è [{i}/{len(pdf_files)}] PDF dosyasƒ± a√ßƒ±lamadƒ±: {pdf_file.name} - {str(e)}")
        
        if len(files_content) == 0:
            print("‚ùå [MevzuatGPT Upload] Y√ºklenecek PDF dosyasƒ± bulunamadƒ±!")
            return None
        
        print(f"‚úÖ [MevzuatGPT Upload] {len(files_content)} PDF dosyasƒ± hazƒ±rlandƒ±")
        
        # Metadata hazƒ±rla
        print(f"üìã [MevzuatGPT Upload] Metadata hazƒ±rlanƒ±yor...")
        metadata_json = json.dumps({"pdf_sections": [
                {
                    "output_filename": m.get("output_filename", ""),
                    "title": m.get("title", ""),
                    "description": m.get("description", ""),
                    "keywords": m.get("keywords", "")
                } for m in metadata_list
            ]}, ensure_ascii=False)
        print(f"   üìä Metadata JSON uzunluƒüu: {len(metadata_json)} karakter")
        
        headers = {'Authorization': f'Bearer {token}'}
        print(f"üöÄ [MevzuatGPT Upload] API'ye istek g√∂nderiliyor...")
        print(f"   ‚è±Ô∏è Timeout: 1200 saniye (20 dakika)")
        
        # curl_cffi i√ßin CurlMime kullan
        if CURL_CFFI_AVAILABLE:
            print(f"   üì¶ CurlMime formatƒ± kullanƒ±lƒ±yor (curl_cffi)")
            multipart = CurlMime()
            
            # Her PDF dosyasƒ±nƒ± ekle (aynƒ± field name 'files' ile)
            for filename, content, content_type in files_content:
                multipart.addpart(name='files', filename=filename, data=content, mimetype=content_type)
                print(f"      ‚úÖ Dosya eklendi: {filename}")
            
            # Form verilerini ekle
            multipart.addpart(name='category', data=category)
            multipart.addpart(name='institution', data=institution)
            multipart.addpart(name='belge_adi', data=belge_adi)
            multipart.addpart(name='metadata', data=metadata_json)
            
            print(f"   üìã Form verileri eklendi: category, institution, belge_adi, metadata")
            resp = requests.post(upload_url, headers=headers, multipart=multipart, timeout=1200)
        else:
            # Standart requests k√ºt√ºphanesi i√ßin
            form_data = {
                'category': category,
                'institution': institution,
                'belge_adi': belge_adi,
                'metadata': metadata_json
            }
            files_to_upload = [('files', (name, content, content_type)) for name, content, content_type in files_content]
            print(f"   üì¶ Standart requests formatƒ± kullanƒ±lƒ±yor")
            resp = requests.post(upload_url, headers=headers, data=form_data, files=files_to_upload, timeout=1200)
        
        print(f"üì° [MevzuatGPT Upload] API yanƒ±tƒ± alƒ±ndƒ±")
        print(f"   üìä Status Code: {resp.status_code}")
        print(f"   üìù Response uzunluƒüu: {len(resp.text)} karakter")
        print(f"   üìã Response headers: {dict(resp.headers)}")
        
        if resp.status_code == 200:
            try:
                response_data = resp.json()
                print(f"‚úÖ [MevzuatGPT Upload] Ba≈üarƒ±lƒ±!")
                print(f"   üì¶ Response type: {type(response_data)}")
                if isinstance(response_data, dict):
                    print(f"   üìä Response keys: {list(response_data.keys())}")
                    # √ñnemli alanlarƒ± g√∂ster
                    if "success" in response_data:
                        print(f"   ‚úÖ Success: {response_data.get('success')}")
                    if "message" in response_data:
                        print(f"   üí¨ Message: {response_data.get('message')}")
                    if "data" in response_data:
                        data = response_data.get('data')
                        if isinstance(data, dict):
                            print(f"   üìä Data keys: {list(data.keys())}")
                        elif isinstance(data, list):
                            print(f"   üìä Data list uzunluƒüu: {len(data)}")
                    if "inserted_count" in response_data:
                        print(f"   üìà Inserted count: {response_data.get('inserted_count')}")
                    if "chunks" in response_data:
                        chunks = response_data.get('chunks')
                        if isinstance(chunks, list):
                            print(f"   üì¶ Chunks sayƒ±sƒ±: {len(chunks)}")
                            if len(chunks) > 0:
                                print(f"   üìã ƒ∞lk chunk √∂rneƒüi: {json.dumps(chunks[0], ensure_ascii=False)[:200]}...")
                
                # Full response'u g√∂ster (kƒ±saltƒ±lmƒ±≈ü)
                response_str = json.dumps(response_data, ensure_ascii=False, indent=2)
                print(f"   üìÑ Full response (ilk 2000 karakter):")
                print(f"      {response_str[:2000]}")
                if len(response_str) > 2000:
                    print(f"      ... (toplam {len(response_str)} karakter)")
                
                return response_data
            except json.JSONDecodeError as e:
                print(f"‚ö†Ô∏è [MevzuatGPT Upload] JSON parse hatasƒ±: {str(e)}")
                print(f"   üìù Raw response: {resp.text[:1000]}")
                return {"status_code": 200, "text": resp.text, "parse_error": str(e)}
        else:
            print(f"‚ùå [MevzuatGPT Upload] Ba≈üarƒ±sƒ±z!")
            print(f"   üìä Status Code: {resp.status_code}")
            print(f"   üìù Response headers: {dict(resp.headers)}")
            print(f"   üìù Response body (ilk 2000 karakter):")
            print(f"      {resp.text[:2000]}")
            if len(resp.text) > 2000:
                print(f"      ... (toplam {len(resp.text)} karakter)")
        return {"status_code": resp.status_code, "text": resp.text}
            
    except requests.exceptions.Timeout as e:
        print(f"‚ùå [MevzuatGPT Upload] Timeout hatasƒ±: {str(e)}")
        return {"error": f"Timeout: {str(e)}"}
    except requests.exceptions.RequestException as e:
        print(f"‚ùå [MevzuatGPT Upload] Request hatasƒ±: {str(e)}")
        return {"error": f"Request error: {str(e)}"}
    except Exception as e:
        print(f"‚ùå [MevzuatGPT Upload] Beklenmeyen hata: {str(e)}")
        import traceback
        print(f"   üìã Traceback: {traceback.format_exc()}")
        return {"error": str(e)}


@app.post("/api/kurum/process", response_model=ProcessResponse, tags=["SGK Scraper"], summary="Link ile PDF indir, analiz et ve y√ºkle")
async def process_item(req: ProcessRequest):
    try:
        # Type kontrol√º
        if req.type.lower() != "kaysis":
            raise HTTPException(
                status_code=400,
                detail=f"Desteklenmeyen scraper tipi: {req.type}. ≈ûu an i√ßin sadece 'kaysis' desteklenmektedir."
            )
        
        # Mode kontrol√º
        mode = req.mode.lower() if req.mode else "t"
        if mode not in ["m", "p", "t"]:
            raise HTTPException(status_code=400, detail="Ge√ßersiz mode. 'm', 'p' veya 't' olmalƒ±.")
        
        print(f"üîß ƒ∞≈ülem modu: {mode.upper()} ({'MevzuatGPT' if mode == 'm' else 'Portal' if mode == 'p' else 'Tamamƒ±'})")
        print(f"üìã Scraper tipi: {req.type}")
        
        # MongoDB'den kurum bilgisini √ßek
        kurum_adi = None
        try:
            client = _get_mongodb_client()
            if client:
                database_name = os.getenv("MONGODB_DATABASE", "mevzuatgpt")
                db = client[database_name]
                kurumlar_collection = db["kurumlar"]
                from bson import ObjectId
                kurum_doc = kurumlar_collection.find_one({"_id": ObjectId(req.kurum_id)})
                if kurum_doc:
                    kurum_adi = kurum_doc.get("kurum_adi", "Bilinmeyen Kurum")
                client.close()
        except Exception as e:
            print(f"‚ö†Ô∏è MongoDB'den kurum bilgisi alƒ±namadƒ±: {str(e)}")
            kurum_adi = "Bilinmeyen Kurum"
        
        print(f"üìã Kurum: {kurum_adi}")
        print(f"üî¢ DETSIS: {req.detsis}")
        
        # Link ve diƒüer bilgileri request'ten al
        pdf_url = req.link
        if not pdf_url:
            raise HTTPException(status_code=400, detail="Link parametresi zorunludur.")
        
        # Category ve document_name request'ten al veya varsayƒ±lan deƒüerler kullan
        category = req.category if req.category else "Genel"
        document_name = req.document_name if req.document_name else "Belge"
        institution = kurum_adi  # Kurum adƒ±nƒ± kullan
        
        print(f"üîó PDF Link: {pdf_url}")
        print(f"üìÑ Belge Adƒ±: {document_name}")
        print(f"üìÇ Kategori: {category}")

        # Belge adƒ± kontrol√º (PDF indirmeden √∂nce)
        print("=" * 80)
        print("üîç BELGE ADI KONTROL√ú (PDF indirmeden √∂nce)")
        print("=" * 80)
        exists_in_mevzuatgpt, exists_in_portal, error_msg = _check_document_name_exists(document_name, mode)
        
        # Mode'a g√∂re kontrol ve dinamik mode ayarlama
        if mode == "t":  # "Hepsini y√ºkle" modu
            if exists_in_mevzuatgpt and exists_in_portal:
                # Her ikisinde de varsa -> Hata ver
                print(f"‚ùå Belge adƒ± kontrol√º ba≈üarƒ±sƒ±z: {error_msg}")
                raise HTTPException(status_code=400, detail=error_msg or "Bu belge adƒ± her iki yerde de zaten mevcut.")
            elif exists_in_mevzuatgpt and not exists_in_portal:
                # Sadece MevzuatGPT'de varsa -> Sadece Portal'a y√ºkle
                print(f"‚ÑπÔ∏è Belge MevzuatGPT'de zaten y√ºkl√º, sadece Portal'a y√ºklenecek.")
                mode = "p"
            elif exists_in_portal and not exists_in_mevzuatgpt:
                # Sadece Portal'da varsa -> Sadece MevzuatGPT'ye y√ºkle
                print(f"‚ÑπÔ∏è Belge Portal'da zaten y√ºkl√º, sadece MevzuatGPT'ye y√ºklenecek.")
                mode = "m"
            else:
                # Hi√ßbirinde yoksa -> Her ikisine de y√ºkle (mode 't' kalƒ±r)
                print(f"‚úÖ Belge her iki yerde de yok, her ikisine de y√ºklenecek.")
        else:
            # 'm' veya 'p' modu i√ßin sadece ilgili kontrol√º yap
            if mode == "m" and exists_in_mevzuatgpt:
                print(f"‚ùå Belge adƒ± kontrol√º ba≈üarƒ±sƒ±z: Bu belge adƒ± MevzuatGPT'de zaten mevcut.")
                raise HTTPException(status_code=400, detail="Bu belge adƒ± MevzuatGPT'de zaten mevcut.")
            elif mode == "p" and exists_in_portal:
                print(f"‚ùå Belge adƒ± kontrol√º ba≈üarƒ±sƒ±z: Bu belge adƒ± Portal'da zaten mevcut.")
                raise HTTPException(status_code=400, detail="Bu belge adƒ± Portal'da zaten mevcut.")
        
        print(f"‚úÖ Belge adƒ± kontrol√º tamamlandƒ± - ƒ∞≈ülem modu: {mode.upper()}")
        print("üì• PDF indirme i≈ülemine ge√ßiliyor...")

        # PDF'i indir
        print("=" * 80)
        print("üì• PDF ƒ∞NDƒ∞RME")
        print("=" * 80)
        print("üì• PDF indiriliyor...")
        pdf_path = await download_pdf_from_url(pdf_url)
        if not validate_pdf_file(pdf_path):
            raise HTTPException(status_code=500, detail="ƒ∞ndirilen dosya ge√ßerli bir PDF deƒüil.")
        print("‚úÖ PDF indirme ba≈üarƒ±lƒ±")

        # Analiz ve metadata (t√ºm modlar i√ßin: MevzuatGPT, Portal ve Tamamƒ±)
        print("=" * 80)
        print("üîç [A≈ûAMA 0] PDF ANALƒ∞Zƒ∞")
        print("=" * 80)
        print(f"   üìÑ PDF dosyasƒ±: {pdf_path}")
        
        api_key = _get_deepseek_api_key()
        if not api_key:
            print("   ‚ö†Ô∏è [A≈ûAMA 0] DeepSeek API anahtarƒ± bulunamadƒ±, manuel b√∂l√ºmleme ve basit metadata kullanƒ±lacak.")
        else:
            print(f"   ‚úÖ [A≈ûAMA 0] DeepSeek API anahtarƒ± bulundu")
        
        pdf_base_name = "document"
        # Kullanƒ±cƒ±nƒ±n OCR tercihini al (t√ºm modlar i√ßin ge√ßerli: m, p, t)
        use_ocr = req.use_ocr if hasattr(req, 'use_ocr') else False
        print(f"   üì∏ OCR kullanƒ±mƒ±: {'Aktif (t√ºm sayfalar OCR ile i≈ülenecek)' if use_ocr else 'Pasif (normal metin √ßƒ±karma)'}")
        
        print(f"   üîÑ Analiz ba≈ülatƒ±lƒ±yor...")
        try:
            analysis_result = _analyze_and_prepare_headless(pdf_path, pdf_base_name, api_key, use_ocr=use_ocr)
            sections = analysis_result['sections']
            metadata_list = analysis_result['metadata_list']
            total_pages = analysis_result.get('total_pages', 0)
            
            print(f"‚úÖ [A≈ûAMA 0] PDF analiz ba≈üarƒ±lƒ±")
            print(f"   üìä Toplam sayfa: {total_pages}")
            print(f"   üìã B√∂l√ºm sayƒ±sƒ±: {len(sections)}")
            print(f"   üìù Metadata sayƒ±sƒ±: {len(metadata_list)}")
            
            # B√∂l√ºm √∂zeti
            for i, section in enumerate(sections[:5], 1):  # ƒ∞lk 5 b√∂l√ºm√º g√∂ster
                print(f"      [{i}] Sayfa {section.get('start_page', '?')}-{section.get('end_page', '?')}")
            if len(sections) > 5:
                print(f"      ... ve {len(sections) - 5} b√∂l√ºm daha")
                
        except Exception as e:
            print(f"‚ùå [A≈ûAMA 0] PDF analiz hatasƒ±: {str(e)}")
            import traceback
            print(f"   üìã Traceback: {traceback.format_exc()}")
            raise HTTPException(status_code=500, detail=f"PDF analiz hatasƒ±: {str(e)}")

        # PDF'leri b√∂l ve √ßƒ±ktƒ±yƒ± olu≈ütur (sadece 'm' ve 't' modlarƒ± i√ßin)
        output_dir = None
        if mode in ["m", "t"]:
            print("=" * 80)
            print("üìÑ [A≈ûAMA 1] PDF B√ñL√úMLEME")
            print("=" * 80)
            print(f"   üìä B√∂l√ºm sayƒ±sƒ±: {len(sections)}")
            print(f"   üìã Metadata sayƒ±sƒ±: {len(metadata_list)}")
            try:
                output_dir = _split_pdfs(pdf_path, sections, metadata_list)
                print(f"‚úÖ [A≈ûAMA 1] PDF b√∂l√ºmleme ba≈üarƒ±lƒ±")
                print(f"   üìÇ Output dizini: {output_dir}")
                
                # Olu≈üturulan dosyalarƒ± kontrol et
                pdf_files = list(Path(output_dir).glob('*.pdf'))
                print(f"   üìÑ Olu≈üturulan PDF sayƒ±sƒ±: {len(pdf_files)}")
                for pdf_file in pdf_files:
                    file_size = pdf_file.stat().st_size
                    print(f"      - {pdf_file.name} ({file_size:,} bytes)")
            except Exception as e:
                print(f"‚ùå [A≈ûAMA 1] PDF b√∂l√ºmleme hatasƒ±: {str(e)}")
                import traceback
                print(f"   üìã Traceback: {traceback.format_exc()}")
                raise HTTPException(status_code=500, detail=f"PDF b√∂l√ºmleme hatasƒ±: {str(e)}")
        else:
            print("‚è≠Ô∏è PDF b√∂l√ºmleme atlandƒ± (Portal modu)")

        # MevzuatGPT'ye y√ºkleme (sadece 'm' ve 't' modlarƒ± i√ßin)
        upload_resp = None
        if mode in ["m", "t"]:
            print("=" * 80)
            print("üì§ [A≈ûAMA 2] MEVZUATGPT'YE Y√úKLEME")
            print("=" * 80)
            
            # Config kontrol√º
            print("üîß [A≈ûAMA 2.1] Config y√ºkleniyor...")
            cfg = _load_config()
            if not cfg:
                print("‚ùå [A≈ûAMA 2.1] Config bulunamadƒ±!")
                raise HTTPException(status_code=500, detail="Config dosyasƒ± bulunamadƒ±")
            print(f"‚úÖ [A≈ûAMA 2.1] Config y√ºklendi")
            print(f"   üåê API Base URL: {cfg.get('api_base_url', 'N/A')}")
            
            # Login kontrol√º
            print("üîê [A≈ûAMA 2.2] MevzuatGPT'ye login yapƒ±lƒ±yor...")
            token = _login_with_config(cfg)
            if not token:
                print("‚ùå [A≈ûAMA 2.2] Login ba≈üarƒ±sƒ±z!")
                raise HTTPException(status_code=500, detail="MevzuatGPT login ba≈üarƒ±sƒ±z")
            print(f"‚úÖ [A≈ûAMA 2.2] Login ba≈üarƒ±lƒ±")
            print(f"   üîë Token uzunluƒüu: {len(token)} karakter")
            
            # Upload i≈ülemi
            print("üì§ [A≈ûAMA 2.3] Bulk upload ba≈ülatƒ±lƒ±yor...")
            if not output_dir:
                print("‚ùå [A≈ûAMA 2.3] Output dizini bulunamadƒ±!")
                raise HTTPException(status_code=500, detail="Output dizini bulunamadƒ±")
            
            upload_resp = _upload_bulk(cfg, token, output_dir, category, institution, document_name, metadata_list)
            
            if upload_resp:
                # Response kontrol√º
                if "error" in upload_resp:
                    print(f"‚ùå [A≈ûAMA 2.3] Upload hatasƒ±: {upload_resp.get('error')}")
                    raise HTTPException(status_code=500, detail=f"Upload hatasƒ±: {upload_resp.get('error')}")
                elif upload_resp.get("status_code") and upload_resp.get("status_code") != 200:
                    print(f"‚ùå [A≈ûAMA 2.3] Upload ba≈üarƒ±sƒ±z: HTTP {upload_resp.get('status_code')}")
                    print(f"   üìù Response: {upload_resp.get('text', '')[:500]}")
                    raise HTTPException(status_code=500, detail=f"Upload ba≈üarƒ±sƒ±z: HTTP {upload_resp.get('status_code')}")
                else:
                    print(f"‚úÖ [A≈ûAMA 2.3] Upload ba≈üarƒ±lƒ±!")
                    print(f"   üì¶ Response keys: {list(upload_resp.keys()) if isinstance(upload_resp, dict) else 'N/A'}")
                    if isinstance(upload_resp, dict):
                        response_str = json.dumps(upload_resp, ensure_ascii=False, indent=2)
                        print(f"   üìä Response detaylarƒ± (ilk 1000 karakter):")
                        print(f"      {response_str[:1000]}")
                        if len(response_str) > 1000:
                            print(f"      ... (toplam {len(response_str)} karakter)")
            else:
                print("‚ùå [A≈ûAMA 2.3] Upload response None d√∂nd√º!")
                raise HTTPException(status_code=500, detail="Upload response None")
        else:
            print("‚è≠Ô∏è MevzuatGPT y√ºkleme atlandƒ± (Portal modu)")

        # Portal'a y√ºkleme (sadece 'p' ve 't' modlarƒ± i√ßin)
        mongodb_metadata_id = None
        if mode in ["p", "t"]:
            print("=" * 80)
            print("üì¶ [A≈ûAMA 3] PORTAL'A Y√úKLEME")
            print("=" * 80)
            try:
                # PDF bilgilerini al
                print("üìä [A≈ûAMA 3.1] PDF bilgileri alƒ±nƒ±yor...")
                processor = PDFProcessor()
                pdf_info = processor.analyze_pdf_structure(pdf_path)
                total_pages = pdf_info.get('total_pages', 0)
                
                # PDF dosya boyutu (MB)
                pdf_size_bytes = os.path.getsize(pdf_path)
                pdf_size_mb = round(pdf_size_bytes / (1024 * 1024), 2)
                print(f"   ‚úÖ PDF bilgileri alƒ±ndƒ±")
                print(f"      üìÑ Toplam sayfa: {total_pages}")
                print(f"      üíæ Dosya boyutu: {pdf_size_bytes:,} bytes ({pdf_size_mb} MB)")
                
                # Keywords ve description'larƒ± topla
                print("üìã [A≈ûAMA 3.2] Keywords ve descriptions toplanƒ±yor...")
                all_keywords = []
                all_descriptions = []
                
                # Mode'a g√∂re metadata kaynaƒüƒ±nƒ± belirle
                if mode == "t" and output_dir:
                    # 't' modunda pdf_sections_metadata.json'dan al
                    print("   üìÇ Metadata kaynaƒüƒ±: pdf_sections_metadata.json")
                    metadata_json_path = Path(output_dir) / "pdf_sections_metadata.json"
                    if metadata_json_path.exists():
                        try:
                            print(f"   üìÑ JSON dosyasƒ± okunuyor: {metadata_json_path}")
                            with open(metadata_json_path, 'r', encoding='utf-8') as f:
                                metadata_json = json.load(f)
                                pdf_sections = metadata_json.get('pdf_sections', [])
                                print(f"   üìä B√∂l√ºm sayƒ±sƒ±: {len(pdf_sections)}")
                                for i, section in enumerate(pdf_sections, 1):
                                    keywords = section.get('keywords', '')
                                    description = section.get('description', '')
                                    if keywords:
                                        # Keywords string ise virg√ºlle ayrƒ±lmƒ±≈ü olabilir
                                        if isinstance(keywords, str):
                                            keywords_list = [k.strip() for k in keywords.split(',') if k.strip()]
                                            all_keywords.extend(keywords_list)
                                        elif isinstance(keywords, list):
                                            all_keywords.extend(keywords)
                                    if description:
                                        all_descriptions.append(description.strip())
                            print(f"   ‚úÖ JSON'dan {len(pdf_sections)} b√∂l√ºm i≈ülendi")
                        except Exception as e:
                            print(f"   ‚ö†Ô∏è Metadata JSON okuma hatasƒ±: {str(e)}")
                    else:
                        print(f"   ‚ö†Ô∏è JSON dosyasƒ± bulunamadƒ±: {metadata_json_path}")
                else:
                    # 'p' modunda veya json yoksa analiz sonu√ßlarƒ±ndan al
                    print("   üìÇ Metadata kaynaƒüƒ±: Analiz sonu√ßlarƒ±")
                    print(f"   üìä Metadata list uzunluƒüu: {len(metadata_list)}")
                    for i, section_meta in enumerate(metadata_list, 1):
                        keywords = section_meta.get('keywords', '')
                        description = section_meta.get('description', '')
                        if keywords:
                            if isinstance(keywords, str):
                                keywords_list = [k.strip() for k in keywords.split(',') if k.strip()]
                                all_keywords.extend(keywords_list)
                            elif isinstance(keywords, list):
                                all_keywords.extend(keywords)
                        if description:
                            all_descriptions.append(description.strip())
                    print(f"   ‚úÖ {len(metadata_list)} b√∂l√ºm i≈ülendi")
                
                # Keywords ve descriptions birle≈ütir
                combined_keywords = ', '.join(all_keywords) if all_keywords else ''
                combined_description = ' '.join(all_descriptions) if all_descriptions else ''
                
                print(f"   üìä Toplanan keywords sayƒ±sƒ±: {len(all_keywords)}")
                print(f"   üìä Toplanan descriptions sayƒ±sƒ±: {len(all_descriptions)}")
                print(f"   üìù Combined keywords uzunluƒüu: {len(combined_keywords)} karakter")
                print(f"   üìù Combined description uzunluƒüu: {len(combined_description)} karakter")
                
                # A√ßƒ±klama karakter sƒ±nƒ±rƒ± (max 500 karakter)
                if len(combined_description) > 500:
                    combined_description = combined_description[:497] + "..."
                    print(f"   ‚ö†Ô∏è Description 500 karaktere kƒ±saltƒ±ldƒ±")
                
                # Ana PDF'yi bunny.net'e y√ºkle
                print("üì§ [A≈ûAMA 3.3] Ana PDF Bunny.net'e y√ºkleniyor...")
                # Dosya adƒ±nƒ± g√ºvenli hale getir (T√ºrk√ße karakterleri ƒ∞ngilizce'ye √ßevir, kaldƒ±rma)
                transliterated_name = _transliterate_turkish(document_name)
                print(f"   üìù Orijinal ad: {document_name}")
                print(f"   üìù Transliterated ad: {transliterated_name}")
                # Sadece harfler, rakamlar, bo≈üluk ve tireleri koru, diƒüer karakterleri kaldƒ±r
                safe_pdf_adi = re.sub(r'[^a-zA-Z0-9\s-]', '', transliterated_name).strip()
                # Bo≈üluklarƒ± alt √ßizgi ile deƒüi≈ütir
                safe_pdf_adi = re.sub(r'\s+', '_', safe_pdf_adi)
                # √áoklu alt √ßizgileri tek alt √ßizgi yap
                safe_pdf_adi = re.sub(r'_+', '_', safe_pdf_adi)
                bunny_filename = f"{safe_pdf_adi}_{ObjectId()}.pdf"
                print(f"   üìù G√ºvenli dosya adƒ±: {bunny_filename}")
                
                pdf_url = _upload_to_bunny(pdf_path, bunny_filename)
                
                if pdf_url:
                    print(f"‚úÖ [A≈ûAMA 3.3] Ana PDF Bunny.net'e y√ºklendi")
                    print(f"   üîó PDF URL: {pdf_url}")
                else:
                    print("‚ö†Ô∏è [A≈ûAMA 3.3] Bunny.net y√ºkleme ba≈üarƒ±sƒ±z, MongoDB i≈ülemi devam ediyor...")
                
                # pdf_adi: tekrar ba≈ülƒ±k metni olarak kaydedilecek
                pdf_adi = document_name
                
                # Slug olu≈ütur (alt tire ile, sƒ±nƒ±rsƒ±z)
                print("üîó [A≈ûAMA 3.4] URL slug olu≈üturuluyor...")
                url_slug = _create_url_slug(document_name)
                print(f"   ‚úÖ URL slug: {url_slug}")
                
                # Y√ºkleme tarihi
                now = datetime.now()
                upload_date_str = now.strftime('%Y-%m-%d')
                upload_datetime_str = now.isoformat()
                print(f"   üìÖ Y√ºkleme tarihi: {upload_datetime_str}")
                
                # PDF'den markdown formatƒ±nda metin √ßƒ±kar
                print("üìù [A≈ûAMA 3.5] PDF i√ßeriƒüi markdown formatƒ±na √ßevriliyor...")
                markdown_content = _extract_pdf_text_markdown(pdf_path)
                if not markdown_content:
                    markdown_content = "PDF i√ßeriƒüi √ßƒ±karƒ±lamadƒ±."
                    print("   ‚ö†Ô∏è PDF i√ßeriƒüi √ßƒ±karƒ±lamadƒ±, varsayƒ±lan mesaj kullanƒ±lƒ±yor")
                else:
                    content_length = len(markdown_content)
                    content_length_kb = round(content_length / 1024, 2)
                    print(f"   ‚úÖ Markdown i√ßerik olu≈üturuldu: {content_length:,} karakter ({content_length_kb} KB)")
                
                # Metadata olu≈ütur
                print("üíæ [A≈ûAMA 3.6] MongoDB metadata hazƒ±rlanƒ±yor...")
                mongodb_metadata = {
                    "pdf_adi": pdf_adi,
                    "kurum_id": req.kurum_id,  # Request'ten gelen kurum ID'sini kullan
                    "belge_turu": category,
                    "belge_durumu": "Y√ºr√ºrl√ºkte",
                    "belge_yayin_tarihi": upload_date_str,
                    "yururluluk_tarihi": upload_date_str,
                    "etiketler": "KAYSƒ∞S",
                    "anahtar_kelimeler": combined_keywords,
                    "aciklama": combined_description,
                    "url_slug": url_slug,
                    "status": "aktif",
                    "sayfa_sayisi": total_pages,
                    "dosya_boyutu_mb": pdf_size_mb,
                    "yukleme_tarihi": upload_datetime_str,
                    "pdf_url": pdf_url or ""
                }
                print(f"   ‚úÖ Metadata hazƒ±rlandƒ± ({len(mongodb_metadata)} alan)")
                
                # MongoDB'ye kaydet
                print("üíæ [A≈ûAMA 3.7] MongoDB'ye kaydediliyor...")
                mongodb_metadata_id = _save_to_mongodb(mongodb_metadata, markdown_content)
                
                if mongodb_metadata_id:
                    print(f"‚úÖ [A≈ûAMA 3.7] MongoDB kaydƒ± ba≈üarƒ±lƒ±: metadata_id={mongodb_metadata_id}")
                else:
                    print("‚ùå [A≈ûAMA 3.7] MongoDB kaydƒ± ba≈üarƒ±sƒ±z")
                    
            except Exception as e:
                print(f"‚ö†Ô∏è MongoDB/Bunny.net i≈ülemleri sƒ±rasƒ±nda hata: {str(e)}")
                # Hata olsa bile ana i≈ülemi tamamla
        
        # T√ºm i≈ülemler ba≈üarƒ±lƒ± olduktan sonra pdf_output klas√∂r√ºn√º temizle
        try:
            print("üßπ pdf_output klas√∂r√º temizleniyor...")
            pdf_output_dir = Path("pdf_output")
            if pdf_output_dir.exists():
                # Klas√∂rdeki t√ºm i√ßeriƒüi temizle (klas√∂rleri de dahil)
                for item in pdf_output_dir.iterdir():
                    if item.is_dir():
                        shutil.rmtree(item)
                    else:
                        item.unlink()
                print("‚úÖ pdf_output klas√∂r√º temizlendi")
        except Exception as e:
            print(f"‚ö†Ô∏è pdf_output temizleme hatasƒ±: {str(e)}")

        # Response mesajƒ±nƒ± mode'a g√∂re √∂zelle≈ütir
        mode_messages = {
            "m": "MevzuatGPT'ye y√ºkleme tamamlandƒ±",
            "p": "Portal'a y√ºkleme tamamlandƒ±",
            "t": "T√ºm i≈ülemler tamamlandƒ± (MevzuatGPT + Portal)"
        }
        message = mode_messages.get(mode, "ƒ∞≈ülem tamamlandƒ±")
        
        return ProcessResponse(
            success=True,
            message=message,
            data=ProcessData(
                category=category,
                institution=institution,
                document_name=document_name,
                output_dir=output_dir,
                sections_count=len(sections) if sections else 0,
                upload_response=upload_resp
            )
        )
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ƒ∞≈ülem sƒ±rasƒ±nda hata olu≈ütu: {str(e)}")


if __name__ == "__main__":
    print("üöÄ FastAPI Server ba≈ülatƒ±lƒ±yor...")
    print("üì° Server: http://0.0.0.0:8000")
    print("üìö API Docs: http://0.0.0.0:8000/docs")
    uvicorn.run(app, host="0.0.0.0", port=8000)

