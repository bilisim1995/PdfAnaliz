"""
FastAPI Server for SGK Scraper
"""
from fastapi import FastAPI, HTTPException, UploadFile, File, Form
from fastapi.responses import JSONResponse
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel, Field
from typing import Dict, Any, Optional, List
import uvicorn
from scrapers.kaysis_scraper import (
    scrape_kaysis_mevzuat,
    print_results_to_console,
    get_uploaded_documents
)
import threading
import re
import os
from pathlib import Path
import json
import subprocess
import platform
from datetime import datetime

# curl_cffi import kontrolÃ¼
try:
    from curl_cffi import requests
    CURL_CFFI_AVAILABLE = True
except ImportError:
    import requests
    CURL_CFFI_AVAILABLE = False

from pdf_processor import PDFProcessor
from deepseek_analyzer import DeepSeekAnalyzer
from utils import download_pdf_from_url, create_output_directories, create_pdf_filename, validate_pdf_file
from datetime import datetime
from pymongo import MongoClient
from pymongo.errors import ConnectionFailure, PyMongoError
from bson import ObjectId
import urllib.parse
import unicodedata
import shutil
from dotenv import load_dotenv
from bs4 import BeautifulSoup
from urllib.parse import urlparse, urljoin

# .env dosyasÄ±nÄ± yÃ¼kle
load_dotenv()

# Swagger/OpenAPI kategorileri
openapi_tags = [
    {
        "name": "SGK Scraper",
        "description": "SGK mevzuatlarÄ±nÄ± tarama, analiz ve yÃ¼kleme iÅŸlemleri."
    },
    {
        "name": "e-Devlet Scraper",
        "description": "TÃ¼rkiye.gov.tr hizmet linklerini toplama ve kaydetme."
    },
    {
        "name": "Links",
        "description": "e-Devlet linkleri iÃ§in listeleme, oluÅŸturma, gÃ¼ncelleme ve silme iÅŸlemleri."
    },
    {
        "name": "Kurumlar",
        "description": "Kurum kayÄ±tlarÄ± iÃ§in CRUD ve logo yÃ¼kleme iÅŸlemleri."
    },
    {
        "name": "Kurum Duyuru",
        "description": "Kurum duyurularÄ± iÃ§in CRUD iÅŸlemleri."
    },
    {
        "name": "MongoDB",
        "description": "Metadata ve Content koleksiyonlarÄ± iÃ§in yÃ¶netim endpointleri."
    },
    {
        "name": "Proxy",
        "description": "Proxy ayarlarÄ± iÃ§in CRUD iÅŸlemleri."
    },
    {
        "name": "Health",
        "description": "Servis saÄŸlÄ±k kontrolÃ¼."
    }
]

app = FastAPI(
    title="SGK Scraper API",
    version="1.0.0",
    description="SGK ve e-Devlet entegrasyonlarÄ± iÃ§in REST API",
    redoc_url=None,
    openapi_tags=openapi_tags
)

# CORS middleware ekle
app.add_middleware(
    CORSMiddleware,
    allow_origins=[
        "http://localhost:3004",
        "http://127.0.0.1:3004",
        "https://yonetim.mevzuatgpt.org",
        "https://app.mevzuatgpt.org",
    ],
    allow_credentials=True,
    allow_methods=["*"],  # TÃ¼m HTTP metodlarÄ±na izin ver
    allow_headers=["*"],  # TÃ¼m header'lara izin ver
)

# Son tarama sonuÃ§larÄ±ndan id -> item eÅŸlemesini tutmak iÃ§in Ã¶nbellek
# { id: { "section_title": str, "baslik": str, "link": str } }
last_item_map: Dict[int, Dict[str, Any]] = {}


def _load_config() -> Optional[Dict[str, Any]]:
    """Config dosyasÄ±nÄ± yÃ¼kler"""
    try:
        with open('config.json', 'r', encoding='utf-8') as f:
            return json.load(f)
    except Exception:
        return None


def _get_mongo_collections():
    """MongoDB client ve ilgili koleksiyonlarÄ± dÃ¶ner (metadata, content)."""
    client = _get_mongodb_client()
    if not client:
        return None, None, None
    database_name = os.getenv("MONGODB_DATABASE", "mevzuatgpt")
    metadata_collection_name = os.getenv("MONGODB_METADATA_COLLECTION", "metadata")
    content_collection_name = os.getenv("MONGODB_CONTENT_COLLECTION", "content")
    db = client[database_name]
    return client, db[metadata_collection_name], db[content_collection_name]


def normalize_for_exact_match(s: str) -> str:
    """Tam eÅŸleÅŸme iÃ§in metni normalize eder (TÃ¼rkÃ§e karakter ve boÅŸluk desteÄŸi)"""
    if not s:
        return ""
    import unicodedata
    # Unicode normalizasyonu
    s = unicodedata.normalize('NFC', s)
    s = s.replace("i\u0307", "i")
    # TÃ¼rkÃ§e kÃ¼Ã§Ã¼k harfe Ã§evirme
    s = s.replace('I', 'Ä±').replace('Ä°', 'i').lower()
    # Fazla boÅŸluklarÄ± temizle ve trim et
    s = re.sub(r'\s+', ' ', s.strip())
    return s


def to_title(s: str) -> str:
    """TÃ¼rkÃ§e karakterleri dikkate alarak Title Case'e Ã§evirir"""
    if not s:
        return ""
    import unicodedata
    # Unicode normalizasyonu
    s = unicodedata.normalize('NFC', s)
    s = s.replace("i\u0307", "i")
    # TÃ¼rkÃ§e kÃ¼Ã§Ã¼k harfe Ã§evirme
    tmp = s.replace('I', 'Ä±').replace('Ä°', 'i').lower()
    # Kelime kelime baÅŸ harf bÃ¼yÃ¼t
    words = re.split(r'(\s+)', tmp)
    titled_parts = []
    for w in words:
        if not w or w.isspace():
            titled_parts.append(w)
            continue
        first = w[0]
        rest = w[1:]
        if first == 'i':
            first_up = 'Ä°'
        elif first == 'Ä±':
            first_up = 'I'
        else:
            first_up = first.upper()
        titled_parts.append(first_up + rest)
    return ''.join(titled_parts)


class ScrapeResponse(BaseModel):
    success: bool
    message: str
    data: Dict[str, Any] = {}


class PortalScanRequest(BaseModel):
    id: str = Field(..., description="Kurum MongoDB ObjectId")
    detsis: str = Field(..., description="DETSIS numarasÄ± (KAYSÄ°S kurum ID'si)")
    type: str = Field(default="kaysis", description="Scraper tipi (varsayÄ±lan: kaysis)")

    model_config = {
        "json_schema_extra": {
            "example": {
                "id": "68bbf6df8ef4e8023c19641d",
                "detsis": "60521689",
                "type": "kaysis"
            }
        }
    }


class ProcessRequest(BaseModel):
    kurum_id: str = Field(..., description="Kurum MongoDB ObjectId")
    detsis: str = Field(..., description="DETSIS numarasÄ± (KAYSÄ°S kurum ID'si)")
    type: str = Field(default="kaysis", description="Scraper tipi (varsayÄ±lan: kaysis)")
    link: str = Field(..., description="PDF indirme linki")
    mode: str = Field(default="t", description="Ä°ÅŸlem modu: 'm' (MevzuatGPT), 'p' (Portal), 't' (TamamÄ±)")
    category: Optional[str] = Field(default=None, description="Belge kategorisi (opsiyonel)")
    document_name: Optional[str] = Field(default=None, description="Belge adÄ± (opsiyonel)")

    model_config = {
        "json_schema_extra": {
            "example": {
                "kurum_id": "68bbf6df8ef4e8023c19641d",
                "detsis": "60521689",
                "type": "kaysis",
                "link": "https://kms.kaysis.gov.tr/Home/Goster/104890",
                "mode": "t",
                "category": "Kanunlar",
                "document_name": "TÃ¼rkiye cumhuriyeti hÃ¼kÃ¼meti ile tunus cumhuriyeti hÃ¼kÃ¼meti arasÄ±nda sosyal gÃ¼venlik anlaÅŸmasÄ±nÄ±n onaylanmasÄ±nÄ±n uygun bulunduÄŸuna dair kanun"
            }
        }
    }


class ProcessData(BaseModel):
    category: str
    institution: str
    document_name: str
    output_dir: Optional[str] = None
    sections_count: int
    upload_response: Optional[Dict[str, Any]] = None


class ProcessResponse(BaseModel):
    success: bool
    message: str
    data: Optional[ProcessData] = None


@app.get("/", tags=["Health"], summary="API kÃ¶k")
async def root():
    """API root endpoint"""
    return {
        "message": "SGK Scraper API",
        "version": "1.0.0",
        "endpoints": {
            "POST /api/mevzuatgpt/scrape": "Kurum mevzuatlarÄ±nÄ± tarar ve konsola yazdÄ±rÄ±r"
        }
    }


@app.post("/api/mevzuatgpt/scrape", response_model=ScrapeResponse, tags=["SGK Scraper"], summary="Kurum mevzuat tarama")
async def scrape_mevzuatgpt(req: PortalScanRequest):
    """
    Belirtilen kurumun mevzuatlarÄ±nÄ± tarar ve sonuÃ§larÄ± konsola yazdÄ±rÄ±r.
    type parametresi ile scraper tipi belirlenir (ÅŸu an iÃ§in sadece 'kaysis' desteklenir).
    """
    try:
        print("\n" + "="*80)
        print(f"ğŸš€ API Endpoint'ten Kurum Mevzuat Tarama Ä°steÄŸi AlÄ±ndÄ± (Kurum ID: {req.id}, Type: {req.type})")
        print("="*80)
        
        # Type kontrolÃ¼
        if req.type.lower() != "kaysis":
            return ScrapeResponse(
                success=False,
                message=f"Desteklenmeyen scraper tipi: {req.type}. Åu an iÃ§in sadece 'kaysis' desteklenmektedir.",
                data={"error": "UNSUPPORTED_TYPE", "type": req.type}
            )
        
        # MongoDB'den kurum bilgisini Ã§ek
        kurum_adi = None
        try:
            client = _get_mongodb_client()
            if client:
                database_name = os.getenv("MONGODB_DATABASE", "mevzuatgpt")
                db = client[database_name]
                kurumlar_collection = db["kurumlar"]
                from bson import ObjectId
                kurum_doc = kurumlar_collection.find_one({"_id": ObjectId(req.id)})
                if kurum_doc:
                    kurum_adi = kurum_doc.get("kurum_adi", "Bilinmeyen Kurum")
                client.close()
        except Exception as e:
            print(f"âš ï¸ MongoDB'den kurum bilgisi alÄ±namadÄ±: {str(e)}")
            kurum_adi = "Bilinmeyen Kurum"
        
        print(f"ğŸ“‹ Kurum: {kurum_adi}")
        print(f"ğŸ”¢ DETSIS: {req.detsis}")
        
        # Ã–nce API'den yÃ¼klÃ¼ documents'larÄ± Ã§ek (Ã§erez kullanmadan, direkt API)
        uploaded_docs = []
        # MongoDB'den portal'da bulunan pdf_adi'larÄ± Ã§ek
        portal_docs = []
        cfg = _load_config()
        if cfg:
            token = _login_with_config(cfg)
            if token:
                api_base_url = cfg.get("api_base_url")
                print(f"ğŸ“¡ API'den yÃ¼klÃ¼ documents Ã§ekiliyor...")
                try:
                    uploaded_docs = get_uploaded_documents(api_base_url, token, use_streamlit=False)
                    print(f"âœ… {len(uploaded_docs)} document bulundu")
                    # Debug: Ä°lk birkaÃ§ belge_adi'yi yazdÄ±r
                    if uploaded_docs:
                        sample_titles = [doc.get("belge_adi", "") for doc in uploaded_docs[:5]]
                        print(f"ğŸ” DEBUG - Ã–rnek belge_adi'ler: {sample_titles}")
                except Exception as e:
                    print(f"âš ï¸ Documents Ã§ekme hatasÄ±: {str(e)}")

        # MongoDB metadata.pdf_adi -> portal_docs
        try:
            client = _get_mongodb_client()
            if client:
                database_name = os.getenv("MONGODB_DATABASE", "mevzuatgpt")
                metadata_collection_name = os.getenv("MONGODB_METADATA_COLLECTION", "metadata")
                db = client[database_name]
                metadata_collection = db[metadata_collection_name]
                # Sadece pdf_adi alanÄ±nÄ± al
                cursor = metadata_collection.find({}, {"pdf_adi": 1})
                count = 0
                for doc in cursor:
                    val = (doc.get("pdf_adi") or "").strip()
                    if val:
                        portal_docs.append({"pdf_adi": val})
                        count += 1
                client.close()
                print(f"âœ… MongoDB'den {count} pdf_adi okundu (portal karÅŸÄ±laÅŸtÄ±rmasÄ± iÃ§in)")
        except Exception as e:
            print(f"âš ï¸ MongoDB portal listesi okunamadÄ±: {str(e)}")
        
        # KAYSÄ°S scraper'Ä± kullan
        if req.type.lower() == "kaysis":
            all_sections, stats = scrape_kaysis_mevzuat(detsis=req.detsis)
            print_results_to_console(all_sections, stats)
        
        # Response hazÄ±rla (benzersiz item id'leri, uploaded durumu ve bÃ¶lÃ¼m baÅŸlÄ±k temizleme)
        item_id_counter = 1
        response_sections = []
        # Ã–nbelleÄŸi sÄ±fÄ±rla
        global last_item_map
        last_item_map = {}
        for section in all_sections:
            raw_title = section['section_title']
            # Sonunda kalan sayÄ±larÄ± temizle (Ã¶rn: "Kanunlar4" -> "Kanunlar")
            clean_title = re.sub(r"\d+\s*$", "", raw_title).strip()
            items = section.get('items', [])
            items_with_ids = []
            for item in items:
                # YÃ¼kleme durumunu belirle - tam eÅŸleÅŸme (normalize edilmiÅŸ)
                item_baslik = item.get('baslik', '')
                item_normalized = normalize_for_exact_match(item_baslik)
                is_uploaded = False
                
                # API'den gelen belgelerle karÅŸÄ±laÅŸtÄ±r (tam eÅŸleÅŸme)
                for doc in uploaded_docs:
                    belge_adi = doc.get("belge_adi", "")
                    if belge_adi:
                        belge_normalized = normalize_for_exact_match(belge_adi)
                        if item_normalized == belge_normalized:
                            is_uploaded = True
                            break
                
                # Portal (MongoDB metadata.pdf_adi karÅŸÄ±laÅŸtÄ±rmasÄ±) - tam eÅŸleÅŸme
                is_in_portal = False
                for doc in portal_docs:
                    pdf_adi = doc.get("pdf_adi", "")
                    if pdf_adi:
                        pdf_normalized = normalize_for_exact_match(pdf_adi)
                        if item_normalized == pdf_normalized:
                            is_in_portal = True
                            break
                
                # Benzersiz id ver ve Ã¶nbelleÄŸe yaz
                item_payload = {
                    "id": item_id_counter,
                    "mevzuatgpt": is_uploaded,
                    "portal": is_in_portal,
                    "baslik": item.get('baslik', ''),
                    "link": item.get('link', '')
                }
                items_with_ids.append(item_payload)

                # Ã–nbelleÄŸe kategori bilgisini de ekleyerek koy
                last_item_map[item_id_counter] = {
                    "section_title": clean_title,
                    "baslik": item_payload["baslik"],
                    "link": item_payload["link"]
                }
                item_id_counter += 1
            response_sections.append({
                "section_title": clean_title,
                "items_count": len(items_with_ids),
                "items": items_with_ids
            })
        
        # sections_stats'Ä± is_title_similar ile yeniden hesapla
        sections_stats_clean = []
        for section in all_sections:
            raw_title = section['section_title']
            clean_title = re.sub(r"\d+\s*$", "", raw_title).strip()
            items = section.get('items', [])
            
            uploaded_count = 0
            not_uploaded_count = 0
            
            for item in items:
                item_baslik = item.get('baslik', '')
                item_normalized = normalize_for_exact_match(item_baslik)
                is_uploaded = False
                
                # API'den gelen belgelerle karÅŸÄ±laÅŸtÄ±r (tam eÅŸleÅŸme)
                for doc in uploaded_docs:
                    belge_adi = doc.get("belge_adi", "")
                    if belge_adi:
                        belge_normalized = normalize_for_exact_match(belge_adi)
                        if item_normalized == belge_normalized:
                            is_uploaded = True
                            break
                
                if is_uploaded:
                    uploaded_count += 1
                else:
                    not_uploaded_count += 1
            
            sections_stats_clean.append({
                "section_title": clean_title,
                "total": len(items),
                "uploaded": uploaded_count,
                "not_uploaded": not_uploaded_count
            })
        
        response_data = {
            "total_sections": stats.get('total_sections', 0),
            "total_items": stats.get('total_items', 0),
            "uploaded_documents_count": stats.get('uploaded_documents_count', 0),
            "sections": response_sections,
            "sections_stats": sections_stats_clean
        }
        
        return ScrapeResponse(
            success=True,
            message=f"{kurum_adi} tarama iÅŸlemi baÅŸarÄ±yla tamamlandÄ±. SonuÃ§lar konsola yazdÄ±rÄ±ldÄ±.",
            data=response_data
        )
        
    except Exception as e:
        print(f"âŒ Hata oluÅŸtu: {str(e)}")
        import traceback
        traceback.print_exc()
        raise HTTPException(
            status_code=500,
            detail=f"Scraping iÅŸlemi sÄ±rasÄ±nda hata oluÅŸtu: {str(e)}"
        )


@app.post("/api/kurum/portal-scan", response_model=ScrapeResponse, tags=["SGK Scraper"], summary="Kurum portal tarama (MongoDB kontrolÃ¼)")
async def scrape_kurum_portal(req: PortalScanRequest):
    """
    Belirtilen kurumun mevzuatlarÄ±nÄ± tarar ve MongoDB metadata koleksiyonundaki kayÄ±tlarla karÅŸÄ±laÅŸtÄ±rÄ±r.
    Portal durumunu (true/false) dÃ¶ner.
    type parametresi ile scraper tipi belirlenir (ÅŸu an iÃ§in sadece 'kaysis' desteklenir).
    """
    try:
        print("\n" + "="*80)
        print(f"ğŸš€ API Endpoint'ten Kurum Portal Tarama Ä°steÄŸi AlÄ±ndÄ± (Kurum ID: {req.id}, Type: {req.type})")
        print("="*80)
        
        # Type kontrolÃ¼
        if req.type.lower() != "kaysis":
            return ScrapeResponse(
                success=False,
                message=f"Desteklenmeyen scraper tipi: {req.type}. Åu an iÃ§in sadece 'kaysis' desteklenmektedir.",
                data={"error": "UNSUPPORTED_TYPE", "type": req.type}
            )
        
        # MongoDB'den kurum bilgisini Ã§ek
        kurum_adi = None
        try:
            client = _get_mongodb_client()
            if client:
                database_name = os.getenv("MONGODB_DATABASE", "mevzuatgpt")
                db = client[database_name]
                kurumlar_collection = db["kurumlar"]
                from bson import ObjectId
                kurum_doc = kurumlar_collection.find_one({"_id": ObjectId(req.id)})
                if kurum_doc:
                    kurum_adi = kurum_doc.get("kurum_adi", "Bilinmeyen Kurum")
                client.close()
        except Exception as e:
            print(f"âš ï¸ MongoDB'den kurum bilgisi alÄ±namadÄ±: {str(e)}")
            kurum_adi = "Bilinmeyen Kurum"
        
        print(f"ğŸ“‹ Kurum: {kurum_adi}")
        print(f"ğŸ”¢ DETSIS: {req.detsis}")
        
        # MongoDB'den portal'da bulunan pdf_adi'larÄ± Ã§ek
        portal_title_set = set()
        try:
            client = _get_mongodb_client()
            if client:
                database_name = os.getenv("MONGODB_DATABASE", "mevzuatgpt")
                metadata_collection_name = os.getenv("MONGODB_METADATA_COLLECTION", "metadata")
                db = client[database_name]
                metadata_collection = db[metadata_collection_name]
                # Sadece pdf_adi alanÄ±nÄ± al
                cursor = metadata_collection.find({}, {"pdf_adi": 1})
                count = 0
                for doc in cursor:
                    val = (doc.get("pdf_adi") or "").strip()
                    if val:
                        portal_title_set.add(to_title(val))
                        count += 1
                client.close()
                print(f"âœ… MongoDB'den {count} pdf_adi okundu (portal karÅŸÄ±laÅŸtÄ±rmasÄ± iÃ§in)")
        except Exception as e:
            print(f"âš ï¸ MongoDB portal listesi okunamadÄ±: {str(e)}")
        
        # KAYSÄ°S scraper'Ä± kullan
        if req.type.lower() == "kaysis":
            all_sections, stats = scrape_kaysis_mevzuat(detsis=req.detsis)
            print_results_to_console(all_sections, stats)
        
        # Response hazÄ±rla (benzersiz item id'leri, portal durumu ve bÃ¶lÃ¼m baÅŸlÄ±k temizleme)
        item_id_counter = 1
        response_sections = []
        # Ã–nbelleÄŸi sÄ±fÄ±rla
        global last_item_map
        last_item_map = {}
        for section in all_sections:
            raw_title = section['section_title']
            # Sonunda kalan sayÄ±larÄ± temizle (Ã¶rn: "Kanunlar4" -> "Kanunlar")
            clean_title = re.sub(r"\d+\s*$", "", raw_title).strip()
            items = section.get('items', [])
            items_with_ids = []
            for item in items:
                # Portal (MongoDB metadata.pdf_adi karÅŸÄ±laÅŸtÄ±rmasÄ±) - %100 eÅŸitlik
                item_title_tc = to_title(item.get('baslik', ''))
                is_in_portal = (item_title_tc in portal_title_set)
                
                # Benzersiz id ver ve Ã¶nbelleÄŸe yaz
                item_payload = {
                    "id": item_id_counter,
                    "portal": is_in_portal,
                    "baslik": item.get('baslik', ''),
                    "link": item.get('link', '')
                }
                items_with_ids.append(item_payload)

                # Ã–nbelleÄŸe kategori bilgisini de ekleyerek koy
                last_item_map[item_id_counter] = {
                    "section_title": clean_title,
                    "baslik": item_payload["baslik"],
                    "link": item_payload["link"]
                }
                item_id_counter += 1
            response_sections.append({
                "section_title": clean_title,
                "items_count": len(items_with_ids),
                "items": items_with_ids
            })
        
        # sections_stats'Ä± portal_title_set ile hesapla
        sections_stats_clean = []
        for section in all_sections:
            raw_title = section['section_title']
            clean_title = re.sub(r"\d+\s*$", "", raw_title).strip()
            items = section.get('items', [])
            
            portal_count = 0
            not_portal_count = 0
            
            for item in items:
                item_title_tc = to_title(item.get('baslik', ''))
                is_in_portal = (item_title_tc in portal_title_set)
                if is_in_portal:
                    portal_count += 1
                else:
                    not_portal_count += 1
            
            sections_stats_clean.append({
                "section_title": clean_title,
                "total": len(items),
                "portal": portal_count,
                "not_portal": not_portal_count
            })
        
        response_data = {
            "total_sections": stats.get('total_sections', 0),
            "total_items": stats.get('total_items', 0),
            "portal_documents_count": len(portal_title_set),
            "sections": response_sections,
            "sections_stats": sections_stats_clean
        }
        
        return ScrapeResponse(
            success=True,
            message=f"{kurum_adi} portal tarama iÅŸlemi baÅŸarÄ±yla tamamlandÄ±. SonuÃ§lar konsola yazdÄ±rÄ±ldÄ±.",
            data=response_data
        )
        
    except Exception as e:
        print(f"âŒ Hata oluÅŸtu: {str(e)}")
        import traceback
        traceback.print_exc()
        raise HTTPException(
            status_code=500,
            detail=f"Portal tarama iÅŸlemi sÄ±rasÄ±nda hata oluÅŸtu: {str(e)}"
        )


@app.get("/health", tags=["Health"], summary="SaÄŸlÄ±k kontrolÃ¼")
async def health_check():
    """
    DetaylÄ± saÄŸlÄ±k kontrolÃ¼ endpoint'i.
    Servis durumu, MongoDB baÄŸlantÄ±sÄ± ve sistem bilgilerini kontrol eder.
    """
    health_status = {
        "status": "healthy",
        "service": "SGK Scraper API",
        "timestamp": datetime.now().isoformat(),
        "checks": {}
    }
    
    # 1. MongoDB baÄŸlantÄ± kontrolÃ¼
    try:
        client = _get_mongodb_client()
        if client:
            client.admin.command('ping')
            client.close()
            health_status["checks"]["mongodb"] = {
                "status": "healthy",
                "message": "MongoDB baÄŸlantÄ±sÄ± baÅŸarÄ±lÄ±"
            }
        else:
            health_status["checks"]["mongodb"] = {
                "status": "unhealthy",
                "message": "MongoDB baÄŸlantÄ±sÄ± kurulamadÄ±"
            }
            health_status["status"] = "degraded"
    except Exception as e:
        health_status["checks"]["mongodb"] = {
            "status": "unhealthy",
            "message": f"MongoDB baÄŸlantÄ± hatasÄ±: {str(e)}"
        }
        health_status["status"] = "degraded"
    
    # 2. Systemd servis durumu kontrolÃ¼
    try:
        service_name = "pdfanalyzerrag"
        
        # systemctl komutunu farklÄ± path'lerde ara (Ã¶nce en yaygÄ±n path'ler)
        systemctl_paths = ["/usr/bin/systemctl", "/bin/systemctl", "systemctl"]
        systemctl_cmd = None
        
        for path in systemctl_paths:
            try:
                if path == "systemctl":
                    # PATH'te ara
                    result = subprocess.run(
                        ["which", "systemctl"],
                        capture_output=True,
                        timeout=2
                    )
                    if result.returncode == 0:
                        systemctl_cmd = result.stdout.strip().decode('utf-8') if result.stdout else "systemctl"
                        break
                else:
                    # Direkt path'i kontrol et
                    result = subprocess.run(
                        ["test", "-f", path],
                        capture_output=True,
                        timeout=2
                    )
                    if result.returncode == 0:
                        systemctl_cmd = path
                        break
            except Exception:
                continue
        
        if systemctl_cmd:
            result = subprocess.run(
                [systemctl_cmd, "is-active", service_name],
                capture_output=True,
                text=True,
                timeout=5
            )
            if result.returncode == 0:
                service_status = result.stdout.strip()
                health_status["checks"]["systemd_service"] = {
                    "status": "healthy" if service_status == "active" else "unhealthy",
                    "message": f"Servis durumu: {service_status}",
                    "service_name": service_name
                }
                if service_status != "active":
                    health_status["status"] = "unhealthy"
            else:
                health_status["checks"]["systemd_service"] = {
                    "status": "unknown",
                    "message": f"Servis durumu kontrol edilemedi: {result.stderr.strip() if result.stderr else 'Servis bulunamadÄ± veya eriÅŸilemedi'}"
                }
        else:
            health_status["checks"]["systemd_service"] = {
                "status": "not_available",
                "message": "systemctl komutu bulunamadÄ± (systemd mevcut deÄŸil veya PATH'te yok)",
                "note": "Bu sistemde systemd servis yÃ¶netimi kullanÄ±lamÄ±yor olabilir"
            }
    except (subprocess.TimeoutExpired, FileNotFoundError, Exception) as e:
        health_status["checks"]["systemd_service"] = {
            "status": "not_available",
            "message": f"Systemd servis kontrolÃ¼ yapÄ±lamadÄ±: {str(e)}",
            "note": "Sistem systemd kullanmÄ±yor olabilir veya yetki sorunu olabilir"
        }
    
    # 3. curl_cffi kontrolÃ¼
    health_status["checks"]["curl_cffi"] = {
        "status": "available" if CURL_CFFI_AVAILABLE else "unavailable",
        "message": "curl_cffi mevcut" if CURL_CFFI_AVAILABLE else "curl_cffi kurulu deÄŸil (standart requests kullanÄ±lÄ±yor)"
    }
    
    # 4. Sistem bilgileri
    health_status["system"] = {
        "platform": platform.system(),
        "platform_release": platform.release(),
        "python_version": platform.python_version()
    }
    
    return health_status


@app.get("/api/health/logs", tags=["Health"], summary="Servis loglarÄ±nÄ± getir")
async def get_service_logs(lines: int = 100):
    """
    Systemd servis loglarÄ±nÄ± getirir.
    
    Args:
        lines: GÃ¶sterilecek log satÄ±rÄ± sayÄ±sÄ± (varsayÄ±lan: 100, maksimum: 1000)
    
    Returns:
        Servis loglarÄ± ve metadata
    """
    try:
        # SatÄ±r sayÄ±sÄ±nÄ± sÄ±nÄ±rla
        lines = max(1, min(lines, 1000))
        
        service_name = "pdfanalyzerrag"
        
        # journalctl komutunu farklÄ± path'lerde ara (Ã¶nce en yaygÄ±n path'ler)
        journalctl_paths = ["/usr/bin/journalctl", "/bin/journalctl", "journalctl"]
        journalctl_cmd = None
        
        for path in journalctl_paths:
            try:
                if path == "journalctl":
                    # PATH'te ara
                    result = subprocess.run(
                        ["which", "journalctl"],
                        capture_output=True,
                        timeout=2
                    )
                    if result.returncode == 0:
                        journalctl_cmd = result.stdout.strip().decode('utf-8') if result.stdout else "journalctl"
                        break
                else:
                    # Direkt path'i kontrol et
                    result = subprocess.run(
                        ["test", "-f", path],
                        capture_output=True,
                        timeout=2
                    )
                    if result.returncode == 0:
                        journalctl_cmd = path
                        break
            except Exception:
                continue
        
        if not journalctl_cmd:
            return {
                "success": False,
                "service_name": service_name,
                "error": "journalctl komutu bulunamadÄ± (systemd mevcut deÄŸil)",
                "timestamp": datetime.now().isoformat(),
                "logs": [],
                "raw_logs": "",
                "note": "Bu sistemde systemd log yÃ¶netimi kullanÄ±lamÄ±yor"
            }
        
        # journalctl komutunu Ã§alÄ±ÅŸtÄ±r
        result = subprocess.run(
            [journalctl_cmd, "-u", service_name, "-n", str(lines), "--no-pager"],
            capture_output=True,
            text=True,
            timeout=10
        )
        
        if result.returncode == 0:
            logs = result.stdout.strip()
            log_lines = logs.split('\n') if logs else []
            
            return {
                "success": True,
                "service_name": service_name,
                "lines_requested": lines,
                "lines_returned": len(log_lines),
                "timestamp": datetime.now().isoformat(),
                "logs": log_lines,
                "raw_logs": logs
            }
        else:
            # journalctl komutu baÅŸarÄ±sÄ±z oldu, alternatif yÃ¶ntem dene
            error_msg = result.stderr.strip() if result.stderr else "Bilinmeyen hata"
            
            # systemctl status komutunu dene
            systemctl_paths = ["/usr/bin/systemctl", "/bin/systemctl", "systemctl"]
            systemctl_cmd = None
            
            for path in systemctl_paths:
                try:
                    if path == "systemctl":
                        # PATH'te ara
                        test_result = subprocess.run(
                            ["which", "systemctl"],
                            capture_output=True,
                            timeout=2
                        )
                        if test_result.returncode == 0:
                            systemctl_cmd = test_result.stdout.strip().decode('utf-8') if test_result.stdout else "systemctl"
                            break
                    else:
                        # Direkt path'i kontrol et
                        test_result = subprocess.run(
                            ["test", "-f", path],
                            capture_output=True,
                            timeout=2
                        )
                        if test_result.returncode == 0:
                            systemctl_cmd = path
                            break
                except Exception:
                    continue
            
            if systemctl_cmd:
                try:
                    status_result = subprocess.run(
                        [systemctl_cmd, "status", service_name, "--no-pager", "-n", str(lines)],
                        capture_output=True,
                        text=True,
                        timeout=10
                    )
                    if status_result.returncode == 0:
                        logs = status_result.stdout.strip()
                        log_lines = logs.split('\n') if logs else []
                        return {
                            "success": True,
                            "service_name": service_name,
                            "lines_requested": lines,
                            "lines_returned": len(log_lines),
                            "timestamp": datetime.now().isoformat(),
                            "logs": log_lines,
                            "raw_logs": logs,
                            "note": "journalctl kullanÄ±lamadÄ±, systemctl status kullanÄ±ldÄ±"
                        }
                except Exception:
                    pass
            
            return {
                "success": False,
                "service_name": service_name,
                "error": f"Loglar alÄ±namadÄ±: {error_msg}",
                "timestamp": datetime.now().isoformat(),
                "logs": [],
                "raw_logs": ""
            }
            
    except subprocess.TimeoutExpired:
        return {
            "success": False,
            "error": "Log alma iÅŸlemi zaman aÅŸÄ±mÄ±na uÄŸradÄ±",
            "timestamp": datetime.now().isoformat(),
            "logs": [],
            "raw_logs": ""
        }
    except FileNotFoundError:
        return {
            "success": False,
            "error": "journalctl komutu bulunamadÄ± (systemd mevcut deÄŸil)",
            "timestamp": datetime.now().isoformat(),
            "logs": [],
            "raw_logs": ""
        }
    except Exception as e:
        return {
            "success": False,
            "error": f"Beklenmeyen hata: {str(e)}",
            "timestamp": datetime.now().isoformat(),
            "logs": [],
            "raw_logs": ""
        }


@app.get("/api/health/status", tags=["Health"], summary="Servis durumu detaylÄ± bilgi")
async def get_service_status():
    """
    Systemd servis durumunu detaylÄ± olarak getirir.
    
    Returns:
        Servis durumu, aktif sÃ¼re, son restart zamanÄ± vb.
    """
    try:
        service_name = "pdfanalyzerrag"
        
        # systemctl komutunu farklÄ± path'lerde ara (Ã¶nce en yaygÄ±n path'ler)
        systemctl_paths = ["/usr/bin/systemctl", "/bin/systemctl", "systemctl"]
        systemctl_cmd = None
        
        for path in systemctl_paths:
            try:
                if path == "systemctl":
                    # PATH'te ara
                    test_result = subprocess.run(
                        ["which", "systemctl"],
                        capture_output=True,
                        timeout=2
                    )
                    if test_result.returncode == 0:
                        systemctl_cmd = test_result.stdout.strip().decode('utf-8') if test_result.stdout else "systemctl"
                        break
                else:
                    # Direkt path'i kontrol et
                    test_result = subprocess.run(
                        ["test", "-f", path],
                        capture_output=True,
                        timeout=2
                    )
                    if test_result.returncode == 0:
                        systemctl_cmd = path
                        break
            except Exception:
                continue
        
        if not systemctl_cmd:
            return {
                "success": False,
                "error": "systemctl komutu bulunamadÄ± (systemd mevcut deÄŸil)",
                "timestamp": datetime.now().isoformat(),
                "note": "Bu sistemde systemd servis yÃ¶netimi kullanÄ±lamÄ±yor"
            }
        
        # systemctl status komutunu Ã§alÄ±ÅŸtÄ±r
        result = subprocess.run(
            [systemctl_cmd, "status", service_name, "--no-pager"],
            capture_output=True,
            text=True,
            timeout=10
        )
        
        status_info = {
            "success": True,
            "service_name": service_name,
            "timestamp": datetime.now().isoformat(),
            "status_output": result.stdout.strip() if result.returncode == 0 else None,
            "error": result.stderr.strip() if result.stderr and result.returncode != 0 else None
        }
        
        # systemctl show komutu ile daha detaylÄ± bilgi al
        try:
            show_result = subprocess.run(
                [systemctl_cmd, "show", service_name, "--no-pager"],
                capture_output=True,
                text=True,
                timeout=10
            )
            if show_result.returncode == 0:
                # Key-value Ã§iftlerini parse et
                details = {}
                for line in show_result.stdout.strip().split('\n'):
                    if '=' in line:
                        key, value = line.split('=', 1)
                        details[key] = value
                status_info["details"] = details
        except Exception:
            pass
        
        return status_info
        
    except subprocess.TimeoutExpired:
        return {
            "success": False,
            "error": "Servis durumu kontrolÃ¼ zaman aÅŸÄ±mÄ±na uÄŸradÄ±",
            "timestamp": datetime.now().isoformat()
        }
    except FileNotFoundError:
        return {
            "success": False,
            "error": "systemctl komutu bulunamadÄ± (systemd mevcut deÄŸil)",
            "timestamp": datetime.now().isoformat()
        }
    except Exception as e:
        return {
            "success": False,
            "error": f"Beklenmeyen hata: {str(e)}",
            "timestamp": datetime.now().isoformat()
        }


# ========================
# MongoDB Admin Endpoints
# ========================

@app.get("/api/mongo/metadata/{id}", tags=["MongoDB"], summary="Metadata getir")
async def get_metadata(id: str):
    try:
        client, metadata_col, content_col = _get_mongo_collections()
        if not client:
            raise HTTPException(status_code=500, detail="MongoDB baÄŸlantÄ±sÄ± kurulamadÄ±")
        try:
            doc = metadata_col.find_one({"_id": ObjectId(id)})
        except Exception:
            client.close()
            raise HTTPException(status_code=400, detail="GeÃ§ersiz metadata _id")
        if not doc:
            client.close()
            raise HTTPException(status_code=404, detail="Metadata bulunamadÄ±")
        doc["_id"] = str(doc["_id"])
        client.close()
        return {"success": True, "data": doc}
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Hata: {str(e)}")


@app.put("/api/mongo/metadata/{id}", tags=["MongoDB"], summary="Metadata gÃ¼ncelle")
async def update_metadata(id: str, body: Dict[str, Any]):
    try:
        client, metadata_col, content_col = _get_mongo_collections()
        if not client:
            raise HTTPException(status_code=500, detail="MongoDB baÄŸlantÄ±sÄ± kurulamadÄ±")
        # GÃ¼venli gÃ¼ncelleme (boÅŸ/null deÄŸerleri set etmeyelim)
        update_data: Dict[str, Any] = {}
        for k, v in (body or {}).items():
            if v is not None:
                update_data[k] = v
        if not update_data:
            client.close()
            return {"success": True, "message": "GÃ¼ncellenecek alan yok"}
        try:
            res = metadata_col.update_one({"_id": ObjectId(id)}, {"$set": update_data})
        except Exception:
            client.close()
            raise HTTPException(status_code=400, detail="GeÃ§ersiz metadata _id")
        client.close()
        if res.matched_count == 0:
            raise HTTPException(status_code=404, detail="Metadata bulunamadÄ±")
        return {"success": True, "modified": res.modified_count}
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Hata: {str(e)}")


@app.get("/api/mongo/content/by-metadata/{metadata_id}", tags=["MongoDB"], summary="Content getir (metadata)")
async def get_content_by_metadata(metadata_id: str):
    try:
        client, metadata_col, content_col = _get_mongo_collections()
        if not client:
            raise HTTPException(status_code=500, detail="MongoDB baÄŸlantÄ±sÄ± kurulamadÄ±")
        try:
            doc = content_col.find_one({"metadata_id": ObjectId(metadata_id)})
        except Exception:
            client.close()
            raise HTTPException(status_code=400, detail="GeÃ§ersiz metadata_id")
        if not doc:
            client.close()
            raise HTTPException(status_code=404, detail="Content bulunamadÄ±")
        doc["_id"] = str(doc["_id"])
        doc["metadata_id"] = str(doc["metadata_id"])
        client.close()
        return {"success": True, "data": doc}
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Hata: {str(e)}")


@app.put("/api/mongo/content/by-metadata/{metadata_id}", tags=["MongoDB"], summary="Content gÃ¼ncelle (metadata)")
async def update_content_by_metadata(metadata_id: str, body: Dict[str, Any]):
    try:
        client, metadata_col, content_col = _get_mongo_collections()
        if not client:
            raise HTTPException(status_code=500, detail="MongoDB baÄŸlantÄ±sÄ± kurulamadÄ±")
        new_content = (body or {}).get("icerik")
        if new_content is None:
            client.close()
            raise HTTPException(status_code=400, detail="Body iÃ§inde 'icerik' alanÄ± gerekli")
        try:
            res = content_col.update_one(
                {"metadata_id": ObjectId(metadata_id)},
                {"$set": {"icerik": new_content}}
            )
        except Exception:
            client.close()
            raise HTTPException(status_code=400, detail="GeÃ§ersiz metadata_id")
        client.close()
        if res.matched_count == 0:
            raise HTTPException(status_code=404, detail="Content bulunamadÄ±")
        return {"success": True, "modified": res.modified_count}
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Hata: {str(e)}")


@app.delete("/api/mongo/metadata/{id}", tags=["MongoDB"], summary="Portal iÃ§eriÄŸini sil (Metadata, Content ve Bunny.net PDF)")
async def delete_portal_content(id: str):
    """
    Portal iÃ§eriÄŸini tamamen siler:
    1. MongoDB metadata kaydÄ±nÄ± siler
    2. MongoDB content kaydÄ±nÄ± siler (metadata_id ile iliÅŸkili)
    3. Bunny.net'teki PDF dosyasÄ±nÄ± siler (pdf_url'den)
    
    NOT: Bu iÅŸlem sadece portal iÃ§in geÃ§erlidir, MevzuatGPT'yi etkilemez.
    """
    try:
        client, metadata_col, content_col = _get_mongo_collections()
        if not client:
            raise HTTPException(status_code=500, detail="MongoDB baÄŸlantÄ±sÄ± kurulamadÄ±")
        
        try:
            # Ã–nce metadata kaydÄ±nÄ± bul
            metadata_doc = metadata_col.find_one({"_id": ObjectId(id)})
            if not metadata_doc:
                client.close()
                raise HTTPException(status_code=404, detail="Metadata bulunamadÄ±")
            
            # pdf_url'i al (Bunny.net'ten silmek iÃ§in)
            pdf_url = metadata_doc.get("pdf_url", "")
            
            print(f"ğŸ—‘ï¸ Portal iÃ§eriÄŸi siliniyor: metadata_id={id}")
            print(f"ğŸ“„ PDF URL: {pdf_url}")
            
            # 1. Content kaydÄ±nÄ± sil (metadata_id ile iliÅŸkili)
            content_result = content_col.delete_one({"metadata_id": ObjectId(id)})
            if content_result.deleted_count > 0:
                print(f"âœ… Content kaydÄ± silindi: {content_result.deleted_count} kayÄ±t")
            else:
                print("âš ï¸ Content kaydÄ± bulunamadÄ± (zaten silinmiÅŸ olabilir)")
            
            # 2. Metadata kaydÄ±nÄ± sil
            metadata_result = metadata_col.delete_one({"_id": ObjectId(id)})
            if metadata_result.deleted_count == 0:
                client.close()
                raise HTTPException(status_code=404, detail="Metadata silinemedi (kayÄ±t bulunamadÄ±)")
            
            print(f"âœ… Metadata kaydÄ± silindi: {metadata_result.deleted_count} kayÄ±t")
            
            # 3. Bunny.net'ten PDF'i sil
            bunny_deleted = False
            if pdf_url:
                bunny_deleted = _delete_from_bunny(pdf_url)
            else:
                print("âš ï¸ PDF URL bulunamadÄ±, Bunny.net silme iÅŸlemi atlandÄ±")
            
            client.close()
            
            # SonuÃ§ mesajÄ±
            result_message = f"Portal iÃ§eriÄŸi baÅŸarÄ±yla silindi. Metadata: âœ…, Content: âœ…"
            if pdf_url:
                if bunny_deleted:
                    result_message += ", Bunny.net PDF: âœ…"
                else:
                    result_message += ", Bunny.net PDF: âš ï¸ (silme baÅŸarÄ±sÄ±z veya dosya bulunamadÄ±)"
            
            return {
                "success": True,
                "message": result_message,
                "deleted": {
                    "metadata": metadata_result.deleted_count,
                    "content": content_result.deleted_count,
                    "bunny_pdf": bunny_deleted
                }
            }
            
        except HTTPException:
            client.close()
            raise
        except Exception as e:
            client.close()
            if "invalid" in str(e).lower() or "objectid" in str(e).lower():
                raise HTTPException(status_code=400, detail="GeÃ§ersiz metadata _id")
            raise HTTPException(status_code=500, detail=f"Silme iÅŸlemi sÄ±rasÄ±nda hata: {str(e)}")
            
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Hata: {str(e)}")


@app.get("/api/mongo/metadata", tags=["MongoDB"], summary="Metadata listele")
async def list_metadata(limit: int = 100, offset: int = 0):
    """TÃ¼m metadata kayÄ±tlarÄ±nÄ± listeler (varsayÄ±lan limit 100)."""
    try:
        client, metadata_col, content_col = _get_mongo_collections()
        if not client:
            raise HTTPException(status_code=500, detail="MongoDB baÄŸlantÄ±sÄ± kurulamadÄ±")
        # GÃ¼venli limit aralÄ±ÄŸÄ±
        if limit <= 0:
            limit = 100
        if limit > 1000:
            limit = 1000
        if offset < 0:
            offset = 0
        total = metadata_col.count_documents({})
        cursor = metadata_col.find({}).skip(offset).limit(limit).sort("olusturulma_tarihi", -1)
        items = []
        for doc in cursor:
            doc["_id"] = str(doc["_id"])
            items.append(doc)
        client.close()
        return {"success": True, "total": total, "limit": limit, "offset": offset, "data": items}
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Hata: {str(e)}")


# ========================
# Kurumlar CRUD Endpoints
# ========================

def _get_kurumlar_collection():
    client = _get_mongodb_client()
    if not client:
        return None, None
    database_name = os.getenv("MONGODB_DATABASE", "mevzuatgpt")
    db = client[database_name]
    return client, db["kurumlar"]


def _get_kurum_duyuru_collection():
    client = _get_mongodb_client()
    if not client:
        return None, None
    database_name = os.getenv("MONGODB_DATABASE", "mevzuatgpt")
    db = client[database_name]
    return client, db["kurum_duyuru"]

def _get_links_collection():
    client = _get_mongodb_client()
    if not client:
        return None, None
    database_name = os.getenv("MONGODB_DATABASE", "mevzuatgpt")
    db = client[database_name]
    return client, db["links"]


@app.get("/api/mongo/kurumlar", tags=["Kurumlar"], summary="KurumlarÄ± listele")
async def list_kurumlar(limit: int = 100, offset: int = 0):
    try:
        client, col = _get_kurumlar_collection()
        if not client:
            raise HTTPException(status_code=500, detail="MongoDB baÄŸlantÄ±sÄ± kurulamadÄ±")
        if limit <= 0:
            limit = 100
        if limit > 1000:
            limit = 1000
        if offset < 0:
            offset = 0
        total = col.count_documents({})
        cursor = col.find({}).skip(offset).limit(limit).sort("olusturulma_tarihi", -1)
        items = []
        for d in cursor:
            d["_id"] = str(d["_id"])
            items.append(d)
        client.close()
        return {"success": True, "total": total, "limit": limit, "offset": offset, "data": items}
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Hata: {str(e)}")


# ==============================
# e-Devlet Link Scraper Endpoint
# ==============================

def _is_valid_url(url: str) -> bool:
    try:
        parsed = urlparse(url)
        return bool(parsed.scheme and parsed.netloc)
    except Exception:
        return False


def _is_safe_edevlet_url(url: str) -> bool:
    try:
        parsed = urlparse(url)
        if parsed.scheme not in ["http", "https"]:
            return False
        hostname = parsed.hostname or ""
        allowed_domains = [
            'turkiye.gov.tr',
            'www.turkiye.gov.tr',
            'gov.tr',
            'e-devlet.gov.tr'
        ]
        for d in allowed_domains:
            if hostname == d or hostname.endswith('.' + d):
                return True
        return False
    except Exception:
        return False


def _extract_links_from_page(base_url: str, html: bytes) -> List[Dict[str, str]]:
    soup = BeautifulSoup(html, 'html.parser')

    # Ã–ncelik verilen selektÃ¶rler
    priority_selectors = [
        'a.integratedService[href]:not([href=""])',
        'a[data-description][href]:not([href=""])'
    ]
    general_selectors = [
        '.service-item a',
        '.link-item a',
        '.menu-item a',
        'li a[href]:not([href="#"]):not([href=""])',
        '.card a',
        '.services-list a',
        '.category-list a',
        '.service-card a',
        'a[href*="/hizmet"]'
    ]

    containers = []
    for selector in priority_selectors:
        containers.extend(soup.select(selector))
    if len(containers) < 5:
        for selector in general_selectors:
            containers.extend(soup.select(selector))

    # TekilleÅŸtir
    seen = set()
    unique = []
    for el in containers:
        href = el.get('href', '')
        if href and href not in seen:
            seen.add(href)
            unique.append(el)

    results: List[Dict[str, str]] = []
    seen_urls_result = set()
    for el in unique:
        href = el.get('href', '').strip()
        if not href:
            continue
        full_url = urljoin(base_url, href)
        if full_url in seen_urls_result:
            continue

        # BaÅŸlÄ±k
        title = el.get_text(strip=True) or el.get('title', '').strip() or el.get('alt', '') or el.get('aria-label', '')
        if not title:
            # Ãœst baÅŸlÄ±klarÄ± dene
            parent = el.parent
            while parent and not title:
                if parent.name in ['h1', 'h2', 'h3', 'h4', 'h5', 'h6']:
                    title = parent.get_text(strip=True)
                    break
                parent = parent.parent
        title = (title or "BaÅŸlÄ±k bulunamadÄ±")[:200]

        # AÃ§Ä±klama
        description = el.get('data-description', '').strip()
        if not description:
            parent = el.parent
            if parent:
                siblings = parent.find_all(['p', 'span', 'div'], class_=lambda x: x and ('desc' in x.lower() or 'summary' in x.lower()))
                for s in siblings:
                    txt = s.get_text(strip=True)
                    if txt and len(txt) > 10:
                        description = txt
                        break
        if not description:
            next_elements = el.find_next_siblings(['p', 'div', 'span'])
            for ne in next_elements[:3]:
                txt = ne.get_text(strip=True)
                if txt and 20 <= len(txt) < 500:
                    description = txt
                    break
        description = (description or "AÃ§Ä±klama bulunamadÄ±")[:500]

        # Filtreler
        if not _is_valid_url(full_url):
            continue
        lower_url = full_url.lower()
        skip_patterns = ['javascript:', 'mailto:', 'tel:', '#', '.pdf', '.doc', '.docx', '.xls', '.xlsx', 'facebook.com', 'twitter.com', 'instagram.com', 'youtube.com']
        if any(p in lower_url for p in skip_patterns):
            continue
        if not title or len(title.strip()) < 3:
            continue
        if 'turkiye.gov.tr' in lower_url:
            # Daha kÄ±sa baÅŸlÄ±klara izin ver
            if len(title.strip()) < 8:
                continue

        results.append({
            "baslik": title,
            "aciklama": description,
            "url": full_url
        })
        seen_urls_result.add(full_url)

    return results


@app.post("/api/mongo/edevlet/scrape", tags=["e-Devlet Scraper"], summary="e-Devlet linkleri topla ve kaydet")
async def scrape_edevlet_links(body: Dict[str, Any]):
    """
    Verilen e-Devlet/TÃ¼rkiye.gov.tr sayfasÄ±ndan hizmet linklerini toplayÄ±p `links` koleksiyonuna kaydeder.
    Beklenen body: {"kurum_id": "ObjectId string", "url": "https://www.turkiye.gov.tr/..."}
    """
    try:
        client, col = _get_links_collection()
        if not client:
            raise HTTPException(status_code=500, detail="MongoDB baÄŸlantÄ±sÄ± kurulamadÄ±")

        kurum_id = (body or {}).get("kurum_id")
        url = (body or {}).get("url")
        if not kurum_id:
            client.close()
            raise HTTPException(status_code=400, detail="'kurum_id' zorunlu")
        if not url:
            client.close()
            raise HTTPException(status_code=400, detail="'url' zorunlu")

        # kurum_id doÄŸrula
        try:
            kurum_oid = ObjectId(str(kurum_id).strip())
        except Exception:
            client.close()
            raise HTTPException(status_code=400, detail="'kurum_id' geÃ§ersiz ObjectId")

        # URL gÃ¼venlik ve format kontrolleri
        if not _is_valid_url(url):
            client.close()
            raise HTTPException(status_code=400, detail="GeÃ§ersiz URL formatÄ±")
        if not _is_safe_edevlet_url(url):
            client.close()
            raise HTTPException(status_code=400, detail="Bu URL izin verilen domainlerde deÄŸil")

        # E-devlet scraper'Ä±nda proxy kullanÄ±lÄ±yor
        proxies = get_proxy_from_db()
        if proxies:
            print("ğŸ” E-devlet scraper'Ä±nda proxy kullanÄ±lÄ±yor...")
        else:
            print("âš ï¸ Proxy bulunamadÄ±, direkt baÄŸlantÄ± deneniyor...")
        
        # SayfayÄ± Ã§ek - GerÃ§ek bir Chrome tarayÄ±cÄ±sÄ±nÄ±n gÃ¶nderdiÄŸi tÃ¼m header'lar
        try:
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',
                'Accept-Language': 'tr-TR,tr;q=0.9,en-US;q=0.8,en;q=0.7',
                'Accept-Encoding': 'gzip, deflate, br',
                'Referer': 'https://www.google.com/',
                'Sec-Ch-Ua': '"Not_A Brand";v="8", "Chromium";v="120", "Google Chrome";v="120"',
                'Sec-Ch-Ua-Mobile': '?0',
                'Sec-Ch-Ua-Platform': '"Windows"',
                'Sec-Fetch-Dest': 'document',
                'Sec-Fetch-Mode': 'navigate',
                'Sec-Fetch-Site': 'cross-site',
                'Sec-Fetch-User': '?1',
                'Upgrade-Insecure-Requests': '1',
                'Connection': 'keep-alive',
                'Cache-Control': 'max-age=0'
            }
            
            # curl_cffi ile Chrome taklidi yap (eÄŸer mevcut ise)
            if CURL_CFFI_AVAILABLE:
                resp = requests.get(
                    url,
                    headers=headers,
                    timeout=15,
                    proxies=proxies,
                    impersonate="chrome110"  # Chrome 110 TLS fingerprint
                )
            else:
                resp = requests.get(url, headers=headers, timeout=15, proxies=proxies)
            resp.raise_for_status()
        except Exception as e:
            client.close()
            raise HTTPException(status_code=502, detail=f"HTTP hatasÄ±: {str(e)}")

        links = _extract_links_from_page(url, resp.content)

        if not links:
            client.close()
            return {"success": True, "inserted_count": 0, "data": []}

        # DokÃ¼manlarÄ± hazÄ±rla
        now_iso = datetime.now().isoformat()
        docs = []
        for item in links:
            docs.append({
                "baslik": item.get("baslik", ""),
                "aciklama": item.get("aciklama", ""),
                "url": item.get("url", ""),
                "kurum_id": kurum_oid,
                "created_at": now_iso
            })

        # Ekle (toplu)
        try:
            res = col.insert_many(docs, ordered=False)
            inserted_count = len(res.inserted_ids)
        except Exception as e:
            client.close()
            raise HTTPException(status_code=500, detail=f"MongoDB ekleme hatasÄ±: {str(e)}")

        # JSON uyumlu dÃ¶nÃ¼ÅŸ (ObjectId dÃ¶nÃ¼ÅŸtÃ¼r)
        def _to_jsonable(o):
            if isinstance(o, ObjectId):
                return str(o)
            if isinstance(o, dict):
                return {k: _to_jsonable(v) for k, v in o.items()}
            if isinstance(o, list):
                return [_to_jsonable(x) for x in o]
            return o

        all_data = _to_jsonable(docs)
        
        client.close()
        return {"success": True, "inserted_count": inserted_count, "data": all_data}
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Hata: {str(e)}")


# ==============================
# Links Koleksiyonu CRUD Endpoints
# ==============================

@app.get("/api/mongo/links", tags=["Links"], summary="Linkleri listele")
async def list_links(limit: int = 100, offset: int = 0):
    try:
        client, col = _get_links_collection()
        if not client:
            raise HTTPException(status_code=500, detail="MongoDB baÄŸlantÄ±sÄ± kurulamadÄ±")
        if limit <= 0:
            limit = 100
        if limit > 1000:
            limit = 1000
        if offset < 0:
            offset = 0
        total = col.count_documents({})
        cursor = col.find({}).skip(offset).limit(limit).sort("_id", -1)
        items = []
        for d in cursor:
            d["_id"] = str(d["_id"]) 
            if "kurum_id" in d and isinstance(d["kurum_id"], ObjectId):
                d["kurum_id"] = str(d["kurum_id"]) 
            items.append(d)
        client.close()
        return {"success": True, "total": total, "limit": limit, "offset": offset, "data": items}
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Hata: {str(e)}")


@app.post("/api/mongo/links", tags=["Links"], summary="Link oluÅŸtur")
async def create_link(body: Dict[str, Any]):
    try:
        client, col = _get_links_collection()
        if not client:
            raise HTTPException(status_code=500, detail="MongoDB baÄŸlantÄ±sÄ± kurulamadÄ±")
        data = body or {}
        baslik = (data.get("baslik") or "").strip()
        aciklama = (data.get("aciklama") or "").strip()
        url = (data.get("url") or "").strip()
        kurum_id = (data.get("kurum_id") or "").strip()

        if not baslik or not url or not kurum_id:
            client.close()
            raise HTTPException(status_code=400, detail="'baslik', 'url' ve 'kurum_id' zorunludur")
        if not _is_valid_url(url):
            client.close()
            raise HTTPException(status_code=400, detail="GeÃ§ersiz URL formatÄ±")
        try:
            kurum_oid = ObjectId(kurum_id)
        except Exception:
            client.close()
            raise HTTPException(status_code=400, detail="'kurum_id' geÃ§ersiz ObjectId")

        doc = {
            "baslik": baslik,
            "aciklama": aciklama,
            "url": url,
            "kurum_id": kurum_oid,
            "created_at": datetime.now().isoformat()
        }
        res = col.insert_one(doc)
        new_id = str(res.inserted_id)
        client.close()
        return {"success": True, "id": new_id}
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Hata: {str(e)}")


@app.get("/api/mongo/links/{id}", tags=["Links"], summary="Link getir")
async def get_link(id: str):
    try:
        client, col = _get_links_collection()
        if not client:
            raise HTTPException(status_code=500, detail="MongoDB baÄŸlantÄ±sÄ± kurulamadÄ±")
        try:
            d = col.find_one({"_id": ObjectId(id)})
        except Exception:
            client.close()
            raise HTTPException(status_code=400, detail="GeÃ§ersiz link id")
        if not d:
            client.close()
            raise HTTPException(status_code=404, detail="KayÄ±t bulunamadÄ±")
        d["_id"] = str(d["_id"]) 
        if "kurum_id" in d and isinstance(d["kurum_id"], ObjectId):
            d["kurum_id"] = str(d["kurum_id"]) 
        client.close()
        return {"success": True, "data": d}
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Hata: {str(e)}")


@app.put("/api/mongo/links/{id}", tags=["Links"], summary="Link gÃ¼ncelle")
async def update_link(id: str, body: Dict[str, Any]):
    try:
        client, col = _get_links_collection()
        if not client:
            raise HTTPException(status_code=500, detail="MongoDB baÄŸlantÄ±sÄ± kurulamadÄ±")
        update_data: Dict[str, Any] = {}
        data = body or {}

        if "baslik" in data and data["baslik"] is not None:
            update_data["baslik"] = str(data["baslik"]).strip()
        if "aciklama" in data and data["aciklama"] is not None:
            update_data["aciklama"] = str(data["aciklama"]).strip()
        if "url" in data and data["url"] is not None:
            link_url = str(data["url"]).strip()
            if not _is_valid_url(link_url):
                client.close()
                raise HTTPException(status_code=400, detail="GeÃ§ersiz URL formatÄ±")
            update_data["url"] = link_url
        if "kurum_id" in data and data["kurum_id"] is not None:
            try:
                update_data["kurum_id"] = ObjectId(str(data["kurum_id"]).strip())
            except Exception:
                client.close()
                raise HTTPException(status_code=400, detail="'kurum_id' geÃ§ersiz ObjectId")

        if not update_data:
            client.close()
            return {"success": True, "modified": 0, "message": "GÃ¼ncellenecek alan yok"}

        try:
            res = col.update_one({"_id": ObjectId(id)}, {"$set": update_data})
        except Exception:
            client.close()
            raise HTTPException(status_code=400, detail="GeÃ§ersiz link id")
        client.close()
        if res.matched_count == 0:
            raise HTTPException(status_code=404, detail="KayÄ±t bulunamadÄ±")
        return {"success": True, "modified": res.modified_count}
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Hata: {str(e)}")


@app.delete("/api/mongo/links/{id}", tags=["Links"], summary="Link sil")
async def delete_link(id: str):
    try:
        client, col = _get_links_collection()
        if not client:
            raise HTTPException(status_code=500, detail="MongoDB baÄŸlantÄ±sÄ± kurulamadÄ±")
        try:
            res = col.delete_one({"_id": ObjectId(id)})
        except Exception:
            client.close()
            raise HTTPException(status_code=400, detail="GeÃ§ersiz link id")
        client.close()
        if res.deleted_count == 0:
            raise HTTPException(status_code=404, detail="KayÄ±t bulunamadÄ±")
        return {"success": True, "deleted": res.deleted_count}
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Hata: {str(e)}")


@app.delete("/api/mongo/links/by-kurum/{kurum_id}", tags=["Links"], summary="Kurumdaki tÃ¼m linkleri sil")
async def delete_links_by_kurum(kurum_id: str):
    """
    Verilen kurum_id iÃ§in links koleksiyonundaki TÃœM kayÄ±tlarÄ± siler.
    """
    try:
        client, col = _get_links_collection()
        if not client:
            raise HTTPException(status_code=500, detail="MongoDB baÄŸlantÄ±sÄ± kurulamadÄ±")
        try:
            kurum_oid = ObjectId(kurum_id)
        except Exception:
            client.close()
            raise HTTPException(status_code=400, detail="'kurum_id' geÃ§ersiz ObjectId")
        res = col.delete_many({"kurum_id": kurum_oid})
        deleted = res.deleted_count if res else 0
        client.close()
        return {"success": True, "deleted_count": deleted}
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Hata: {str(e)}")

# Kurum Duyuru CRUD Endpoints
# ==============================

@app.get("/api/mongo/kurum-duyuru", tags=["Kurum Duyuru"], summary="Kurum duyurularÄ± listele")
async def list_kurum_duyuru(limit: int = 100, offset: int = 0):
    try:
        client, col = _get_kurum_duyuru_collection()
        if not client:
            raise HTTPException(status_code=500, detail="MongoDB baÄŸlantÄ±sÄ± kurulamadÄ±")
        if limit <= 0:
            limit = 100
        if limit > 1000:
            limit = 1000
        if offset < 0:
            offset = 0
        total = col.count_documents({})
        cursor = col.find({}).skip(offset).limit(limit).sort("_id", -1)
        items = []
        for d in cursor:
            d["_id"] = str(d["_id"])
            if "kurum_id" in d and isinstance(d["kurum_id"], ObjectId):
                d["kurum_id"] = str(d["kurum_id"])
            items.append(d)
        client.close()
        return {"success": True, "total": total, "limit": limit, "offset": offset, "data": items}
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Hata: {str(e)}")


@app.post("/api/mongo/kurum-duyuru", tags=["Kurum Duyuru"], summary="Kurum duyurusu oluÅŸtur")
async def create_kurum_duyuru(body: Dict[str, Any]):
    try:
        client, col = _get_kurum_duyuru_collection()
        if not client:
            raise HTTPException(status_code=500, detail="MongoDB baÄŸlantÄ±sÄ± kurulamadÄ±")
        data = body or {}
        kurum_id = (data.get("kurum_id") or "").strip()
        duyuru_linki = (data.get("duyuru_linki") or "").strip()
        if not kurum_id:
            client.close()
            raise HTTPException(status_code=400, detail="'kurum_id' zorunlu")
        if not duyuru_linki:
            client.close()
            raise HTTPException(status_code=400, detail="'duyuru_linki' zorunlu")
        # Basit URL kontrolÃ¼
        if not re.match(r"^https?://", duyuru_linki):
            client.close()
            raise HTTPException(status_code=400, detail="'duyuru_linki' geÃ§erli bir URL olmalÄ±")
        # kurum_id ObjectId'e dÃ¶nÃ¼ÅŸtÃ¼r
        try:
            kurum_oid = ObjectId(kurum_id)
        except Exception:
            client.close()
            raise HTTPException(status_code=400, detail="'kurum_id' geÃ§ersiz ObjectId")
        doc = {
            "kurum_id": kurum_oid,
            "duyuru_linki": duyuru_linki,
            "olusturulma_tarihi": datetime.now().isoformat()
        }
        res = col.insert_one(doc)
        new_id = str(res.inserted_id)
        client.close()
        return {"success": True, "id": new_id}
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Hata: {str(e)}")


@app.get("/api/mongo/kurum-duyuru/{id}", tags=["Kurum Duyuru"], summary="Kurum duyurusu getir")
async def get_kurum_duyuru(id: str):
    try:
        client, col = _get_kurum_duyuru_collection()
        if not client:
            raise HTTPException(status_code=500, detail="MongoDB baÄŸlantÄ±sÄ± kurulamadÄ±")
        try:
            d = col.find_one({"_id": ObjectId(id)})
        except Exception:
            client.close()
            raise HTTPException(status_code=400, detail="GeÃ§ersiz duyuru id")
        if not d:
            client.close()
            raise HTTPException(status_code=404, detail="Duyuru bulunamadÄ±")
        d["_id"] = str(d["_id"])
        if "kurum_id" in d and isinstance(d["kurum_id"], ObjectId):
            d["kurum_id"] = str(d["kurum_id"])
        client.close()
        return {"success": True, "data": d}
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Hata: {str(e)}")


@app.put("/api/mongo/kurum-duyuru/{id}", tags=["Kurum Duyuru"], summary="Kurum duyurusu gÃ¼ncelle")
async def update_kurum_duyuru(id: str, body: Dict[str, Any]):
    try:
        client, col = _get_kurum_duyuru_collection()
        if not client:
            raise HTTPException(status_code=500, detail="MongoDB baÄŸlantÄ±sÄ± kurulamadÄ±")
        update_data: Dict[str, Any] = {}
        data = body or {}
        if "kurum_id" in data and data["kurum_id"] is not None:
            try:
                update_data["kurum_id"] = ObjectId(str(data["kurum_id"]).strip())
            except Exception:
                client.close()
                raise HTTPException(status_code=400, detail="'kurum_id' geÃ§ersiz ObjectId")
        if "duyuru_linki" in data and data["duyuru_linki"] is not None:
            link = str(data["duyuru_linki"]).strip()
            if not link:
                client.close()
                raise HTTPException(status_code=400, detail="'duyuru_linki' boÅŸ olamaz")
            if not re.match(r"^https?://", link):
                client.close()
                raise HTTPException(status_code=400, detail="'duyuru_linki' geÃ§erli bir URL olmalÄ±")
            update_data["duyuru_linki"] = link
        if not update_data:
            client.close()
            return {"success": True, "modified": 0, "message": "GÃ¼ncellenecek alan yok"}
        try:
            res = col.update_one({"_id": ObjectId(id)}, {"$set": update_data})
        except Exception:
            client.close()
            raise HTTPException(status_code=400, detail="GeÃ§ersiz duyuru id")
        client.close()
        if res.matched_count == 0:
            raise HTTPException(status_code=404, detail="Duyuru bulunamadÄ±")
        return {"success": True, "modified": res.modified_count}
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Hata: {str(e)}")


@app.delete("/api/mongo/kurum-duyuru/{id}", tags=["Kurum Duyuru"], summary="Kurum duyurusu sil")
async def delete_kurum_duyuru(id: str):
    try:
        client, col = _get_kurum_duyuru_collection()
        if not client:
            raise HTTPException(status_code=500, detail="MongoDB baÄŸlantÄ±sÄ± kurulamadÄ±")
        try:
            res = col.delete_one({"_id": ObjectId(id)})
        except Exception:
            client.close()
            raise HTTPException(status_code=400, detail="GeÃ§ersiz duyuru id")
        client.close()
        if res.deleted_count == 0:
            raise HTTPException(status_code=404, detail="Duyuru bulunamadÄ±")
        return {"success": True, "deleted": res.deleted_count}
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Hata: {str(e)}")


# ==============================
# Proxy Koleksiyonu YardÄ±mcÄ± FonksiyonlarÄ±
# ==============================

def _get_proxy_collection():
    """Proxy koleksiyonunu dÃ¶ner"""
    client = _get_mongodb_client()
    if not client:
        return None, None
    database_name = os.getenv("MONGODB_DATABASE", "mevzuatgpt")
    db = client[database_name]
    return client, db["proxies"]


def get_proxy_from_db() -> Optional[Dict[str, str]]:
    """
    MongoDB'den aktif proxy bilgilerini Ã§eker.
    Returns: {'http': 'http://user:pass@host:port', 'https': 'http://user:pass@host:port'} veya None
    """
    try:
        client, col = _get_proxy_collection()
        if not client:
            return None
        
        # Aktif proxy'yi bul (is_active=True olan ilk kayÄ±t)
        proxy_doc = col.find_one({"is_active": True}, sort=[("created_at", -1)])
        client.close()
        
        if not proxy_doc:
            return None
        
        host = proxy_doc.get("host", "").strip()
        port = proxy_doc.get("port", "").strip()
        username = proxy_doc.get("username", "").strip()
        password = proxy_doc.get("password", "").strip()
        
        if not host or not port:
            return None
        
        # Proxy URL'ini oluÅŸtur
        if username and password:
            proxy_auth = f"{username}:{password}"
            proxy_url = f"{proxy_auth}@{host}:{port}"
        else:
            proxy_url = f"{host}:{port}"
        
        return {
            'http': f'http://{proxy_url}',
            'https': f'http://{proxy_url}'
        }
    except Exception as e:
        print(f"âš ï¸ Proxy bilgisi Ã§ekilemedi: {str(e)}")
        return None


# ==============================
# Proxy Koleksiyonu CRUD Endpoints
# ==============================

@app.get("/api/mongo/proxies", tags=["Proxy"], summary="Proxy listele")
async def list_proxies(limit: int = 100, offset: int = 0):
    try:
        client, col = _get_proxy_collection()
        if not client:
            raise HTTPException(status_code=500, detail="MongoDB baÄŸlantÄ±sÄ± kurulamadÄ±")
        
        # Password'u gizle
        cursor = col.find().sort("created_at", -1).skip(offset).limit(limit)
        proxies = []
        for doc in cursor:
            proxy_data = {
                "id": str(doc["_id"]),
                "host": doc.get("host", ""),
                "port": doc.get("port", ""),
                "username": doc.get("username", ""),
                "password": "***" if doc.get("password") else "",  # Password'u gizle
                "is_active": doc.get("is_active", False),
                "created_at": doc.get("created_at", ""),
                "updated_at": doc.get("updated_at", "")
            }
            proxies.append(proxy_data)
        
        total = col.count_documents({})
        client.close()
        return {"success": True, "total": total, "data": proxies}
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Hata: {str(e)}")


@app.post("/api/mongo/proxies", tags=["Proxy"], summary="Proxy oluÅŸtur")
async def create_proxy(body: Dict[str, Any]):
    try:
        client, col = _get_proxy_collection()
        if not client:
            raise HTTPException(status_code=500, detail="MongoDB baÄŸlantÄ±sÄ± kurulamadÄ±")
        
        data = body or {}
        host = (data.get("host") or "").strip()
        port = (data.get("port") or "").strip()
        username = (data.get("username") or "").strip()
        password = (data.get("password") or "").strip()
        is_active = data.get("is_active", True)
        
        if not host or not port:
            client.close()
            raise HTTPException(status_code=400, detail="'host' ve 'port' zorunludur")
        
        # Port'un sayÄ±sal olup olmadÄ±ÄŸÄ±nÄ± kontrol et
        try:
            port_int = int(port)
            if port_int < 1 or port_int > 65535:
                client.close()
                raise HTTPException(status_code=400, detail="Port 1-65535 arasÄ±nda olmalÄ±dÄ±r")
        except ValueError:
            client.close()
            raise HTTPException(status_code=400, detail="Port geÃ§erli bir sayÄ± olmalÄ±dÄ±r")
        
        # EÄŸer yeni proxy aktif yapÄ±lÄ±yorsa, diÄŸer aktif proxy'leri pasif yap
        if is_active:
            col.update_many({"is_active": True}, {"$set": {"is_active": False, "updated_at": datetime.now().isoformat()}})
        
        doc = {
            "host": host,
            "port": port,
            "username": username,
            "password": password,
            "is_active": is_active,
            "created_at": datetime.now().isoformat(),
            "updated_at": datetime.now().isoformat()
        }
        
        res = col.insert_one(doc)
        new_id = str(res.inserted_id)
        client.close()
        return {"success": True, "id": new_id}
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Hata: {str(e)}")


@app.get("/api/mongo/proxies/{id}", tags=["Proxy"], summary="Proxy getir")
async def get_proxy(id: str):
    try:
        client, col = _get_proxy_collection()
        if not client:
            raise HTTPException(status_code=500, detail="MongoDB baÄŸlantÄ±sÄ± kurulamadÄ±")
        
        try:
            doc = col.find_one({"_id": ObjectId(id)})
        except Exception:
            client.close()
            raise HTTPException(status_code=400, detail="GeÃ§ersiz proxy id")
        
        client.close()
        if not doc:
            raise HTTPException(status_code=404, detail="Proxy bulunamadÄ±")
        
        return {
            "success": True,
            "data": {
                "id": str(doc["_id"]),
                "host": doc.get("host", ""),
                "port": doc.get("port", ""),
                "username": doc.get("username", ""),
                "password": "***" if doc.get("password") else "",  # Password'u gizle
                "is_active": doc.get("is_active", False),
                "created_at": doc.get("created_at", ""),
                "updated_at": doc.get("updated_at", "")
            }
        }
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Hata: {str(e)}")


@app.put("/api/mongo/proxies/{id}", tags=["Proxy"], summary="Proxy gÃ¼ncelle")
async def update_proxy(id: str, body: Dict[str, Any]):
    try:
        client, col = _get_proxy_collection()
        if not client:
            raise HTTPException(status_code=500, detail="MongoDB baÄŸlantÄ±sÄ± kurulamadÄ±")
        
        data = body or {}
        update_data = {"updated_at": datetime.now().isoformat()}
        
        if "host" in data:
            host = (data.get("host") or "").strip()
            if not host:
                client.close()
                raise HTTPException(status_code=400, detail="'host' boÅŸ olamaz")
            update_data["host"] = host
        
        if "port" in data:
            port = (data.get("port") or "").strip()
            if not port:
                client.close()
                raise HTTPException(status_code=400, detail="'port' boÅŸ olamaz")
            try:
                port_int = int(port)
                if port_int < 1 or port_int > 65535:
                    client.close()
                    raise HTTPException(status_code=400, detail="Port 1-65535 arasÄ±nda olmalÄ±dÄ±r")
            except ValueError:
                client.close()
                raise HTTPException(status_code=400, detail="Port geÃ§erli bir sayÄ± olmalÄ±dÄ±r")
            update_data["port"] = port
        
        if "username" in data:
            update_data["username"] = (data.get("username") or "").strip()
        
        if "password" in data:
            update_data["password"] = (data.get("password") or "").strip()
        
        if "is_active" in data:
            is_active = data.get("is_active", False)
            # EÄŸer proxy aktif yapÄ±lÄ±yorsa, diÄŸer aktif proxy'leri pasif yap
            if is_active:
                col.update_many(
                    {"is_active": True, "_id": {"$ne": ObjectId(id)}},
                    {"$set": {"is_active": False, "updated_at": datetime.now().isoformat()}}
                )
            update_data["is_active"] = is_active
        
        if not update_data or len(update_data) == 1:  # Sadece updated_at varsa
            client.close()
            return {"success": True, "modified": 0, "message": "GÃ¼ncellenecek alan yok"}
        
        try:
            res = col.update_one({"_id": ObjectId(id)}, {"$set": update_data})
        except Exception:
            client.close()
            raise HTTPException(status_code=400, detail="GeÃ§ersiz proxy id")
        
        client.close()
        if res.matched_count == 0:
            raise HTTPException(status_code=404, detail="Proxy bulunamadÄ±")
        return {"success": True, "modified": res.modified_count}
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Hata: {str(e)}")


@app.delete("/api/mongo/proxies/{id}", tags=["Proxy"], summary="Proxy sil")
async def delete_proxy(id: str):
    try:
        client, col = _get_proxy_collection()
        if not client:
            raise HTTPException(status_code=500, detail="MongoDB baÄŸlantÄ±sÄ± kurulamadÄ±")
        
        try:
            res = col.delete_one({"_id": ObjectId(id)})
        except Exception:
            client.close()
            raise HTTPException(status_code=400, detail="GeÃ§ersiz proxy id")
        
        client.close()
        if res.deleted_count == 0:
            raise HTTPException(status_code=404, detail="Proxy bulunamadÄ±")
        return {"success": True, "deleted": res.deleted_count}
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Hata: {str(e)}")


@app.post("/api/mongo/proxies/test", tags=["Proxy"], summary="Proxy baÄŸlantÄ± testi (KAYSÄ°S)")
async def test_proxy_connection(body: Dict[str, Any]):
    """
    Proxy baÄŸlantÄ±sÄ±nÄ± KAYSÄ°S sitesine test eder.
    curl_cffi kullanarak Chrome tarayÄ±cÄ±sÄ±nÄ± taklit eder ve WAF engellemelerini aÅŸar.
    
    Args:
        body: {"id": "proxy_id", "detsis": "22620739"} (detsis opsiyonel, varsayÄ±lan: 22620739 - SGK)
    
    Returns:
        Test sonuÃ§larÄ± (IP bilgisi, baÄŸlantÄ± durumu, hata mesajlarÄ±)
    """
    try:
        # Body'den proxy ID'yi al
        if not body or not body.get("id"):
            raise HTTPException(status_code=400, detail="Body'de 'id' alanÄ± zorunludur")
        
        proxy_id = str(body.get("id")).strip()
        if not proxy_id:
            raise HTTPException(status_code=400, detail="Proxy ID boÅŸ olamaz")
        
        # Proxy bilgilerini MongoDB'den Ã§ek
        client, col = _get_proxy_collection()
        if not client:
            raise HTTPException(status_code=500, detail="MongoDB baÄŸlantÄ±sÄ± kurulamadÄ±")
        
        try:
            proxy_doc = col.find_one({"_id": ObjectId(proxy_id)})
        except Exception:
            client.close()
            raise HTTPException(status_code=400, detail="GeÃ§ersiz proxy id formatÄ±")
        
        client.close()
        
        if not proxy_doc:
            raise HTTPException(status_code=404, detail=f"Proxy bulunamadÄ± (ID: {proxy_id})")
        
        # Proxy bilgilerini hazÄ±rla
        host = proxy_doc.get("host", "").strip()
        port = proxy_doc.get("port", "").strip()
        username = proxy_doc.get("username", "").strip()
        password = proxy_doc.get("password", "").strip()
        
        if not host or not port:
            raise HTTPException(status_code=400, detail="Proxy bilgileri eksik (host veya port)")
        
        # Proxy URL'ini oluÅŸtur
        if username and password:
            proxy_auth = f"{username}:{password}"
            proxy_url = f"{proxy_auth}@{host}:{port}"
        else:
            proxy_url = f"{host}:{port}"
        
        proxies = {
            'http': f'http://{proxy_url}',
            'https': f'http://{proxy_url}'
        }
        
        # DETSIS numarasÄ±nÄ± al (varsayÄ±lan: 22620739 - SGK)
        detsis = "22620739"
        if body.get("detsis"):
            detsis = str(body.get("detsis")).strip()
        
        test_url = f"https://kms.kaysis.gov.tr/Home/Kurum/{detsis}"
        
        result = {
            "success": False,
            "proxy_id": proxy_id,
            "proxy_host": host,
            "proxy_port": port,
            "test_url": test_url,
            "detsis": detsis,
            "ip_info": None,
            "connection_status": None,
            "http_status": None,
            "response_size": None,
            "error": None,
            "curl_cffi_available": CURL_CFFI_AVAILABLE
        }
        
        # 1. IP kontrolÃ¼
        try:
            print(f"ğŸŒ Proxy IP adresi kontrol ediliyor... (Proxy ID: {proxy_id})")
            ip_response = requests.get(
                'https://ipv4.icanhazip.com',
                proxies=proxies,
                timeout=10,
                impersonate="chrome110" if CURL_CFFI_AVAILABLE else None
            )
            ip_address = ip_response.text.strip()
            
            # IP lokasyon bilgisini al
            try:
                geo_response = requests.get(
                    f'http://ip-api.com/json/{ip_address}?fields=status,country,countryCode,city,query',
                    proxies=proxies,
                    timeout=10,
                    impersonate="chrome110" if CURL_CFFI_AVAILABLE else None
                )
                geo_data = geo_response.json()
                
                if geo_data.get('status') == 'success':
                    result["ip_info"] = {
                        "ip": ip_address,
                        "country": geo_data.get('country', 'Bilinmiyor'),
                        "country_code": geo_data.get('countryCode', 'Bilinmiyor'),
                        "city": geo_data.get('city', 'Bilinmiyor'),
                        "is_turkey": geo_data.get('countryCode') == 'TR'
                    }
                else:
                    result["ip_info"] = {"ip": ip_address}
            except Exception as e:
                result["ip_info"] = {"ip": ip_address, "error": str(e)}
        except Exception as e:
            result["ip_info"] = {"error": f"IP kontrolÃ¼ baÅŸarÄ±sÄ±z: {str(e)}"}
        
        # 2. KAYSÄ°S baÄŸlantÄ± testi
        try:
            print(f"ğŸŒ KAYSÄ°S sitesine baÄŸlanÄ±lÄ±yor... (Proxy ID: {proxy_id})")
            
            # GerÃ§ek bir Chrome tarayÄ±cÄ±sÄ±nÄ±n gÃ¶nderdiÄŸi tÃ¼m header'lar
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',
                'Accept-Language': 'tr-TR,tr;q=0.9,en-US;q=0.8,en;q=0.7',
                'Accept-Encoding': 'gzip, deflate, br',
                'Referer': 'https://www.google.com/',
                'Sec-Ch-Ua': '"Not_A Brand";v="8", "Chromium";v="120", "Google Chrome";v="120"',
                'Sec-Ch-Ua-Mobile': '?0',
                'Sec-Ch-Ua-Platform': '"Windows"',
                'Sec-Fetch-Dest': 'document',
                'Sec-Fetch-Mode': 'navigate',
                'Sec-Fetch-Site': 'cross-site',
                'Sec-Fetch-User': '?1',
                'Upgrade-Insecure-Requests': '1',
                'Connection': 'keep-alive',
                'Cache-Control': 'max-age=0'
            }
            
            # curl_cffi ile Chrome taklidi yap (eÄŸer mevcut ise)
            if CURL_CFFI_AVAILABLE:
                response = requests.get(
                    test_url,
                    headers=headers,
                    proxies=proxies,
                    timeout=30,
                    impersonate="chrome110"  # Chrome 110 TLS fingerprint
                )
            else:
                response = requests.get(test_url, headers=headers, timeout=30, proxies=proxies)
            
            result["http_status"] = response.status_code
            result["response_size"] = len(response.content)
            
            if response.status_code == 200:
                result["success"] = True
                result["connection_status"] = "success"
                
                # HTML iÃ§eriÄŸinde baÅŸarÄ±lÄ± yÃ¼kleme iÅŸaretleri kontrol et
                content = response.text.lower()
                if 'accordion' in content or 'panel' in content or 'kurum' in content:
                    result["content_check"] = "KAYSÄ°S yapÄ±sÄ± tespit edildi"
                else:
                    result["content_check"] = "Sayfa yÃ¼klendi ancak beklenen iÃ§erik bulunamadÄ±"
            else:
                result["connection_status"] = "failed"
                result["error"] = f"HTTP {response.status_code}: {response.text[:200] if response.text else 'BoÅŸ yanÄ±t'}"
                
        except requests.exceptions.ProxyError as e:
            result["connection_status"] = "proxy_error"
            result["error"] = f"Proxy hatasÄ±: {str(e)}"
        except requests.exceptions.Timeout:
            result["connection_status"] = "timeout"
            result["error"] = "Zaman aÅŸÄ±mÄ±: BaÄŸlantÄ± 30 saniye iÃ§inde tamamlanamadÄ±"
        except requests.exceptions.ConnectionError as e:
            result["connection_status"] = "connection_error"
            result["error"] = f"BaÄŸlantÄ± hatasÄ±: {str(e)}"
        except Exception as e:
            result["connection_status"] = "error"
            result["error"] = f"Beklenmeyen hata: {str(e)}"
        
        return result
        
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Test sÄ±rasÄ±nda hata: {str(e)}")

@app.post("/api/mongo/kurumlar", tags=["Kurumlar"], summary="Kurum oluÅŸtur")
async def create_kurum(
    kurum_adi: str = Form(...),
    aciklama: Optional[str] = Form(None),
    detsis: Optional[str] = Form(None),
    logo: Optional[UploadFile] = File(None)
):
    """
    Yeni kurum oluÅŸturur (multipart/form-data).
    - kurum_adi: Zorunlu
    - aciklama: Opsiyonel
    - detsis: Opsiyonel (DETSIS numarasÄ±)
    - logo: Opsiyonel (PNG, JPG, JPEG, SVG, GIF, WEBP)
    """
    try:
        client, col = _get_kurumlar_collection()
        if not client:
            raise HTTPException(status_code=500, detail="MongoDB baÄŸlantÄ±sÄ± kurulamadÄ±")
        
        if not kurum_adi or not str(kurum_adi).strip():
            client.close()
            raise HTTPException(status_code=400, detail="'kurum_adi' zorunlu")
        
        # Logo varsa yÃ¼kle
        logo_url = None
        if logo:
            # Dosya formatÄ±nÄ± kontrol et
            allowed_extensions = {'.png', '.jpg', '.jpeg', '.svg', '.gif', '.webp'}
            file_extension = Path(logo.filename or '').suffix.lower()
            
            if file_extension not in allowed_extensions:
                client.close()
                raise HTTPException(
                    status_code=400,
                    detail=f"Desteklenmeyen dosya formatÄ±. Ä°zin verilen formatlar: {', '.join(allowed_extensions)}"
                )
            
            # Content type'Ä± belirle
            content_type_map = {
                '.png': 'image/png',
                '.jpg': 'image/jpeg',
                '.jpeg': 'image/jpeg',
                '.svg': 'image/svg+xml',
                '.gif': 'image/gif',
                '.webp': 'image/webp'
            }
            content_type = content_type_map.get(file_extension, logo.content_type or 'image/png')
            
            # Dosya iÃ§eriÄŸini oku
            file_data = await logo.read()
            
            # Dosya adÄ±nÄ± oluÅŸtur
            safe_filename = _transliterate_turkish(kurum_adi)
            safe_filename = re.sub(r'[^a-zA-Z0-9\s-]', '', safe_filename).strip()
            safe_filename = re.sub(r'\s+', '_', safe_filename)
            safe_filename = re.sub(r'_+', '_', safe_filename)
            
            # GeÃ§ici ID oluÅŸtur (henÃ¼z MongoDB'de yok)
            temp_id = str(ObjectId())
            logo_filename = f"{safe_filename}_{temp_id}{file_extension}"
            
            # Bunny.net'e yÃ¼kle
            logo_url = _upload_logo_to_bunny(file_data, logo_filename, content_type)
            
            if not logo_url:
                client.close()
                raise HTTPException(status_code=500, detail="Logo Bunny.net'e yÃ¼klenemedi")
        
        # MongoDB'ye kaydet
        data = {
            "kurum_adi": kurum_adi.strip(),
            "olusturulma_tarihi": datetime.now().isoformat()
        }
        
        if aciklama:
            data["aciklama"] = aciklama.strip()
        
        if detsis:
            data["detsis"] = detsis.strip()
        
        if logo_url:
            data["kurum_logo"] = logo_url
        
        res = col.insert_one(data)
        new_id = str(res.inserted_id)
        client.close()
        
        return {
            "success": True,
            "id": new_id,
            "logo_url": logo_url
        }
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Hata: {str(e)}")


@app.get("/api/mongo/kurumlar/{id}", tags=["Kurumlar"], summary="Kurum getir")
async def get_kurum(id: str):
    try:
        client, col = _get_kurumlar_collection()
        if not client:
            raise HTTPException(status_code=500, detail="MongoDB baÄŸlantÄ±sÄ± kurulamadÄ±")
        try:
            d = col.find_one({"_id": ObjectId(id)})
        except Exception:
            client.close()
            raise HTTPException(status_code=400, detail="GeÃ§ersiz kurum id")
        if not d:
            client.close()
            raise HTTPException(status_code=404, detail="Kurum bulunamadÄ±")
        d["_id"] = str(d["_id"])
        client.close()
        return {"success": True, "data": d}
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Hata: {str(e)}")


@app.put("/api/mongo/kurumlar/{id}", tags=["Kurumlar"], summary="Kurum gÃ¼ncelle (logo destekli)")
async def update_kurum(
    id: str,
    kurum_adi: Optional[str] = Form(None),
    aciklama: Optional[str] = Form(None),
    detsis: Optional[str] = Form(None),
    logo: Optional[UploadFile] = File(None)
):
    """
    Kurum bilgilerini gÃ¼nceller (multipart/form-data).
    - kurum_adi: Opsiyonel (gÃ¶nderilirse gÃ¼ncellenir)
    - aciklama: Opsiyonel (gÃ¶nderilirse gÃ¼ncellenir)
    - detsis: Opsiyonel (gÃ¶nderilirse gÃ¼ncellenir - DETSIS numarasÄ±)
    - logo: Opsiyonel (gÃ¶nderilirse yÃ¼klenir ve gÃ¼ncellenir) (PNG, JPG, JPEG, SVG, GIF, WEBP)
    """
    try:
        client, col = _get_kurumlar_collection()
        if not client:
            raise HTTPException(status_code=500, detail="MongoDB baÄŸlantÄ±sÄ± kurulamadÄ±")
        
        # Kurum var mÄ± kontrol et
        try:
            kurum = col.find_one({"_id": ObjectId(id)})
        except Exception:
            client.close()
            raise HTTPException(status_code=400, detail="GeÃ§ersiz kurum id")
        
        if not kurum:
            client.close()
            raise HTTPException(status_code=404, detail="Kurum bulunamadÄ±")
        
        update_data: Dict[str, Any] = {}
        
        # Logo varsa yÃ¼kle
        if logo:
            # Dosya formatÄ±nÄ± kontrol et
            allowed_extensions = {'.png', '.jpg', '.jpeg', '.svg', '.gif', '.webp'}
            file_extension = Path(logo.filename or '').suffix.lower()
            
            if file_extension not in allowed_extensions:
                client.close()
                raise HTTPException(
                    status_code=400,
                    detail=f"Desteklenmeyen dosya formatÄ±. Ä°zin verilen formatlar: {', '.join(allowed_extensions)}"
                )
            
            # Content type'Ä± belirle
            content_type_map = {
                '.png': 'image/png',
                '.jpg': 'image/jpeg',
                '.jpeg': 'image/jpeg',
                '.svg': 'image/svg+xml',
                '.gif': 'image/gif',
                '.webp': 'image/webp'
            }
            content_type = content_type_map.get(file_extension, logo.content_type or 'image/png')
            
            # Dosya iÃ§eriÄŸini oku
            file_data = await logo.read()
            
            # Dosya adÄ±nÄ± oluÅŸtur (kurum adÄ±ndan veya mevcut kurum adÄ±ndan)
            kurum_adi_for_filename = kurum_adi.strip() if kurum_adi else kurum.get('kurum_adi', 'kurum')
            safe_filename = _transliterate_turkish(kurum_adi_for_filename)
            safe_filename = re.sub(r'[^a-zA-Z0-9\s-]', '', safe_filename).strip()
            safe_filename = re.sub(r'\s+', '_', safe_filename)
            safe_filename = re.sub(r'_+', '_', safe_filename)
            logo_filename = f"{safe_filename}_{id}{file_extension}"
            
            # Bunny.net'e yÃ¼kle
            logo_url = _upload_logo_to_bunny(file_data, logo_filename, content_type)
            
            if not logo_url:
                client.close()
                raise HTTPException(status_code=500, detail="Logo Bunny.net'e yÃ¼klenemedi")
            
            update_data["kurum_logo"] = logo_url
        
        # DiÄŸer alanlarÄ± gÃ¼ncelle
        if kurum_adi is not None:
            update_data["kurum_adi"] = kurum_adi.strip()
        
        if aciklama is not None:
            update_data["aciklama"] = aciklama.strip()
        
        if detsis is not None:
            update_data["detsis"] = detsis.strip()
        
        if not update_data:
            client.close()
            return {"success": True, "modified": 0, "message": "GÃ¼ncellenecek alan yok"}
        
        # MongoDB'yi gÃ¼ncelle
        try:
            res = col.update_one({"_id": ObjectId(id)}, {"$set": update_data})
        except Exception:
            client.close()
            raise HTTPException(status_code=400, detail="GeÃ§ersiz kurum id")
        
        client.close()
        
        if res.matched_count == 0:
            raise HTTPException(status_code=404, detail="Kurum bulunamadÄ±")
        
        return {
            "success": True,
            "modified": res.modified_count,
            "logo_url": update_data.get("kurum_logo") if "kurum_logo" in update_data else None
        }
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Hata: {str(e)}")


@app.delete("/api/mongo/kurumlar/{id}", tags=["Kurumlar"], summary="Kurum sil")
async def delete_kurum(id: str):
    try:
        client, col = _get_kurumlar_collection()
        if not client:
            raise HTTPException(status_code=500, detail="MongoDB baÄŸlantÄ±sÄ± kurulamadÄ±")
        try:
            res = col.delete_one({"_id": ObjectId(id)})
        except Exception:
            client.close()
            raise HTTPException(status_code=400, detail="GeÃ§ersiz kurum id")
        client.close()
        if res.deleted_count == 0:
            raise HTTPException(status_code=404, detail="Kurum bulunamadÄ±")
        return {"success": True, "deleted": res.deleted_count}
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Hata: {str(e)}")


def _get_deepseek_api_key() -> Optional[str]:
    # 1) Env
    env_key = os.getenv("DEEPSEEK_API_KEY", "").strip()
    if env_key:
        return env_key
    # 2) config.json
    cfg = _load_config()
    if cfg:
        cfg_key = (cfg.get("deepseek_api_key") or "").strip()
        if cfg_key:
            return cfg_key
    # 3) Replit local state (opsiyonel)
    try:
        replit_state = Path(".local/state/replit/agent/filesystem/filesystem_state.json")
        if replit_state.exists():
            content = replit_state.read_text(encoding='utf-8', errors='ignore')
            # Basit bir anahtar aramasÄ±
            import re as _re
            m = _re.search(r"sk-[A-Za-z0-9]{32,}", content)
            if m:
                return m.group(0)
    except Exception:
        pass
    return None


def _login_with_config(cfg: Dict[str, Any]) -> Optional[str]:
    try:
        api_base_url = cfg.get("api_base_url")
        email = cfg.get("admin_email")
        password = cfg.get("admin_password")
        if not all([api_base_url, email, password]):
            return None
        # API isteklerinde proxy kullanÄ±lmÄ±yor
        
        login_url = f"{api_base_url.rstrip('/')}/api/auth/login"
        resp = requests.post(login_url, headers={"Content-Type": "application/json"}, json={
            "email": email,
            "password": password
        }, timeout=60)
        if resp.status_code == 200:
            data = resp.json()
            return data.get("access_token")
        return None
    except Exception:
        return None


def _transliterate_turkish(text: str) -> str:
    """TÃ¼rkÃ§e karakterleri Ä°ngilizce karÅŸÄ±lÄ±klarÄ±na Ã§evirir (kaldÄ±rmaz)"""
    if not text:
        return ""
    
    # TÃ¼rkÃ§e karakterleri Ä°ngilizce karÅŸÄ±lÄ±klarÄ±na Ã§evir
    char_map = {
        'Ã§': 'c', 'ÄŸ': 'g', 'Ä±': 'i', 'Ã¶': 'o', 'ÅŸ': 's', 'Ã¼': 'u',
        'Ã‡': 'C', 'Ä': 'G', 'Ä°': 'I', 'Ã–': 'O', 'Å': 'S', 'Ãœ': 'U'
    }
    
    result = text
    for tr_char, en_char in char_map.items():
        result = result.replace(tr_char, en_char)
    
    return result


def _create_url_slug(text: str) -> str:
    """URL-friendly slug oluÅŸturur (alt tire ile, sÄ±nÄ±rsÄ±z)"""
    if not text:
        return "pdf_document"
    
    # TÃ¼rkÃ§e karakterleri Ä°ngilizce karÅŸÄ±lÄ±klarÄ±na Ã§evir
    slug = _transliterate_turkish(text)
    
    # Unicode normalize
    slug = unicodedata.normalize('NFKD', slug)
    
    # KÃ¼Ã§Ã¼k harf yap
    slug = slug.lower()
    
    # Sadece harfler, rakamlar ve boÅŸluk
    slug = re.sub(r'[^a-z0-9\s]', '', slug)
    
    # Ã‡oklu boÅŸluklarÄ± alt tire ile deÄŸiÅŸtir
    slug = re.sub(r'\s+', '_', slug)
    
    # Ã‡oklu alt tireleri tek alt tire yap
    slug = re.sub(r'_+', '_', slug)
    
    # BaÅŸÄ±ndaki ve sonundaki alt tireleri kaldÄ±r
    slug = slug.strip('_')
    
    # KÄ±saltma yok, tam uzunluk
    
    return slug or "pdf_document"


def _upload_to_bunny(pdf_path: str, filename: str) -> Optional[str]:
    """PDF'i Bunny.net'e yÃ¼kler ve public URL dÃ¶ner"""
    try:
        api_key = os.getenv("BUNNY_STORAGE_API_KEY")
        storage_zone = os.getenv("BUNNY_STORAGE_ZONE", "mevzuatgpt")
        storage_region = os.getenv("BUNNY_STORAGE_REGION", "storage.bunnycdn.com")
        storage_endpoint = os.getenv("BUNNY_STORAGE_ENDPOINT", "https://cdn.mevzuatgpt.org")
        storage_folder = os.getenv("BUNNY_STORAGE_FOLDER", "portal")
        
        if not api_key:
            print("Bunny.net API anahtarÄ± bulunamadÄ±")
            return None
        
        # PDF dosyasÄ±nÄ± oku
        with open(pdf_path, 'rb') as f:
            pdf_data = f.read()
        
        # URL-safe filename
        safe_filename = urllib.parse.quote(filename)
        upload_url = f"https://{storage_region}/{storage_zone}/{storage_folder}/{safe_filename}"
        
        headers = {
            'AccessKey': api_key,
            'Content-Type': 'application/pdf',
            'User-Agent': 'SGK-Scraper-API/1.0'
        }
        
        response = requests.put(upload_url, headers=headers, data=pdf_data, timeout=60)
        
        if response.status_code == 201:
            public_url = f"{storage_endpoint}/{storage_folder}/{safe_filename}"
            print(f"PDF baÅŸarÄ±yla Bunny.net'e yÃ¼klendi: {public_url}")
            return public_url
        else:
            print(f"Bunny.net yÃ¼kleme hatasÄ±: {response.status_code} - {response.text}")
            return None
            
    except Exception as e:
        print(f"Bunny.net yÃ¼kleme hatasÄ±: {str(e)}")
        return None


def _upload_logo_to_bunny(file_data: bytes, filename: str, content_type: str) -> Optional[str]:
    """Logo/resim dosyasÄ±nÄ± Bunny.net'e yÃ¼kler ve public URL dÃ¶ner (referans koddaki mantÄ±k)"""
    try:
        api_key = os.getenv("BUNNY_STORAGE_API_KEY")
        storage_zone = os.getenv("BUNNY_STORAGE_ZONE", "mevzuatgpt")
        storage_region = os.getenv("BUNNY_STORAGE_REGION", "storage.bunnycdn.com")
        storage_endpoint = os.getenv("BUNNY_STORAGE_ENDPOINT", "https://cdn.mevzuatgpt.org")
        storage_folder = os.getenv("BUNNY_STORAGE_FOLDER", "portal")
        
        if not api_key:
            print("Bunny.net API anahtarÄ± bulunamadÄ±")
            return None
        
        # URL-safe filename
        safe_filename = urllib.parse.quote(filename)
        upload_url = f"https://{storage_region}/{storage_zone}/{storage_folder}/{safe_filename}"
        
        print(f"Logo yÃ¼kleniyor: {upload_url}")
        
        headers = {
            'AccessKey': api_key,
            'Content-Type': content_type,
            'User-Agent': 'SGK-Scraper-API/1.0'
        }
        
        # Upload file
        response = requests.put(upload_url, headers=headers, data=file_data, timeout=30)
        
        if response.status_code == 201:
            # Return public URL
            public_url = f"{storage_endpoint}/{storage_folder}/{safe_filename}"
            print("Logo baÅŸarÄ±yla Bunny.net'e yÃ¼klendi")
            return public_url
        else:
            print(f"Logo yÃ¼kleme hatasÄ±: {response.status_code} - {response.text}")
            return None
            
    except requests.exceptions.Timeout:
        print("Logo yÃ¼kleme zaman aÅŸÄ±mÄ±na uÄŸradÄ±")
        return None
    except requests.exceptions.RequestException as e:
        print(f"Logo yÃ¼kleme aÄŸ hatasÄ±: {str(e)}")
        return None
    except Exception as e:
        print(f"Beklenmeyen logo yÃ¼kleme hatasÄ±: {str(e)}")
        return None


def _delete_from_bunny(pdf_url: str) -> bool:
    """Bunny.net'ten PDF dosyasÄ±nÄ± siler"""
    try:
        if not pdf_url or not pdf_url.strip():
            print("âš ï¸ PDF URL boÅŸ, silme iÅŸlemi atlandÄ±")
            return False
        
        api_key = os.getenv("BUNNY_STORAGE_API_KEY")
        storage_zone = os.getenv("BUNNY_STORAGE_ZONE", "mevzuatgpt")
        storage_region = os.getenv("BUNNY_STORAGE_REGION", "storage.bunnycdn.com")
        storage_endpoint = os.getenv("BUNNY_STORAGE_ENDPOINT", "https://cdn.mevzuatgpt.org")
        storage_folder = os.getenv("BUNNY_STORAGE_FOLDER", "portal")
        
        if not api_key:
            print("âš ï¸ Bunny.net API anahtarÄ± bulunamadÄ±, silme iÅŸlemi atlandÄ±")
            return False
        
        # PDF URL'den dosya adÄ±nÄ± Ã§Ä±kar
        # Format: https://cdn.mevzuatgpt.org/portal/filename.pdf
        # veya: https://cdn.mevzuatgpt.org/portal/filename%20with%20spaces.pdf
        try:
            # URL'den dosya adÄ±nÄ± al
            if storage_endpoint in pdf_url:
                # Endpoint'ten sonraki kÄ±smÄ± al
                file_path = pdf_url.split(storage_endpoint, 1)[1]
                # BaÅŸÄ±ndaki /portal/ kÄ±smÄ±nÄ± kaldÄ±r
                if file_path.startswith(f"/{storage_folder}/"):
                    filename = file_path[len(f"/{storage_folder}/"):]
                else:
                    filename = file_path.lstrip("/")
            else:
                # FarklÄ± format olabilir, direkt dosya adÄ±nÄ± Ã§Ä±kar
                filename = os.path.basename(pdf_url)
            
            if not filename:
                print(f"âš ï¸ PDF URL'den dosya adÄ± Ã§Ä±karÄ±lamadÄ±: {pdf_url}")
                return False
            
            # URL decode yap (eÄŸer encoded ise)
            filename = urllib.parse.unquote(filename)
            
            # URL-safe filename (tekrar encode et)
            safe_filename = urllib.parse.quote(filename)
            
            # Delete URL oluÅŸtur
            delete_url = f"https://{storage_region}/{storage_zone}/{storage_folder}/{safe_filename}"
            
            headers = {
                'AccessKey': api_key,
                'User-Agent': 'SGK-Scraper-API/1.0'
            }
            
            print(f"ğŸ—‘ï¸ Bunny.net'ten siliniyor: {filename}")
            response = requests.delete(delete_url, headers=headers, timeout=30)
            
            if response.status_code == 200 or response.status_code == 204:
                print(f"âœ… PDF Bunny.net'ten baÅŸarÄ±yla silindi: {filename}")
                return True
            elif response.status_code == 404:
                print(f"âš ï¸ PDF Bunny.net'te bulunamadÄ± (zaten silinmiÅŸ olabilir): {filename}")
                return True  # Zaten yoksa baÅŸarÄ±lÄ± say
            else:
                print(f"âš ï¸ Bunny.net silme hatasÄ±: {response.status_code} - {response.text}")
                return False
                
        except Exception as parse_error:
            print(f"âš ï¸ PDF URL parse hatasÄ±: {str(parse_error)}")
            return False
            
    except Exception as e:
        print(f"âš ï¸ Bunny.net silme hatasÄ±: {str(e)}")
        return False


def _get_mongodb_client() -> Optional[MongoClient]:
    """MongoDB baÄŸlantÄ±sÄ± oluÅŸturur"""
    try:
        connection_string = os.getenv("MONGODB_CONNECTION_STRING")
        if not connection_string:
            print("MongoDB baÄŸlantÄ± dizesi bulunamadÄ±")
            return None
        
        client = MongoClient(connection_string, serverSelectionTimeoutMS=5000)
        # Test connection
        client.admin.command('ping')
        return client
    except Exception as e:
        print(f"MongoDB baÄŸlantÄ± hatasÄ±: {str(e)}")
        return None


def _save_to_mongodb(metadata: Dict[str, Any], content: str) -> Optional[str]:
    """Metadata ve content'i MongoDB'ye kaydeder, metadata_id dÃ¶ner"""
    try:
        client = _get_mongodb_client()
        if not client:
            return None
        
        database_name = os.getenv("MONGODB_DATABASE", "mevzuatgpt")
        metadata_collection_name = os.getenv("MONGODB_METADATA_COLLECTION", "metadata")
        content_collection_name = os.getenv("MONGODB_CONTENT_COLLECTION", "content")
        
        db = client[database_name]
        metadata_collection = db[metadata_collection_name]
        content_collection = db[content_collection_name]
        
        # Metadata kaydet
        clean_metadata = {}
        for key, value in metadata.items():
            if value is not None and value != '':
                clean_metadata[key] = value
        
        clean_metadata['olusturulma_tarihi'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        
        metadata_result = metadata_collection.insert_one(clean_metadata)
        metadata_id = str(metadata_result.inserted_id)
        
        # Content kaydet
        content_doc = {
            'metadata_id': ObjectId(metadata_id),
            'icerik': content,
            'olusturulma_tarihi': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        }
        
        content_collection.insert_one(content_doc)
        
        client.close()
        print(f"MongoDB'ye kaydedildi: metadata_id={metadata_id}")
        return metadata_id
        
    except Exception as e:
        print(f"MongoDB kayÄ±t hatasÄ±: {str(e)}")
        return None


def _extract_pdf_text_markdown(pdf_path: str) -> Optional[str]:
    """PDF'den markdown formatÄ±nda metin Ã§Ä±karÄ±r (OCR desteÄŸi ile)"""
    try:
        import pdfplumber
        from io import BytesIO
        
        extracted_text = ""
        total_pages = 0
        
        # Ã–nce PDF yapÄ±sÄ±nÄ± analiz et (daha doÄŸru tespit iÃ§in)
        processor = PDFProcessor()
        pdf_structure = processor.analyze_pdf_structure(pdf_path)
        total_pages = pdf_structure.get('total_pages', 0)
        text_coverage = pdf_structure.get('text_coverage', 0.0)
        has_text = pdf_structure.get('has_text', False)
        needs_ocr = pdf_structure.get('needs_ocr', False)
        
        # Resim formatÄ± kontrolÃ¼: EÄŸer PDF resim formatÄ±ndaysa direkt OCR ile baÅŸla
        # %30 eÅŸiÄŸi: Metin kapsamÄ± dÃ¼ÅŸÃ¼kse kalite zayÄ±f olabilir, OCR daha iyi sonuÃ§ verebilir
        # AyrÄ±ca, eÄŸer metin varsa ama Ã§ok azsa (sadece baÅŸlÄ±klar), OCR gerekli
        # Ortalama sayfa baÅŸÄ±na metin miktarÄ±nÄ± kontrol et
        avg_text_per_page = 0
        if total_pages > 0:
            # HÄ±zlÄ± kontrol: Ä°lk 3 sayfadan ortalama metin miktarÄ±nÄ± hesapla
            import pdfplumber
            from io import BytesIO
            with open(pdf_path, 'rb') as f:
                pdf_bytes = f.read()
            pdf_file_obj = BytesIO(pdf_bytes)
            with pdfplumber.open(pdf_file_obj) as pdf:
                quick_check_pages = min(3, total_pages)
                quick_total_text = 0
                for page_num in range(quick_check_pages):
                    try:
                        page = pdf.pages[page_num]
                        page_text = page.extract_text()
                        if page_text:
                            quick_total_text += len(page_text.strip())
                    except Exception:
                        pass
                avg_text_per_page = quick_total_text / quick_check_pages if quick_check_pages > 0 else 0
        
        # EÄŸer ortalama sayfa baÅŸÄ±na metin 300 karakterden azsa, muhtemelen sadece baÅŸlÄ±klar var
        is_image_pdf = not has_text or text_coverage < 0.3 or needs_ocr or (has_text and avg_text_per_page < 300)
        
        if is_image_pdf:
            print(f"ğŸ“¸ PDF resim formatÄ±nda tespit edildi (kapsam: %{text_coverage*100:.1f}). OCR ile tÃ¼m {total_pages} sayfa iÅŸleniyor (sÄ±nÄ±rlama olmadan)...")
            try:
                if processor._check_ocr_available():
                    # Direkt OCR ile tÃ¼m sayfalarÄ± iÅŸle (sÄ±nÄ±rlama yok)
                    print(f"ğŸ”„ OCR baÅŸlatÄ±lÄ±yor: {total_pages} sayfa iÅŸlenecek...")
                    ocr_text = processor.extract_text_from_pages(pdf_path, 1, total_pages, use_ocr=True)
                    if ocr_text and len(ocr_text.strip()) > 0:
                        extracted_text = _format_text_as_markdown(ocr_text)
                        ocr_char_count = len(ocr_text)
                        ocr_line_count = len([line for line in ocr_text.split('\n') if line.strip()])
                        print(f"âœ… OCR tamamlandÄ±: {total_pages} sayfa iÅŸlendi, {ocr_char_count:,} karakter, {ocr_line_count:,} satÄ±r Ã§Ä±karÄ±ldÄ±")
                        return extracted_text.strip()
                    else:
                        print("âš ï¸ OCR ile metin Ã§Ä±karÄ±lamadÄ±")
                else:
                    print("âš ï¸ OCR kÃ¼tÃ¼phaneleri kurulu deÄŸil veya Poppler/Tesseract eksik")
                    print("âš ï¸ Kurulum iÃ§in: 'apt-get install poppler-utils tesseract-ocr tesseract-ocr-tur' (Linux)")
                    print("âš ï¸ veya: 'brew install poppler tesseract tesseract-lang' (macOS)")
            except Exception as ocr_error:
                error_msg = str(ocr_error)
                print(f"âŒ OCR hatasÄ±: {error_msg}")
                if "poppler" in error_msg.lower() or "pdftoppm" in error_msg.lower():
                    print("âŒ Poppler kurulu deÄŸil! 'apt-get install poppler-utils' komutunu Ã§alÄ±ÅŸtÄ±rÄ±n.")
                elif "tesseract" in error_msg.lower():
                    print("âŒ Tesseract kurulu deÄŸil! 'apt-get install tesseract-ocr tesseract-ocr-tur' komutunu Ã§alÄ±ÅŸtÄ±rÄ±n.")
                import traceback
                traceback.print_exc()
                return None
        
        # Normal metin Ã§Ä±karma: PDF'de yeterli metin var
        with open(pdf_path, 'rb') as f:
            pdf_bytes = f.read()
        
        pdf_file_obj = BytesIO(pdf_bytes)
        
        with pdfplumber.open(pdf_file_obj) as pdf:
            if total_pages == 0:
                total_pages = len(pdf.pages)
            
            # HÄ±zlÄ± kontrol: Ä°lk 3 sayfadan metin Ã§Ä±kar
            quick_check_pages = min(3, total_pages)
            total_text_length = 0
            pages_with_text = 0
            
            for page_num in range(quick_check_pages):
                try:
                    page = pdf.pages[page_num]
                    page_text = page.extract_text()
                    if page_text and len(page_text.strip()) > 10:
                        total_text_length += len(page_text.strip())
                        pages_with_text += 1
                except Exception:
                    pass
            
            # Metin kapsamÄ±nÄ± hesapla (ilk 3 sayfadan)
            quick_coverage = pages_with_text / quick_check_pages if quick_check_pages > 0 else 0.0
            
            # Resim formatÄ± kontrolÃ¼: EÄŸer ilk 3 sayfada hiÃ§ metin yoksa, Ã§ok az metin varsa 
            # veya metin kapsamÄ± %30'dan azsa veya toplam metin Ã§ok azsa direkt OCR ile tÃ¼m sayfalarÄ± iÅŸle
            # %30 eÅŸiÄŸi: Metin kapsamÄ± dÃ¼ÅŸÃ¼kse kalite zayÄ±f olabilir, OCR daha iyi sonuÃ§ verebilir
            # AyrÄ±ca, eÄŸer metin varsa ama Ã§ok azsa (1000 karakterden az), bu da resim formatÄ± olabilir
            should_use_ocr_directly = (
                pages_with_text == 0 or 
                (pages_with_text < 2 and total_text_length < 500) or
                quick_coverage < 0.3 or
                (pages_with_text > 0 and total_text_length < 1000)  # Metin var ama Ã§ok az
            )
            
            if should_use_ocr_directly:
                print(f"ğŸ“¸ PDF resim formatÄ±nda tespit edildi (ilk {quick_check_pages} sayfada kapsam: %{quick_coverage*100:.1f}, metin: {pages_with_text}/{quick_check_pages} sayfa). OCR ile tÃ¼m {total_pages} sayfa iÅŸleniyor (sÄ±nÄ±rlama olmadan)...")
                try:
                    processor = PDFProcessor()
                    if processor._check_ocr_available():
                        # Direkt OCR ile tÃ¼m sayfalarÄ± iÅŸle (sÄ±nÄ±rlama yok)
                        print(f"ğŸ”„ OCR baÅŸlatÄ±lÄ±yor: {total_pages} sayfa iÅŸlenecek...")
                        ocr_text = processor.extract_text_from_pages(pdf_path, 1, total_pages, use_ocr=True)
                        if ocr_text and len(ocr_text.strip()) > 0:
                            extracted_text = _format_text_as_markdown(ocr_text)
                            ocr_char_count = len(ocr_text)
                            ocr_line_count = len([line for line in ocr_text.split('\n') if line.strip()])
                            print(f"âœ… OCR tamamlandÄ±: {total_pages} sayfa iÅŸlendi, {ocr_char_count:,} karakter, {ocr_line_count:,} satÄ±r Ã§Ä±karÄ±ldÄ±")
                            return extracted_text.strip()
                        else:
                            print("âš ï¸ OCR ile metin Ã§Ä±karÄ±lamadÄ±")
                    else:
                        print("âš ï¸ OCR kÃ¼tÃ¼phaneleri kurulu deÄŸil veya Poppler/Tesseract eksik")
                        print("âš ï¸ Kurulum iÃ§in: 'apt-get install poppler-utils tesseract-ocr tesseract-ocr-tur' (Linux)")
                        print("âš ï¸ veya: 'brew install poppler tesseract tesseract-lang' (macOS)")
                except Exception as ocr_error:
                    error_msg = str(ocr_error)
                    print(f"âŒ OCR hatasÄ±: {error_msg}")
                    if "poppler" in error_msg.lower() or "pdftoppm" in error_msg.lower():
                        print("âŒ Poppler kurulu deÄŸil! 'apt-get install poppler-utils' komutunu Ã§alÄ±ÅŸtÄ±rÄ±n.")
                    elif "tesseract" in error_msg.lower():
                        print("âŒ Tesseract kurulu deÄŸil! 'apt-get install tesseract-ocr tesseract-ocr-tur' komutunu Ã§alÄ±ÅŸtÄ±rÄ±n.")
                    import traceback
                    traceback.print_exc()
                    return None
            
            # Normal metin Ã§Ä±karma: TÃ¼m sayfalarÄ± iÅŸle (metin kapsamÄ± yeterliyse)
            total_text_length = 0
            pages_with_text = 0
            
            for page_num, page in enumerate(pdf.pages, 1):
                try:
                    page_text = page.extract_text()
                    if page_text and len(page_text.strip()) > 10:
                        # Basit markdown formatÄ±
                        formatted_text = _format_text_as_markdown(page_text)
                        extracted_text += formatted_text + "\n\n"
                        total_text_length += len(page_text.strip())
                        pages_with_text += 1
                    else:
                        # Metin yoksa OCR ile dene
                        processor = PDFProcessor()
                        if processor._check_ocr_available():
                            try:
                                ocr_text = processor._extract_text_with_ocr(pdf_path, page_num - 1)
                                if ocr_text and len(ocr_text.strip()) > 0:
                                    formatted_text = _format_text_as_markdown(ocr_text)
                                    extracted_text += formatted_text + "\n\n"
                                    total_text_length += len(ocr_text.strip())
                                    pages_with_text += 1
                            except Exception:
                                pass
                except Exception as page_error:
                    # Sayfa hatasÄ± varsa OCR ile dene
                    processor = PDFProcessor()
                    if processor._check_ocr_available():
                        try:
                            ocr_text = processor._extract_text_with_ocr(pdf_path, page_num - 1)
                            if ocr_text and len(ocr_text.strip()) > 0:
                                formatted_text = _format_text_as_markdown(ocr_text)
                                extracted_text += formatted_text + "\n\n"
                        except Exception:
                            pass
                    continue
        
            # Metin kapsamÄ±nÄ± kontrol et: EÄŸer %30'dan az sayfa metin iÃ§eriyorsa veya toplam metin Ã§ok azsa OCR kullan
            # %30 eÅŸiÄŸi: Metin kapsamÄ± dÃ¼ÅŸÃ¼kse kalite zayÄ±f olabilir, OCR daha iyi sonuÃ§ verebilir
            text_coverage = pages_with_text / total_pages if total_pages > 0 else 0.0
            should_use_ocr = text_coverage < 0.3 or total_text_length < 1000
        
        # EÄŸer metin yetersizse OCR ile tÃ¼m sayfalarÄ± iÅŸle
        if should_use_ocr and total_pages > 0:
            print(f"ğŸ“¸ PDF'de metin bulunamadÄ± veya yetersiz (kapsam: %{text_coverage*100:.1f}, toplam: {total_text_length} karakter), OCR ile tÃ¼m {total_pages} sayfa iÅŸleniyor...")
            try:
                processor = PDFProcessor()
                if processor._check_ocr_available():
                    # TÃ¼m sayfalar iÃ§in OCR yap (use_ocr=True ile zorunlu OCR)
                    # end_page dahil olacak ÅŸekilde total_pages kullan
                    print(f"ğŸ”„ OCR baÅŸlatÄ±lÄ±yor: {total_pages} sayfa iÅŸlenecek...")
                    ocr_text = processor.extract_text_from_pages(pdf_path, 1, total_pages, use_ocr=True)
                    if ocr_text and len(ocr_text.strip()) > 100:
                        extracted_text = _format_text_as_markdown(ocr_text)
                        ocr_char_count = len(ocr_text)
                        ocr_line_count = len([line for line in ocr_text.split('\n') if line.strip()])
                        print(f"âœ… OCR tamamlandÄ±: {total_pages} sayfa iÅŸlendi, {ocr_char_count:,} karakter, {ocr_line_count:,} satÄ±r Ã§Ä±karÄ±ldÄ±")
                    else:
                        print("âš ï¸ OCR ile metin Ã§Ä±karÄ±lamadÄ± veya Ã§ok az metin Ã§Ä±karÄ±ldÄ±")
                        if ocr_text:
                            print(f"âš ï¸ Ã‡Ä±karÄ±lan metin uzunluÄŸu: {len(ocr_text)} karakter (Ã§ok kÄ±sa)")
                else:
                    print("âš ï¸ OCR kÃ¼tÃ¼phaneleri kurulu deÄŸil veya Poppler/Tesseract eksik")
                    print("âš ï¸ Kurulum iÃ§in: 'apt-get install poppler-utils tesseract-ocr tesseract-ocr-tur' (Linux)")
                    print("âš ï¸ veya: 'brew install poppler tesseract tesseract-lang' (macOS)")
            except Exception as ocr_error:
                error_msg = str(ocr_error)
                print(f"âŒ OCR hatasÄ±: {error_msg}")
                # Poppler veya Tesseract eksikse Ã¶zel mesaj
                if "poppler" in error_msg.lower() or "pdftoppm" in error_msg.lower():
                    print("âŒ Poppler kurulu deÄŸil! 'apt-get install poppler-utils' komutunu Ã§alÄ±ÅŸtÄ±rÄ±n.")
                elif "tesseract" in error_msg.lower():
                    print("âŒ Tesseract kurulu deÄŸil! 'apt-get install tesseract-ocr tesseract-ocr-tur' komutunu Ã§alÄ±ÅŸtÄ±rÄ±n.")
                import traceback
                traceback.print_exc()
        
        return extracted_text.strip() if extracted_text.strip() else None
        
    except Exception as e:
        print(f"PDF metin Ã§Ä±karma hatasÄ±: {str(e)}")
        import traceback
        traceback.print_exc()
        return None


def _format_text_as_markdown(text: str) -> str:
    """Metni markdown formatÄ±na Ã§evirir"""
    try:
        if not text:
            return ""
        
        lines = text.split('\n')
        formatted_lines = []
        
        for line in lines:
            line = line.strip()
            if not line:
                continue
            
            # Sayfa numaralarÄ±nÄ± atla
            if re.match(r'^\d+$', line) or re.match(r'^sayfa\s+\d+', line.lower()):
                continue
            
            # Ana baÅŸlÄ±klar (bÃ¼yÃ¼k harf, 10+ karakter)
            if line.isupper() and len(line) > 10 and not re.match(r'^\d+', line):
                formatted_lines.append(f"\n## {line.title()}\n")
            
            # Madde baÅŸlÄ±klarÄ±
            elif re.match(r'^MADDE\s+\d+', line, re.IGNORECASE):
                formatted_lines.append(f"\n### {line.title()}\n")
            
            # BÃ¶lÃ¼m baÅŸlÄ±klarÄ±
            elif re.match(r'^BÃ–LÃœM\s+[IVX\d]+', line, re.IGNORECASE):
                formatted_lines.append(f"\n## {line.title()}\n")
            
            # Alt baÅŸlÄ±klar (numaralÄ±)
            elif re.match(r'^\d+\.\s+[A-ZÃœÃ‡ÄIÄ°Ã–Å]', line):
                formatted_lines.append(f"\n**{line}**\n")
            
            # Normal paragraflar
            else:
                if len(line) > 50:
                    formatted_lines.append(f"{line}\n")
                else:
                    formatted_lines.append(f"**{line}**\n")
        
        return '\n'.join(formatted_lines)
        
    except Exception:
        return text


def _analyze_and_prepare_headless(pdf_path: str, pdf_base_name: str, api_key: Optional[str]) -> Dict[str, Any]:
    """Streamlit'e baÄŸlÄ± olmadan analiz ve metadata Ã¼retimini yapar."""
    processor = PDFProcessor()
    pdf_structure = processor.analyze_pdf_structure(pdf_path)
    total_pages = pdf_structure['total_pages']
    
    # Resim formatÄ± kontrolÃ¼: EÄŸer PDF resim formatÄ±ndaysa direkt OCR ile baÅŸla
    text_coverage = pdf_structure.get('text_coverage', 0.0)
    has_text = pdf_structure.get('has_text', False)
    needs_ocr = pdf_structure.get('needs_ocr', False)
    
    # Ortalama sayfa baÅŸÄ±na metin miktarÄ±nÄ± kontrol et (sadece baÅŸlÄ±klar mÄ± yoksa gerÃ§ek iÃ§erik mi?)
    avg_text_per_page = 0
    if total_pages > 0:
        # HÄ±zlÄ± kontrol: Ä°lk 3 sayfadan ortalama metin miktarÄ±nÄ± hesapla
        import pdfplumber
        from io import BytesIO
        with open(pdf_path, 'rb') as f:
            pdf_bytes = f.read()
        pdf_file_obj = BytesIO(pdf_bytes)
        with pdfplumber.open(pdf_file_obj) as pdf:
            quick_check_pages = min(3, total_pages)
            quick_total_text = 0
            for page_num in range(quick_check_pages):
                try:
                    page = pdf.pages[page_num]
                    page_text = page.extract_text()
                    if page_text:
                        quick_total_text += len(page_text.strip())
                except Exception:
                    pass
            avg_text_per_page = quick_total_text / quick_check_pages if quick_check_pages > 0 else 0
    
    # Resim formatÄ±: Metin yoksa veya Ã§ok az metin varsa (%30'dan az) veya OCR gerekliyse
    # %30 eÅŸiÄŸi: Metin kapsamÄ± dÃ¼ÅŸÃ¼kse kalite zayÄ±f olabilir, OCR daha iyi sonuÃ§ verebilir
    # AyrÄ±ca, eÄŸer metin varsa ama Ã§ok azsa (sadece baÅŸlÄ±klar), OCR gerekli
    is_image_pdf = not has_text or text_coverage < 0.3 or needs_ocr or (has_text and avg_text_per_page < 300)
    
    use_ocr = is_image_pdf  # Resim formatÄ±ndaysa OCR kullan
    
    if is_image_pdf:
        print(f"ğŸ“¸ PDF resim formatÄ±nda tespit edildi (kapsam: %{text_coverage*100:.1f}, ortalama: {avg_text_per_page:.0f} karakter/sayfa). OCR ile tÃ¼m {total_pages} sayfa iÅŸlenecek (sÄ±nÄ±rlama olmadan)...")
    
    use_ai = bool(api_key)
    if use_ai:
        analyzer = DeepSeekAnalyzer(api_key)
        try:
            # OCR modunda da intelligent sections kullanabiliriz (use_ocr parametresi ile)
            sections = processor.create_intelligent_sections(pdf_path, total_pages, analyzer, use_ocr=use_ocr)
        except Exception:
            sections = processor.create_optimal_sections(pdf_path, total_pages, 3, 10)
    else:
        sections = processor.create_optimal_sections(pdf_path, total_pages, 3, 10)

    metadata_list: List[Dict[str, Any]] = []
    if use_ai:
        analyzer = DeepSeekAnalyzer(api_key)
    else:
        analyzer = None  # type: ignore
    
    if use_ocr:
        print(f"ğŸ“¸ OCR modu aktif: TÃ¼m {total_pages} sayfa OCR ile iÅŸlenecek (sÄ±nÄ±rlama olmadan)")
    
    for i, section in enumerate(sections):
        section_text = processor.extract_text_from_pages(pdf_path, section['start_page'], section['end_page'], use_ocr=use_ocr)
        if use_ai and section_text.strip():
            analysis = analyzer.analyze_section_content(section_text)  # type: ignore
            title = analysis.get('title', f'BÃ¶lÃ¼m {i + 1}')
            description = analysis.get('description', 'Bu bÃ¶lÃ¼m iÃ§in aÃ§Ä±klama oluÅŸturulamadÄ±.')
            keywords = analysis.get('keywords', f'bÃ¶lÃ¼m {i + 1}')
        else:
            title = f"BÃ¶lÃ¼m {i + 1}"
            description = "Bu bÃ¶lÃ¼m iÃ§in otomatik aÃ§Ä±klama oluÅŸturulamadÄ±."
            keywords = f"bÃ¶lÃ¼m {i + 1}"

        output_filename = create_pdf_filename(pdf_base_name, i + 1, section['start_page'], section['end_page'], title)
        metadata_list.append({
            "output_filename": output_filename,
            "start_page": section['start_page'],
            "end_page": section['end_page'],
            "title": title,
            "description": description,
            "keywords": keywords
        })

    return {"sections": sections, "metadata_list": metadata_list, "total_pages": total_pages}


def _split_pdfs(pdf_path: str, sections: List[Dict[str, int]], metadata_list: List[Dict[str, Any]]) -> str:
    output_dir = create_output_directories()
    from pypdf import PdfReader, PdfWriter
    with open(pdf_path, 'rb') as source:
        reader = PdfReader(source)
        for section, metadata in zip(sections, metadata_list):
            writer = PdfWriter()
            for page_num in range(section['start_page'] - 1, section['end_page']):
                if page_num < len(reader.pages):
                    writer.add_page(reader.pages[page_num])
            out_path = Path(output_dir) / metadata['output_filename']
            with open(out_path, 'wb') as f:
                writer.write(f)
    # JSON metadata dosyasÄ± da kaydedilsin
    json_path = Path(output_dir) / "pdf_sections_metadata.json"
    with open(json_path, 'w', encoding='utf-8') as jf:
        json.dump({"pdf_sections": metadata_list}, jf, ensure_ascii=False, indent=2)
    return output_dir


def _upload_bulk(cfg: Dict[str, Any], token: str, output_dir: str, category: str, institution: str, belge_adi: str, metadata_list: List[Dict[str, Any]]) -> Optional[Dict[str, Any]]:
    try:
        api_base_url = cfg.get("api_base_url")
        if not api_base_url:
            return None
        upload_url = f"{api_base_url.rstrip('/')}/api/admin/documents/bulk-upload"
        files_to_upload = []
        file_handles = []
        for pdf_file in sorted(Path(output_dir).glob('*.pdf')):
            f = open(pdf_file, 'rb')
            file_handles.append(f)
            files_to_upload.append(('files', (pdf_file.name, f, 'application/pdf')))

        form_data = {
            'category': category,
            'institution': institution,
            'belge_adi': belge_adi,
            'metadata': json.dumps({"pdf_sections": [
                {
                    "output_filename": m.get("output_filename", ""),
                    "title": m.get("title", ""),
                    "description": m.get("description", ""),
                    "keywords": m.get("keywords", "")
                } for m in metadata_list
            ]}, ensure_ascii=False)
        }
        # API isteklerinde proxy kullanÄ±lmÄ±yor
        
        headers = {'Authorization': f'Bearer {token}'}
        resp = requests.post(upload_url, headers=headers, data=form_data, files=files_to_upload, timeout=300)
        for f in file_handles:
            try:
                f.close()
            except Exception:
                pass
        if resp.status_code == 200:
            return resp.json()
        return {"status_code": resp.status_code, "text": resp.text}
    except Exception as e:
        return {"error": str(e)}


@app.post("/api/kurum/process", response_model=ProcessResponse, tags=["SGK Scraper"], summary="Link ile PDF indir, analiz et ve yÃ¼kle")
async def process_item(req: ProcessRequest):
    try:
        # Type kontrolÃ¼
        if req.type.lower() != "kaysis":
            raise HTTPException(
                status_code=400,
                detail=f"Desteklenmeyen scraper tipi: {req.type}. Åu an iÃ§in sadece 'kaysis' desteklenmektedir."
            )
        
        # Mode kontrolÃ¼
        mode = req.mode.lower() if req.mode else "t"
        if mode not in ["m", "p", "t"]:
            raise HTTPException(status_code=400, detail="GeÃ§ersiz mode. 'm', 'p' veya 't' olmalÄ±.")
        
        print(f"ğŸ”§ Ä°ÅŸlem modu: {mode.upper()} ({'MevzuatGPT' if mode == 'm' else 'Portal' if mode == 'p' else 'TamamÄ±'})")
        print(f"ğŸ“‹ Scraper tipi: {req.type}")
        
        # MongoDB'den kurum bilgisini Ã§ek
        kurum_adi = None
        try:
            client = _get_mongodb_client()
            if client:
                database_name = os.getenv("MONGODB_DATABASE", "mevzuatgpt")
                db = client[database_name]
                kurumlar_collection = db["kurumlar"]
                from bson import ObjectId
                kurum_doc = kurumlar_collection.find_one({"_id": ObjectId(req.kurum_id)})
                if kurum_doc:
                    kurum_adi = kurum_doc.get("kurum_adi", "Bilinmeyen Kurum")
                client.close()
        except Exception as e:
            print(f"âš ï¸ MongoDB'den kurum bilgisi alÄ±namadÄ±: {str(e)}")
            kurum_adi = "Bilinmeyen Kurum"
        
        print(f"ğŸ“‹ Kurum: {kurum_adi}")
        print(f"ğŸ”¢ DETSIS: {req.detsis}")
        
        # Link ve diÄŸer bilgileri request'ten al
        pdf_url = req.link
        if not pdf_url:
            raise HTTPException(status_code=400, detail="Link parametresi zorunludur.")
        
        # Category ve document_name request'ten al veya varsayÄ±lan deÄŸerler kullan
        category = req.category if req.category else "Genel"
        document_name = req.document_name if req.document_name else "Belge"
        institution = kurum_adi  # Kurum adÄ±nÄ± kullan
        
        print(f"ğŸ”— PDF Link: {pdf_url}")
        print(f"ğŸ“„ Belge AdÄ±: {document_name}")
        print(f"ğŸ“‚ Kategori: {category}")

        # PDF'i indir
        print("ğŸ“¥ PDF indiriliyor...")
        pdf_path = await download_pdf_from_url(pdf_url)
        if not validate_pdf_file(pdf_path):
            raise HTTPException(status_code=500, detail="Ä°ndirilen dosya geÃ§erli bir PDF deÄŸil.")
        print("âœ… PDF indirme baÅŸarÄ±lÄ±")

        # Analiz ve metadata (tÃ¼m modlar iÃ§in)
        print("ğŸ” PDF analiz ediliyor...")
        api_key = _get_deepseek_api_key()
        if not api_key:
            # DeepSeek anahtarÄ± yoksa analiz kalitesiz olacaÄŸÄ±ndan bildirim yap
            print("[warn] DeepSeek API anahtarÄ± bulunamadÄ±, manuel bÃ¶lÃ¼mleme ve basit metadata kullanÄ±lacak.")
        pdf_base_name = "document"
        analysis_result = _analyze_and_prepare_headless(pdf_path, pdf_base_name, api_key)
        sections = analysis_result['sections']
        metadata_list = analysis_result['metadata_list']
        print("âœ… PDF analiz baÅŸarÄ±lÄ±")

        # PDF'leri bÃ¶l ve Ã§Ä±ktÄ±yÄ± oluÅŸtur (sadece 'm' ve 't' modlarÄ± iÃ§in)
        output_dir = None
        if mode in ["m", "t"]:
            print("ğŸ“„ PDF bÃ¶lÃ¼mleme yapÄ±lÄ±yor...")
            output_dir = _split_pdfs(pdf_path, sections, metadata_list)
            print("âœ… PDF bÃ¶lÃ¼mleme baÅŸarÄ±lÄ±")
        else:
            print("â­ï¸ PDF bÃ¶lÃ¼mleme atlandÄ± (Portal modu)")

        # MevzuatGPT'ye yÃ¼kleme (sadece 'm' ve 't' modlarÄ± iÃ§in)
        upload_resp = None
        if mode in ["m", "t"]:
            print("ğŸ“¤ PDF'ler MevzuatGPT'ye yÃ¼kleniyor...")
            cfg = _load_config()
            if cfg:
                token = _login_with_config(cfg)
                if token:
                    upload_resp = _upload_bulk(cfg, token, output_dir, category, institution, document_name, metadata_list)
                    if upload_resp:
                        print("âœ… PDF'ler MevzuatGPT'ye yÃ¼klendi")
                    else:
                        print("âš ï¸ PDF yÃ¼kleme baÅŸarÄ±sÄ±z")
            else:
                print("âš ï¸ Config bulunamadÄ±, PDF yÃ¼kleme atlandÄ±")
        else:
            print("â­ï¸ MevzuatGPT yÃ¼kleme atlandÄ± (Portal modu)")

        # Portal'a yÃ¼kleme (sadece 'p' ve 't' modlarÄ± iÃ§in)
        mongodb_metadata_id = None
        if mode in ["p", "t"]:
            try:
                print("ğŸ“¦ MongoDB ve Bunny.net iÅŸlemleri baÅŸlatÄ±lÄ±yor...")
                print("ğŸ“¤ Ana PDF Bunny.net'e yÃ¼kleniyor...")
                
                # PDF bilgilerini al
                processor = PDFProcessor()
                pdf_info = processor.analyze_pdf_structure(pdf_path)
                total_pages = pdf_info.get('total_pages', 0)
                
                # PDF dosya boyutu (MB)
                pdf_size_bytes = os.path.getsize(pdf_path)
                pdf_size_mb = round(pdf_size_bytes / (1024 * 1024), 2)
                
                # Keywords ve description'larÄ± topla
                all_keywords = []
                all_descriptions = []
                
                # Mode'a gÃ¶re metadata kaynaÄŸÄ±nÄ± belirle
                if mode == "t" and output_dir:
                    # 't' modunda pdf_sections_metadata.json'dan al
                    metadata_json_path = Path(output_dir) / "pdf_sections_metadata.json"
                    if metadata_json_path.exists():
                        try:
                            with open(metadata_json_path, 'r', encoding='utf-8') as f:
                                metadata_json = json.load(f)
                                pdf_sections = metadata_json.get('pdf_sections', [])
                                for section in pdf_sections:
                                    keywords = section.get('keywords', '')
                                    description = section.get('description', '')
                                    if keywords:
                                        # Keywords string ise virgÃ¼lle ayrÄ±lmÄ±ÅŸ olabilir
                                        if isinstance(keywords, str):
                                            keywords_list = [k.strip() for k in keywords.split(',') if k.strip()]
                                            all_keywords.extend(keywords_list)
                                        elif isinstance(keywords, list):
                                            all_keywords.extend(keywords)
                                    if description:
                                        all_descriptions.append(description.strip())
                        except Exception as e:
                            print(f"Metadata JSON okuma hatasÄ±: {str(e)}")
                else:
                    # 'p' modunda veya json yoksa analiz sonuÃ§larÄ±ndan al
                    for section_meta in metadata_list:
                        keywords = section_meta.get('keywords', '')
                        description = section_meta.get('description', '')
                        if keywords:
                            if isinstance(keywords, str):
                                keywords_list = [k.strip() for k in keywords.split(',') if k.strip()]
                                all_keywords.extend(keywords_list)
                            elif isinstance(keywords, list):
                                all_keywords.extend(keywords)
                        if description:
                            all_descriptions.append(description.strip())
                
                # Keywords ve descriptions birleÅŸtir
                combined_keywords = ', '.join(all_keywords) if all_keywords else ''
                combined_description = ' '.join(all_descriptions) if all_descriptions else ''
                
                # AÃ§Ä±klama karakter sÄ±nÄ±rÄ± (max 500 karakter)
                if len(combined_description) > 500:
                    combined_description = combined_description[:497] + "..."
                
                # Ana PDF'yi bunny.net'e yÃ¼kle
                # Dosya adÄ±nÄ± gÃ¼venli hale getir (TÃ¼rkÃ§e karakterleri Ä°ngilizce'ye Ã§evir, kaldÄ±rma)
                transliterated_name = _transliterate_turkish(document_name)
                # Sadece harfler, rakamlar, boÅŸluk ve tireleri koru, diÄŸer karakterleri kaldÄ±r
                safe_pdf_adi = re.sub(r'[^a-zA-Z0-9\s-]', '', transliterated_name).strip()
                # BoÅŸluklarÄ± alt Ã§izgi ile deÄŸiÅŸtir
                safe_pdf_adi = re.sub(r'\s+', '_', safe_pdf_adi)
                # Ã‡oklu alt Ã§izgileri tek alt Ã§izgi yap
                safe_pdf_adi = re.sub(r'_+', '_', safe_pdf_adi)
                bunny_filename = f"{safe_pdf_adi}_{ObjectId()}.pdf"
                pdf_url = _upload_to_bunny(pdf_path, bunny_filename)
                
                # pdf_adi: tekrar baÅŸlÄ±k metni olarak kaydedilecek
                pdf_adi = document_name
                
                # Slug oluÅŸtur (alt tire ile, sÄ±nÄ±rsÄ±z)
                url_slug = _create_url_slug(document_name)
                
                # YÃ¼kleme tarihi
                now = datetime.now()
                upload_date_str = now.strftime('%Y-%m-%d')
                upload_datetime_str = now.isoformat()
                
                if pdf_url:
                    print("âœ… Ana PDF Bunny.net'e yÃ¼klendi")
                else:
                    print("âš ï¸ Bunny.net yÃ¼kleme baÅŸarÄ±sÄ±z, MongoDB iÅŸlemi devam ediyor...")
                
                # PDF'den markdown formatÄ±nda metin Ã§Ä±kar
                print("ğŸ“ PDF iÃ§eriÄŸi markdown formatÄ±na Ã§evriliyor...")
                markdown_content = _extract_pdf_text_markdown(pdf_path)
                if not markdown_content:
                    markdown_content = "PDF iÃ§eriÄŸi Ã§Ä±karÄ±lamadÄ±."
                
                # Metadata oluÅŸtur
                print("ğŸ’¾ MongoDB'ye kaydediliyor...")
                mongodb_metadata = {
                    "pdf_adi": pdf_adi,
                    "kurum_id": req.kurum_id,  # Request'ten gelen kurum ID'sini kullan
                    "belge_turu": category,
                    "belge_durumu": "YÃ¼rÃ¼rlÃ¼kte",
                    "belge_yayin_tarihi": upload_date_str,
                    "yururluluk_tarihi": upload_date_str,
                    "etiketler": "KAYSÄ°S",
                    "anahtar_kelimeler": combined_keywords,
                    "aciklama": combined_description,
                    "url_slug": url_slug,
                    "status": "aktif",
                    "sayfa_sayisi": total_pages,
                    "dosya_boyutu_mb": pdf_size_mb,
                    "yukleme_tarihi": upload_datetime_str,
                    "pdf_url": pdf_url or ""
                }
                
                # MongoDB'ye kaydet
                mongodb_metadata_id = _save_to_mongodb(mongodb_metadata, markdown_content)
                
                if mongodb_metadata_id:
                    print(f"âœ… MongoDB kaydÄ± baÅŸarÄ±lÄ±: metadata_id={mongodb_metadata_id}")
                else:
                    print("âš ï¸ MongoDB kaydÄ± baÅŸarÄ±sÄ±z")
                    
            except Exception as e:
                print(f"âš ï¸ MongoDB/Bunny.net iÅŸlemleri sÄ±rasÄ±nda hata: {str(e)}")
                # Hata olsa bile ana iÅŸlemi tamamla
        
        # TÃ¼m iÅŸlemler baÅŸarÄ±lÄ± olduktan sonra pdf_output klasÃ¶rÃ¼nÃ¼ temizle
        try:
            print("ğŸ§¹ pdf_output klasÃ¶rÃ¼ temizleniyor...")
            pdf_output_dir = Path("pdf_output")
            if pdf_output_dir.exists():
                # KlasÃ¶rdeki tÃ¼m iÃ§eriÄŸi temizle (klasÃ¶rleri de dahil)
                for item in pdf_output_dir.iterdir():
                    if item.is_dir():
                        shutil.rmtree(item)
                    else:
                        item.unlink()
                print("âœ… pdf_output klasÃ¶rÃ¼ temizlendi")
        except Exception as e:
            print(f"âš ï¸ pdf_output temizleme hatasÄ±: {str(e)}")

        # Response mesajÄ±nÄ± mode'a gÃ¶re Ã¶zelleÅŸtir
        mode_messages = {
            "m": "MevzuatGPT'ye yÃ¼kleme tamamlandÄ±",
            "p": "Portal'a yÃ¼kleme tamamlandÄ±",
            "t": "TÃ¼m iÅŸlemler tamamlandÄ± (MevzuatGPT + Portal)"
        }
        message = mode_messages.get(mode, "Ä°ÅŸlem tamamlandÄ±")
        
        return ProcessResponse(
            success=True,
            message=message,
            data=ProcessData(
                category=category,
                institution=institution,
                document_name=document_name,
                output_dir=output_dir,
                sections_count=len(sections) if sections else 0,
                upload_response=upload_resp
            )
        )
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Ä°ÅŸlem sÄ±rasÄ±nda hata oluÅŸtu: {str(e)}")


if __name__ == "__main__":
    print("ğŸš€ FastAPI Server baÅŸlatÄ±lÄ±yor...")
    print("ğŸ“¡ Server: http://0.0.0.0:8000")
    print("ğŸ“š API Docs: http://0.0.0.0:8000/docs")
    uvicorn.run(app, host="0.0.0.0", port=8000)

