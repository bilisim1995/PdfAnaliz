{"file_contents":{"pdf_processor.py":{"content":"import pypdf\nfrom pathlib import Path\nimport tempfile\nimport math\nfrom typing import List, Dict, Any\n\nclass PDFProcessor:\n    \"\"\"PDF iÅŸleme ve bÃ¶lÃ¼mlendirme sÄ±nÄ±fÄ±\"\"\"\n    \n    def __init__(self):\n        pass\n    \n    def analyze_pdf_structure(self, pdf_path: str) -> Dict[str, Any]:\n        \"\"\"PDF dosyasÄ±nÄ±n yapÄ±sÄ±nÄ± analiz eder\"\"\"\n        try:\n            with open(pdf_path, 'rb') as file:\n                reader = pypdf.PdfReader(file)\n                total_pages = len(reader.pages)\n                \n                # Ä°lk birkaÃ§ sayfaydan metin Ã¶rneÄŸi al\n                sample_text = \"\"\n                sample_pages = min(3, total_pages)\n                \n                for i in range(sample_pages):\n                    try:\n                        page_text = reader.pages[i].extract_text()\n                        sample_text += page_text + \"\\n\"\n                    except Exception as e:\n                        continue\n                \n                return {\n                    'total_pages': total_pages,\n                    'sample_text': sample_text[:1000],  # Ä°lk 1000 karakter\n                    'has_text': len(sample_text.strip()) > 0\n                }\n        except Exception as e:\n            raise Exception(f\"PDF analiz hatasÄ±: {str(e)}\")\n    \n    def create_optimal_sections(self, pdf_path: str, total_pages: int, \n                             min_pages: int, max_pages: int) -> List[Dict[str, int]]:\n        \"\"\"RAG iÃ§in optimal bÃ¶lÃ¼mler oluÅŸturur\"\"\"\n        sections = []\n        \n        if total_pages <= max_pages:\n            # Tek bÃ¶lÃ¼m yeterli\n            sections.append({\n                'start_page': 1,\n                'end_page': total_pages\n            })\n        else:\n            # Ã‡oklu bÃ¶lÃ¼m gerekli\n            # Optimal bÃ¶lÃ¼m sayÄ±sÄ±nÄ± hesapla\n            ideal_section_size = (min_pages + max_pages) // 2\n            estimated_sections = math.ceil(total_pages / ideal_section_size)\n            \n            # BÃ¶lÃ¼mleri oluÅŸtur\n            pages_per_section = total_pages // estimated_sections\n            remainder = total_pages % estimated_sections\n            \n            current_page = 1\n            \n            for i in range(estimated_sections):\n                # BazÄ± bÃ¶lÃ¼mlere fazladan sayfa ekle\n                section_size = pages_per_section\n                if i < remainder:\n                    section_size += 1\n                \n                # Minimum ve maksimum sÄ±nÄ±rlarÄ± kontrol et\n                section_size = max(min_pages, min(max_pages, section_size))\n                \n                end_page = min(current_page + section_size - 1, total_pages)\n                \n                sections.append({\n                    'start_page': current_page,\n                    'end_page': end_page\n                })\n                \n                current_page = end_page + 1\n                \n                # Son sayfaya ulaÅŸÄ±ldÄ±ysa dur\n                if end_page >= total_pages:\n                    break\n        \n        return sections\n    \n    def create_section_pdf(self, source_pdf_path: str, start_page: int, \n                          end_page: int, output_dir: str, section_num: int) -> str:\n        \"\"\"Belirtilen sayfa aralÄ±ÄŸÄ±nda yeni PDF oluÅŸturur\"\"\"\n        try:\n            with open(source_pdf_path, 'rb') as source_file:\n                reader = pypdf.PdfReader(source_file)\n                writer = pypdf.PdfWriter()\n                \n                # SayfalarÄ± ekle (1-indexed'dan 0-indexed'a Ã§evir)\n                for page_num in range(start_page - 1, end_page):\n                    if page_num < len(reader.pages):\n                        writer.add_page(reader.pages[page_num])\n                \n                # Ã‡Ä±ktÄ± dosya adÄ±nÄ± oluÅŸtur\n                output_filename = f\"{section_num:02d}_Bolum_{start_page}-{end_page}.pdf\"\n                output_path = Path(output_dir) / output_filename\n                \n                # PDF'i kaydet\n                with open(output_path, 'wb') as output_file:\n                    writer.write(output_file)\n                \n                return str(output_path)\n                \n        except Exception as e:\n            raise Exception(f\"BÃ¶lÃ¼m PDF oluÅŸturma hatasÄ±: {str(e)}\")\n    \n    def extract_text_from_pages(self, pdf_path: str, start_page: int, end_page: int) -> str:\n        \"\"\"Belirtilen sayfa aralÄ±ÄŸÄ±ndan metin Ã§Ä±karÄ±r\"\"\"\n        try:\n            text = \"\"\n            with open(pdf_path, 'rb') as file:\n                reader = pypdf.PdfReader(file)\n                \n                for page_num in range(start_page - 1, end_page):\n                    if page_num < len(reader.pages):\n                        try:\n                            page_text = reader.pages[page_num].extract_text()\n                            text += page_text + \"\\n\"\n                        except Exception as e:\n                            # Sayfa metin Ã§Ä±karma hatasÄ± - devam et\n                            text += f\"[Sayfa {page_num + 1}: Metin Ã§Ä±karÄ±lamadÄ±]\\n\"\n                            continue\n            \n            return text\n        except Exception as e:\n            raise Exception(f\"Metin Ã§Ä±karma hatasÄ±: {str(e)}\")\n    \n    def extract_all_page_texts(self, pdf_path: str) -> List[str]:\n        \"\"\"TÃ¼m sayfalarÄ±n metinlerini Ã§Ä±karÄ±r\"\"\"\n        try:\n            page_texts = []\n            with open(pdf_path, 'rb') as file:\n                reader = pypdf.PdfReader(file)\n                \n                for page_num in range(len(reader.pages)):\n                    try:\n                        page_text = reader.pages[page_num].extract_text()\n                        page_texts.append(page_text if page_text else \"\")\n                    except Exception as e:\n                        # Sayfa metin Ã§Ä±karma hatasÄ± - boÅŸ string ekle\n                        page_texts.append(\"\")\n                        continue\n            \n            return page_texts\n        except Exception as e:\n            raise Exception(f\"Sayfa metinleri Ã§Ä±karma hatasÄ±: {str(e)}\")\n    \n    def create_intelligent_sections(self, pdf_path: str, total_pages: int, analyzer) -> List[Dict[str, int]]:\n        \"\"\"AI kullanarak iÃ§erik bazlÄ± optimal bÃ¶lÃ¼mler oluÅŸturur\"\"\"\n        try:\n            # TÃ¼m sayfalarÄ±n metinlerini Ã§Ä±kar\n            page_texts = self.extract_all_page_texts(pdf_path)\n            \n            # AI'dan bÃ¶lÃ¼m Ã¶nerileri al\n            suggested_sections = analyzer.suggest_content_based_sections(page_texts, total_pages)\n            \n            # BÃ¶lÃ¼mleri formatla\n            sections = []\n            for section in suggested_sections:\n                sections.append({\n                    'start_page': section['start_page'],\n                    'end_page': section['end_page'],\n                    'reason': section.get('reason', '')\n                })\n            \n            return sections\n            \n        except Exception as e:\n            print(f\"AkÄ±llÄ± bÃ¶lÃ¼mleme hatasÄ±: {str(e)}\")\n            # Fallback: Basit eÅŸit bÃ¶lÃ¼mleme\n            return self.create_optimal_sections(pdf_path, total_pages, 3, 10)\n    \n    def get_pdf_metadata(self, pdf_path: str) -> Dict[str, Any]:\n        \"\"\"PDF metadata bilgilerini alÄ±r\"\"\"\n        try:\n            with open(pdf_path, 'rb') as file:\n                reader = pypdf.PdfReader(file)\n                metadata = reader.metadata if reader.metadata else {}\n                \n                return {\n                    'title': metadata.get('/Title', ''),\n                    'author': metadata.get('/Author', ''),\n                    'subject': metadata.get('/Subject', ''),\n                    'creator': metadata.get('/Creator', ''),\n                    'producer': metadata.get('/Producer', ''),\n                    'creation_date': str(metadata.get('/CreationDate', '')),\n                    'modification_date': str(metadata.get('/ModDate', ''))\n                }\n        except Exception as e:\n            return {'error': f\"Metadata okuma hatasÄ±: {str(e)}\"}\n","size_bytes":8071},"create_test_pdf.py":{"content":"from pypdf import PdfWriter, PdfReader\nfrom reportlab.pdfgen import canvas\nfrom reportlab.lib.pagesizes import letter\nimport io\n\n# Create a simple multi-page PDF for testing\ndef create_test_pdf(filename, num_pages=5):\n    # Create a PDF with reportlab\n    packet = io.BytesIO()\n    can = canvas.Canvas(packet, pagesize=letter)\n    \n    for page_num in range(1, num_pages + 1):\n        can.drawString(100, 750, f\"Test PDF - Page {page_num}\")\n        can.drawString(100, 700, f\"This is a test document for RAG segmentation.\")\n        can.drawString(100, 650, f\"Content: Lorem ipsum dolor sit amet, consectetur adipiscing elit.\")\n        can.drawString(100, 600, f\"This page contains some sample text to test the application.\")\n        can.drawString(100, 550, f\"Page number: {page_num} of {num_pages}\")\n        can.showPage()\n    \n    can.save()\n    \n    # Save to file\n    packet.seek(0)\n    with open(filename, 'wb') as f:\n        f.write(packet.read())\n    \n    print(f\"Created test PDF: {filename}\")\n\nif __name__ == \"__main__\":\n    create_test_pdf(\"test_sample.pdf\", 5)\n","size_bytes":1072},"replit.md":{"content":"# PDF RAG BÃ¶lÃ¼mlendirme AracÄ±\n\n## Overview\n\nThis is a Streamlit-based web application that processes PDF documents and segments them into optimized chunks for RAG (Retrieval Augmented Generation) systems. The application offers two sectioning strategies: AI-powered intelligent sectioning that analyzes content to create semantically meaningful sections, and manual sectioning based on fixed page ranges. Using DeepSeek AI, it generates comprehensive metadata including titles, descriptions, and keywords for each section. Users can upload PDFs from their computer or download them from URLs, and the system automatically creates document sections with AI-generated metadata to improve retrieval performance in RAG applications.\n\n## Recent Changes (October 22, 2025)\n\n- Added **Intelligent Content-Based Sectioning**: AI analyzes PDF content to create semantically coherent sections based on topic changes and content flow\n- Implemented **Dual Sectioning Strategies**: Users can choose between AI-powered intelligent sectioning or manual fixed-page sectioning\n- Enhanced **Section Reasoning**: AI provides explanations for why each section was created\n- Improved **Error Handling**: Better error messages when DeepSeek API encounters issues\n- Increased **Maximum Page Limit**: Raised from 20 to 30 pages per section for manual mode\n\n## User Preferences\n\nPreferred communication style: Simple, everyday language.\n\n## System Architecture\n\n### Frontend Architecture\n\n**Decision**: Streamlit web framework  \n**Rationale**: Provides rapid development of data-centric applications with minimal frontend code. Streamlit's session state management handles processing workflows and maintains user context across interactions.\n\n**Key Components**:\n- Session state management for tracking processing status, JSON output, and output directories\n- File upload interface supporting both local files and URL-based downloads\n- Sidebar configuration panel for API keys and processing parameters\n\n### Backend Architecture\n\n**Decision**: Modular Python architecture with separation of concerns  \n**Rationale**: Each major functionality is isolated into dedicated modules for maintainability and testability.\n\n**Core Modules**:\n\n1. **PDFProcessor** (`pdf_processor.py`)\n   - Handles PDF structure analysis and page counting\n   - **Dual Sectioning Modes**:\n     - `create_optimal_sections()`: Creates sections based on min/max page parameters (manual mode)\n     - `create_intelligent_sections()`: Uses AI to analyze content and create semantically meaningful sections\n   - `extract_all_page_texts()`: Extracts text from all pages for content analysis\n   - Uses pypdf library for PDF manipulation\n   - Extracts sample text from initial pages for structure analysis\n\n2. **DeepSeekAnalyzer** (`deepseek_analyzer.py`)\n   - Integrates with DeepSeek AI API via OpenAI client interface\n   - **Content Analysis Functions**:\n     - `analyze_section_content()`: Generates metadata (title, description, keywords) for individual sections\n     - `suggest_content_based_sections()`: Analyzes entire PDF to suggest optimal section boundaries\n   - Implements intelligent section validation and fallback strategies\n   - Content length limiting (8000 characters for metadata, sampling for sectioning) for token management\n   - Handles edge cases like insufficient text content\n   - Uses Turkish language for metadata generation\n   - Provides reasoning for each suggested section boundary\n\n3. **Utils** (`utils.py`)\n   - PDF download functionality from URLs with validation\n   - Content-type verification and PDF magic number checking\n   - Temporary file management with unique UUID-based naming\n   - HTTP request handling with proper headers and timeout configurations\n\n### Data Processing Flow\n\n**Decision**: Sequential processing pipeline  \n**Rationale**: Ensures data integrity and allows for error handling at each stage.\n\n**Pipeline Stages**:\n1. PDF acquisition (upload or URL download)\n2. Structure analysis (page count, text extraction)\n3. Section creation based on page ranges\n4. AI-powered metadata generation per section\n5. JSON output generation with structured metadata\n\n### AI Integration\n\n**Decision**: DeepSeek API for content analysis  \n**Rationale**: Provides cost-effective, high-quality Turkish language support for metadata generation.\n\n**Integration Details**:\n- Uses OpenAI-compatible client interface\n- Custom base URL pointing to DeepSeek API\n- Structured prompt engineering for consistent metadata format\n- Token optimization through content truncation\n- Graceful degradation for sections with insufficient content\n\n### Error Handling Strategy\n\n**Decision**: Defensive programming with explicit error messages  \n**Rationale**: Provides clear feedback for debugging and user guidance.\n\n**Error Handling Patterns**:\n- URL validation and content-type checking before processing\n- PDF magic number verification for downloaded files\n- File size validation (minimum 1KB)\n- Try-catch blocks with descriptive error messages\n- Fallback metadata for empty or invalid sections\n\n### File Management\n\n**Decision**: Temporary file storage with UUID-based naming  \n**Rationale**: Prevents naming conflicts and automatic cleanup via OS temp directory management.\n\n**Implementation**:\n- Uses Python's tempfile module for secure temporary storage\n- UUID hex strings for unique file identification\n- No persistent storage requirement reduces infrastructure complexity\n\n## External Dependencies\n\n### AI Services\n- **DeepSeek API**: Primary AI service for content analysis and metadata generation\n  - OpenAI-compatible API interface\n  - Base URL: https://api.deepseek.com\n  - Requires API key authentication (default configured in environment)\n\n### Python Libraries\n- **Streamlit**: Web application framework for UI and interaction flow\n- **openai**: Client library for DeepSeek API integration\n- **pypdf**: PDF parsing and text extraction\n- **requests**: HTTP client for URL-based PDF downloads\n\n### External Integrations\n- PDF downloads from arbitrary URLs with User-Agent spoofing\n- HTTP request handling with 30-second timeout\n- Content validation through headers and magic number verification\n\n### Configuration\n- Environment variable support for `DEEPSEEK_API_KEY`\n- Hardcoded fallback API key for development convenience\n- Configurable section parameters (min/max pages per section)","size_bytes":6357},"app.py":{"content":"import streamlit as st\nimport os\nimport tempfile\nfrom pathlib import Path\nimport json\nimport shutil\nfrom pdf_processor import PDFProcessor\nfrom deepseek_analyzer import DeepSeekAnalyzer\nfrom utils import download_pdf_from_url, create_output_directories, create_pdf_filename\n\ndef main():\n    st.title(\"ğŸ“„ PDF RAG BÃ¶lÃ¼mlendirme AracÄ±\")\n    st.markdown(\"PDF dosyalarÄ±nÄ±zÄ± RAG iÃ§in optimize edilmiÅŸ bÃ¶lÃ¼mlere ayÄ±rÄ±n ve AI ile analiz edin.\")\n    \n    # Initialize session state\n    if 'processing_complete' not in st.session_state:\n        st.session_state.processing_complete = False\n    if 'json_output' not in st.session_state:\n        st.session_state.json_output = \"\"\n    if 'output_dir' not in st.session_state:\n        st.session_state.output_dir = \"\"\n    if 'sections' not in st.session_state:\n        st.session_state.sections = []\n    if 'analysis_complete' not in st.session_state:\n        st.session_state.analysis_complete = False\n    if 'pdf_path_temp' not in st.session_state:\n        st.session_state.pdf_path_temp = \"\"\n    if 'pdf_base_name' not in st.session_state:\n        st.session_state.pdf_base_name = \"\"\n    if 'metadata_list' not in st.session_state:\n        st.session_state.metadata_list = []\n    \n    # Sidebar for configuration\n    st.sidebar.header(\"âš™ï¸ Ayarlar\")\n    \n    # DeepSeek API Key\n    api_key = os.getenv(\"DEEPSEEK_API_KEY\", \"sk-8c15dc40c6b44cde9880f7a47b4be333\")\n    \n    # PDF source selection\n    st.header(\"1ï¸âƒ£ PDF KaynaÄŸÄ±nÄ± SeÃ§in\")\n    source_option = st.radio(\n        \"PDF kaynaÄŸÄ±nÄ±zÄ± seÃ§in:\",\n        [\"ğŸ’» Bilgisayardan dosya yÃ¼kle\", \"ğŸŒ URL'den indir\"]\n    )\n    \n    pdf_file = None\n    pdf_path = None\n    uploaded_file = None\n    \n    if source_option == \"ğŸ’» Bilgisayardan dosya yÃ¼kle\":\n        uploaded_file = st.file_uploader(\n            \"PDF dosyanÄ±zÄ± seÃ§in:\",\n            type=['pdf'],\n            help=\"RAG iÃ§in bÃ¶lÃ¼mlendirilecek PDF dosyanÄ±zÄ± yÃ¼kleyin\"\n        )\n        if uploaded_file is not None:\n            # Save uploaded file to temporary location\n            with tempfile.NamedTemporaryFile(delete=False, suffix='.pdf') as tmp_file:\n                tmp_file.write(uploaded_file.getbuffer())\n                pdf_path = tmp_file.name\n            st.success(f\"âœ… Dosya yÃ¼klendi: {uploaded_file.name}\")\n    \n    elif source_option == \"ğŸŒ URL'den indir\":\n        url_input = st.text_input(\n            \"PDF URL'sini girin:\",\n            placeholder=\"https://example.com/document.pdf\",\n            help=\"Ä°ndirilecek PDF dosyasÄ±nÄ±n URL'sini girin\"\n        )\n        \n        if url_input:\n            if st.button(\"ğŸ“¥ PDF'i Ä°ndir\"):\n                with st.spinner(\"PDF indiriliyor...\"):\n                    try:\n                        pdf_path = download_pdf_from_url(url_input)\n                        st.success(\"âœ… PDF baÅŸarÄ±yla indirildi!\")\n                    except Exception as e:\n                        st.error(f\"âŒ PDF indirme hatasÄ±: {str(e)}\")\n                        pdf_path = None\n    \n    # Processing section\n    if pdf_path:\n        st.header(\"2ï¸âƒ£ PDF Ä°ÅŸleme AyarlarÄ±\")\n        \n        # BÃ¶lÃ¼mleme stratejisi seÃ§imi\n        sectioning_mode = st.radio(\n            \"BÃ¶lÃ¼mleme Stratejisi:\",\n            [\"ğŸ¤– AkÄ±llÄ± BÃ¶lÃ¼mleme (AI bazlÄ±, iÃ§eriÄŸe gÃ¶re)\", \"ğŸ“ Manuel BÃ¶lÃ¼mleme (sabit sayfa aralÄ±ÄŸÄ±)\"],\n            help=\"AkÄ±llÄ± bÃ¶lÃ¼mleme: AI, PDF iÃ§eriÄŸini analiz ederek en mantÄ±klÄ± bÃ¶lÃ¼mleri oluÅŸturur. Manuel bÃ¶lÃ¼mleme: Sayfa sayÄ±sÄ±na gÃ¶re eÅŸit bÃ¶lÃ¼mler oluÅŸturur.\"\n        )\n        \n        min_pages_per_section = 1\n        max_pages_per_section = 30\n        \n        if sectioning_mode == \"ğŸ“ Manuel BÃ¶lÃ¼mleme (sabit sayfa aralÄ±ÄŸÄ±)\":\n            col1, col2 = st.columns(2)\n            with col1:\n                min_pages_per_section = st.number_input(\n                    \"Minimum sayfa/bÃ¶lÃ¼m:\",\n                    min_value=1,\n                    max_value=10,\n                    value=3,\n                    help=\"Her bÃ¶lÃ¼mde minimum sayfa sayÄ±sÄ±\"\n                )\n            \n            with col2:\n                max_pages_per_section = st.number_input(\n                    \"Maximum sayfa/bÃ¶lÃ¼m:\",\n                    min_value=2,\n                    max_value=30,\n                    value=10,\n                    help=\"Her bÃ¶lÃ¼mde maximum sayfa sayÄ±sÄ±\"\n                )\n        else:\n            st.info(\"ğŸ¤– AI, PDF iÃ§eriÄŸini analiz ederek en uygun bÃ¶lÃ¼mleme stratejisini belirleyecek. Bu iÅŸlem biraz daha uzun sÃ¼rebilir.\")\n            \n            # API key kontrolÃ¼\n            if not api_key or api_key == \"\":\n                st.warning(\"âš ï¸ AkÄ±llÄ± bÃ¶lÃ¼mleme iÃ§in DeepSeek API anahtarÄ± gereklidir. LÃ¼tfen Ã¶nce API anahtarÄ±nÄ±zÄ± girin.\")\n        \n        # Process PDF button\n        if st.button(\"ğŸš€ PDF'i Analiz Et ve BÃ¶lÃ¼mle\", type=\"primary\"):\n            if sectioning_mode == \"ğŸ“ Manuel BÃ¶lÃ¼mleme (sabit sayfa aralÄ±ÄŸÄ±)\" and min_pages_per_section >= max_pages_per_section:\n                st.error(\"âŒ Minimum sayfa sayÄ±sÄ±, maximum sayfa sayÄ±sÄ±ndan kÃ¼Ã§Ã¼k olmalÄ±dÄ±r!\")\n            else:\n                # PDF dosya adÄ±nÄ± kaydet\n                if source_option == \"ğŸ’» Bilgisayardan dosya yÃ¼kle\" and uploaded_file:\n                    st.session_state.pdf_base_name = Path(uploaded_file.name).stem\n                else:\n                    st.session_state.pdf_base_name = \"document\"\n                \n                analyze_and_prepare(pdf_path, api_key, sectioning_mode, min_pages_per_section, max_pages_per_section)\n    \n    # Analysis results section\n    if st.session_state.analysis_complete and not st.session_state.processing_complete:\n        st.header(\"2ï¸âƒ£ Analiz SonuÃ§larÄ± ve JSON Ã–nizleme\")\n        \n        # Display JSON output\n        st.subheader(\"ğŸ“Š OluÅŸturulacak BÃ¶lÃ¼mler (JSON)\")\n        st.text_area(\n            \"JSON Ã‡Ä±ktÄ±sÄ±:\",\n            value=st.session_state.json_output,\n            height=400,\n            help=\"PDF parÃ§alandÄ±ÄŸÄ±nda bu yapÄ±da bÃ¶lÃ¼mler oluÅŸturulacak\"\n        )\n        \n        # Split PDF button\n        st.divider()\n        col1, col2, col3 = st.columns([2, 2, 1])\n        with col2:\n            if st.button(\"ğŸ“„ PDF'leri ParÃ§ala ve Kaydet\", type=\"primary\", help=\"JSON'a gÃ¶re PDF'leri bÃ¶lÃ¼mlere ayÄ±rÄ±p kaydeder\"):\n                split_pdf_files()\n        with col3:\n            if st.button(\"ğŸ”„ Yeniden BaÅŸla\", help=\"Analizi iptal et ve baÅŸa dÃ¶n\"):\n                reset_and_cleanup()\n                st.rerun()\n    \n    # Results section\n    if st.session_state.processing_complete:\n        st.header(\"3ï¸âƒ£ Ä°ÅŸlem SonuÃ§larÄ±\")\n        \n        # Display JSON output\n        st.subheader(\"ğŸ“Š BÃ¶lÃ¼m Metadata (JSON)\")\n        st.text_area(\n            \"JSON Ã‡Ä±ktÄ±sÄ±:\",\n            value=st.session_state.json_output,\n            height=400,\n            help=\"OluÅŸturulan bÃ¶lÃ¼mler ve metadata bilgileri\"\n        )\n        \n        # Download JSON button\n        if st.session_state.json_output:\n            st.download_button(\n                label=\"ğŸ’¾ JSON'u Ä°ndir\",\n                data=st.session_state.json_output,\n                file_name=\"pdf_sections_metadata.json\",\n                mime=\"application/json\"\n            )\n        \n        # Show output directory\n        if st.session_state.output_dir:\n            st.info(f\"ğŸ“ BÃ¶lÃ¼mlenmiÅŸ PDF dosyalarÄ± ÅŸurada kaydedildi: `{st.session_state.output_dir}`\")\n        \n        # Reset button\n        st.divider()\n        col1, col2 = st.columns([3, 1])\n        with col2:\n            if st.button(\"ğŸ—‘ï¸ Verileri SÄ±fÄ±rla\", type=\"secondary\", help=\"TÃ¼m iÅŸlemi sÄ±fÄ±rlar, dosyalarÄ± siler ve uygulamayÄ± yeniden baÅŸlatÄ±r\"):\n                reset_and_cleanup()\n                st.rerun()\n\ndef analyze_and_prepare(pdf_path, api_key, sectioning_mode, min_pages, max_pages):\n    \"\"\"Analyze PDF and prepare metadata without splitting files\"\"\"\n    try:\n        # PDF yolunu kaydet\n        st.session_state.pdf_path_temp = pdf_path\n        # Create progress bar\n        progress_bar = st.progress(0)\n        status_text = st.empty()\n        \n        # Step 1: Initialize components\n        status_text.text(\"ğŸ”§ BileÅŸenler baÅŸlatÄ±lÄ±yor...\")\n        progress_bar.progress(10)\n        \n        processor = PDFProcessor()\n        analyzer = DeepSeekAnalyzer(api_key)\n        \n        # Step 2: Create output directories\n        status_text.text(\"ğŸ“ Ã‡Ä±ktÄ± klasÃ¶rleri oluÅŸturuluyor...\")\n        progress_bar.progress(20)\n        \n        output_dir = create_output_directories()\n        st.session_state.output_dir = output_dir\n        \n        # Step 3: Analyze PDF structure\n        status_text.text(\"ğŸ“– PDF yapÄ±sÄ± analiz ediliyor...\")\n        progress_bar.progress(30)\n        \n        pdf_info = processor.analyze_pdf_structure(pdf_path)\n        st.info(f\"ğŸ“„ PDF Bilgisi: {pdf_info['total_pages']} sayfa tespit edildi\")\n        \n        # Step 4: Create optimal sections\n        if sectioning_mode == \"ğŸ¤– AkÄ±llÄ± BÃ¶lÃ¼mleme (AI bazlÄ±, iÃ§eriÄŸe gÃ¶re)\":\n            status_text.text(\"ğŸ¤– AI ile iÃ§erik bazlÄ± bÃ¶lÃ¼mler oluÅŸturuluyor...\")\n            progress_bar.progress(40)\n            \n            try:\n                sections = processor.create_intelligent_sections(\n                    pdf_path, \n                    pdf_info['total_pages'], \n                    analyzer\n                )\n                \n                # BÃ¶lÃ¼m nedenlerini gÃ¶ster\n                st.success(f\"ğŸ¤– AI {len(sections)} anlamlÄ± bÃ¶lÃ¼m oluÅŸturdu\")\n                with st.expander(\"ğŸ“‹ BÃ¶lÃ¼mleme DetaylarÄ±\"):\n                    for i, section in enumerate(sections):\n                        st.write(f\"**BÃ¶lÃ¼m {i+1}:** Sayfa {section['start_page']}-{section['end_page']}\")\n                        if section.get('reason'):\n                            st.write(f\"   â””â”€ *{section['reason']}*\")\n            except Exception as e:\n                st.warning(f\"âš ï¸ AI bÃ¶lÃ¼mleme baÅŸarÄ±sÄ±z oldu: {str(e)}\")\n                st.info(\"ğŸ“ Otomatik olarak manuel bÃ¶lÃ¼mleme moduna geÃ§iliyor...\")\n                \n                # Fallback: Manuel bÃ¶lÃ¼mleme\n                sections = processor.create_optimal_sections(\n                    pdf_path, \n                    pdf_info['total_pages'], \n                    3,  # Default min pages\n                    10  # Default max pages\n                )\n        else:\n            status_text.text(\"âœ‚ï¸ Manuel bÃ¶lÃ¼mler oluÅŸturuluyor...\")\n            \n            sections = processor.create_optimal_sections(\n                pdf_path, \n                pdf_info['total_pages'], \n                min_pages, \n                max_pages\n            )\n        \n        # Session state'e sections'Ä± kaydet\n        st.session_state.sections = sections\n        \n        progress_bar.progress(50)\n        \n        if sectioning_mode != \"ğŸ¤– AkÄ±llÄ± BÃ¶lÃ¼mleme (AI bazlÄ±, iÃ§eriÄŸe gÃ¶re)\":\n            st.info(f\"ğŸ“ {len(sections)} bÃ¶lÃ¼m oluÅŸturuldu\")\n        \n        # Step 5: Analyze content and prepare metadata (WITHOUT creating PDF files)\n        status_text.text(\"ğŸ¤– AI ile iÃ§erik analiz ediliyor...\")\n        progress_bar.progress(70)\n        \n        metadata_list = []\n        \n        for i, section in enumerate(sections):\n            # Extract text for analysis\n            section_text = processor.extract_text_from_pages(\n                pdf_path, \n                section['start_page'], \n                section['end_page']\n            )\n            \n            # Analyze with DeepSeek\n            if section_text.strip():  # Only analyze if there's actual text\n                analysis = analyzer.analyze_section_content(section_text)\n                \n                # API hata kontrolÃ¼\n                if 'API Analiz HatasÄ±' in analysis.get('title', ''):\n                    st.warning(f\"âš ï¸ BÃ¶lÃ¼m {i + 1} iÃ§in AI analizi baÅŸarÄ±sÄ±z oldu. Hata: {analysis.get('description', '')}\")\n                \n                title = analysis.get('title', f'BÃ¶lÃ¼m {i + 1}')\n                \n                # Dosya adÄ±nÄ± oluÅŸtur (TÃ¼rkÃ§e karaktersiz)\n                output_filename = create_pdf_filename(\n                    st.session_state.pdf_base_name,\n                    i + 1,\n                    section['start_page'],\n                    section['end_page'],\n                    title\n                )\n                \n                metadata = {\n                    \"output_filename\": output_filename,\n                    \"start_page\": section['start_page'],\n                    \"end_page\": section['end_page'],\n                    \"title\": title,\n                    \"description\": analysis.get('description', 'Bu bÃ¶lÃ¼m iÃ§in aÃ§Ä±klama oluÅŸturulamadÄ±.'),\n                    \"keywords\": analysis.get('keywords', f'bÃ¶lÃ¼m_{i + 1}')\n                }\n            else:\n                # Fallback for sections with no extractable text\n                output_filename = create_pdf_filename(\n                    st.session_state.pdf_base_name,\n                    i + 1,\n                    section['start_page'],\n                    section['end_page'],\n                    \"\"\n                )\n                \n                metadata = {\n                    \"output_filename\": output_filename,\n                    \"start_page\": section['start_page'],\n                    \"end_page\": section['end_page'],\n                    \"title\": f\"BÃ¶lÃ¼m {i + 1}\",\n                    \"description\": \"Bu bÃ¶lÃ¼mde metin iÃ§eriÄŸi tespit edilemedi. GÃ¶rsel iÃ§erik veya tablo bulunuyor olabilir.\",\n                    \"keywords\": f\"bÃ¶lÃ¼m_{i + 1},gÃ¶rsel_iÃ§erik\"\n                }\n            \n            metadata_list.append(metadata)\n            \n            # Update progress\n            section_progress = 70 + (i + 1) / len(sections) * 20\n            progress_bar.progress(int(section_progress))\n            status_text.text(f\"ğŸ¤– BÃ¶lÃ¼m {i + 1}/{len(sections)} analiz edildi...\")\n        \n        # Save metadata list to session state\n        st.session_state.metadata_list = metadata_list\n        \n        # Step 6: Generate final JSON\n        status_text.text(\"ğŸ“„ JSON Ã§Ä±ktÄ±sÄ± oluÅŸturuluyor...\")\n        progress_bar.progress(95)\n        \n        final_json = {\n            \"pdf_sections\": metadata_list\n        }\n        \n        json_output = json.dumps(final_json, ensure_ascii=False, indent=2)\n        st.session_state.json_output = json_output\n        \n        # Complete\n        progress_bar.progress(100)\n        status_text.text(\"âœ… Analiz tamamlandÄ±!\")\n        st.session_state.analysis_complete = True\n        \n        st.success(f\"ğŸ‰ Analiz baÅŸarÄ±yla tamamlandÄ±! {len(sections)} bÃ¶lÃ¼m iÃ§in metadata oluÅŸturuldu.\")\n        st.info(\"ğŸ‘‡ AÅŸaÄŸÄ±da JSON Ã§Ä±ktÄ±sÄ±nÄ± inceleyebilir ve PDF'leri parÃ§alayabilirsiniz.\")\n        \n    except Exception as e:\n        st.error(f\"âŒ Ä°ÅŸlem sÄ±rasÄ±nda hata oluÅŸtu: {str(e)}\")\n        st.exception(e)\n\ndef split_pdf_files():\n    \"\"\"Split PDF files according to prepared metadata\"\"\"\n    try:\n        # Create progress bar\n        progress_bar = st.progress(0)\n        status_text = st.empty()\n        \n        # Get data from session state\n        pdf_path = st.session_state.pdf_path_temp\n        metadata_list = st.session_state.metadata_list\n        sections = st.session_state.sections\n        \n        # Step 1: Create output directories\n        status_text.text(\"ğŸ“ Ã‡Ä±ktÄ± klasÃ¶rleri oluÅŸturuluyor...\")\n        progress_bar.progress(10)\n        \n        output_dir = create_output_directories()\n        st.session_state.output_dir = output_dir\n        \n        # Step 2: Split PDF files\n        status_text.text(\"âœ‚ï¸ PDF dosyalarÄ± parÃ§alanÄ±yor...\")\n        progress_bar.progress(30)\n        \n        processor = PDFProcessor()\n        \n        for i, (section, metadata) in enumerate(zip(sections, metadata_list)):\n            # Create section PDF with the specified filename\n            output_path = Path(output_dir) / metadata['output_filename']\n            \n            # Create PDF using processor\n            with open(pdf_path, 'rb') as source_file:\n                import pypdf\n                reader = pypdf.PdfReader(source_file)\n                writer = pypdf.PdfWriter()\n                \n                # Add pages to writer\n                for page_num in range(section['start_page'] - 1, section['end_page']):\n                    if page_num < len(reader.pages):\n                        writer.add_page(reader.pages[page_num])\n                \n                # Save PDF\n                with open(output_path, 'wb') as output_file:\n                    writer.write(output_file)\n            \n            # Update progress\n            file_progress = 30 + (i + 1) / len(sections) * 60\n            progress_bar.progress(int(file_progress))\n            status_text.text(f\"âœ‚ï¸ BÃ¶lÃ¼m {i + 1}/{len(sections)} oluÅŸturuldu...\")\n        \n        # Step 3: Save JSON to file\n        status_text.text(\"ğŸ’¾ JSON dosyasÄ± kaydediliyor...\")\n        progress_bar.progress(95)\n        \n        json_path = Path(output_dir) / \"pdf_sections_metadata.json\"\n        with open(json_path, 'w', encoding='utf-8') as f:\n            f.write(st.session_state.json_output)\n        \n        # Complete\n        progress_bar.progress(100)\n        status_text.text(\"âœ… PDF parÃ§alama tamamlandÄ±!\")\n        st.session_state.processing_complete = True\n        st.session_state.analysis_complete = False  # Analiz bÃ¶lÃ¼mÃ¼nÃ¼ gizle\n        \n        st.success(f\"ğŸ‰ {len(sections)} PDF dosyasÄ± baÅŸarÄ±yla oluÅŸturuldu!\")\n        st.balloons()\n        \n    except Exception as e:\n        st.error(f\"âŒ PDF parÃ§alama sÄ±rasÄ±nda hata oluÅŸtu: {str(e)}\")\n        st.exception(e)\n\ndef reset_and_cleanup():\n    \"\"\"Reset all session state and clean up files\"\"\"\n    try:\n        # DosyalarÄ± ve klasÃ¶rÃ¼ sil\n        if st.session_state.output_dir and os.path.exists(st.session_state.output_dir):\n            shutil.rmtree(st.session_state.output_dir)\n            print(f\"KlasÃ¶r silindi: {st.session_state.output_dir}\")\n    except Exception as e:\n        print(f\"KlasÃ¶r silme hatasÄ±: {str(e)}\")\n    \n    # Session state'i temizle\n    st.session_state.processing_complete = False\n    st.session_state.json_output = \"\"\n    st.session_state.output_dir = \"\"\n    st.session_state.sections = []\n    st.session_state.analysis_complete = False\n    st.session_state.pdf_path_temp = \"\"\n    st.session_state.pdf_base_name = \"\"\n    st.session_state.metadata_list = []\n\nif __name__ == \"__main__\":\n    main()\n","size_bytes":18640},"deepseek_analyzer.py":{"content":"import openai\nimport json\nimport re\nfrom typing import Dict, Any\n\nclass DeepSeekAnalyzer:\n    \"\"\"DeepSeek AI ile PDF iÃ§erik analizi\"\"\"\n    \n    def __init__(self, api_key: str):\n        self.client = openai.OpenAI(\n            api_key=api_key,\n            base_url=\"https://api.deepseek.com\"\n        )\n    \n    def analyze_section_content(self, text_content: str) -> Dict[str, Any]:\n        \"\"\"PDF bÃ¶lÃ¼m iÃ§eriÄŸini analiz ederek metadata oluÅŸturur\"\"\"\n        \n        if not text_content or len(text_content.strip()) < 10:\n            return {\n                'title': 'Ä°Ã§erik Tespit Edilemedi',\n                'description': 'Bu bÃ¶lÃ¼mde yeterli metin iÃ§eriÄŸi bulunamadÄ±.',\n                'keywords': 'iÃ§erik_yok'\n            }\n        \n        try:\n            # Metin uzunluÄŸunu sÄ±nÄ±rla (token limiti iÃ§in)\n            max_chars = 8000\n            if len(text_content) > max_chars:\n                text_content = text_content[:max_chars] + \"...\"\n            \n            prompt = f\"\"\"\nAÅŸaÄŸÄ±daki PDF bÃ¶lÃ¼mÃ¼ iÃ§eriÄŸini analiz et ve RAG (Retrieval Augmented Generation) sisteminde kullanÄ±lmak Ã¼zere metadata oluÅŸtur.\n\nÄ°Ã‡ERÄ°K:\n{text_content}\n\nGÃ–REV:\nBu iÃ§erik iÃ§in aÅŸaÄŸÄ±daki bilgileri oluÅŸtur:\n\n1. BAÅLIK: Ä°Ã§eriÄŸin ana konusunu Ã¶zetleyen kÄ±sa ve aÃ§Ä±klayÄ±cÄ± baÅŸlÄ±k (maksimum 100 karakter)\n2. AÃ‡IKLAMA: Ä°Ã§eriÄŸin detaylÄ± aÃ§Ä±klamasÄ±, ne hakkÄ±nda olduÄŸu, hangi konularÄ± kapsadÄ±ÄŸÄ± (150-300 kelime)\n3. ANAHTAR KELÄ°MELER: RAG sisteminde arama iÃ§in kullanÄ±lacak anahtar kelimeler (virgÃ¼lle ayrÄ±lmÄ±ÅŸ, alt Ã§izgi kullan, maksimum 15 kelime)\n\nKURALLAR:\n- TÃ¼rkÃ§e karakter kullan\n- Anahtar kelimelerde boÅŸluk yerine alt Ã§izgi kullan (Ã¶rn: \"prim_borcu\")\n- Teknik terimler ve mevzuat referanslarÄ± Ã¶nemli\n- RAG sisteminde bulunabilirlik iÃ§in optimize et\n- Sadece verilen iÃ§eriÄŸe dayalÄ± bilgi ver\n\nÃ‡IKTI FORMATI (sadece JSON dÃ¶ndÃ¼r):\n{{\n    \"title\": \"BaÅŸlÄ±k buraya\",\n    \"description\": \"AÃ§Ä±klama buraya\",\n    \"keywords\": \"kelime1,kelime2,kelime3\"\n}}\n\"\"\"\n\n            response = self.client.chat.completions.create(\n                model=\"deepseek-chat\",\n                messages=[\n                    {\n                        \"role\": \"system\", \n                        \"content\": \"Sen bir PDF analiz uzmanÄ±sÄ±n. Verilen metinleri analiz ederek RAG sistemi iÃ§in optimal metadata oluÅŸturuyorsun. Sadece JSON formatÄ±nda yanÄ±t ver.\"\n                    },\n                    {\n                        \"role\": \"user\", \n                        \"content\": prompt\n                    }\n                ],\n                temperature=0.1,\n                max_tokens=1000\n            )\n            \n            # API yanÄ±tÄ±nÄ± al\n            result_text = response.choices[0].message.content\n            if not result_text:\n                raise ValueError(\"API'den boÅŸ yanÄ±t alÄ±ndÄ±\")\n            result_text = result_text.strip()\n            \n            # JSON'Ä± ayÄ±kla\n            json_match = re.search(r'\\{.*\\}', result_text, re.DOTALL)\n            if json_match:\n                result_json = json.loads(json_match.group())\n                \n                # SonuÃ§larÄ± temizle ve doÄŸrula\n                cleaned_result = self._clean_analysis_result(result_json)\n                return cleaned_result\n            else:\n                raise ValueError(\"API yanÄ±tÄ±nda JSON bulunamadÄ±\")\n                \n        except Exception as e:\n            error_msg = f\"DeepSeek analiz hatasÄ±: {str(e)}\"\n            print(error_msg)\n            # Hata durumunda fallback metadata\n            return {\n                'title': 'API Analiz HatasÄ±',\n                'description': f\"Bu bÃ¶lÃ¼mÃ¼n AI analizi yapÄ±lamadÄ±. Hata: {str(e)}. Ä°Ã§erik yaklaÅŸÄ±k {len(text_content)} karakter barÄ±ndÄ±rmaktadÄ±r.\",\n                'keywords': 'api_hatasÄ±,analiz_yapÄ±lamadÄ±'\n            }\n    \n    def _clean_analysis_result(self, result: Dict[str, Any]) -> Dict[str, Any]:\n        \"\"\"API sonucunu temizler ve doÄŸrular\"\"\"\n        cleaned = {}\n        \n        # BaÅŸlÄ±ÄŸÄ± temizle\n        title = result.get('title', '').strip()\n        if not title or len(title) < 5:\n            title = \"PDF BÃ¶lÃ¼mÃ¼\"\n        elif len(title) > 150:\n            title = title[:147] + \"...\"\n        cleaned['title'] = title\n        \n        # AÃ§Ä±klamayÄ± temizle\n        description = result.get('description', '').strip()\n        if not description or len(description) < 20:\n            description = \"Bu PDF bÃ¶lÃ¼mÃ¼ Ã¶nemli bilgiler iÃ§ermektedir.\"\n        elif len(description) > 1000:\n            description = description[:997] + \"...\"\n        cleaned['description'] = description\n        \n        # Anahtar kelimeleri temizle\n        keywords = result.get('keywords', '').strip()\n        if not keywords:\n            keywords = \"pdf_bÃ¶lÃ¼mÃ¼,dokÃ¼man\"\n        else:\n            # Anahtar kelimeleri iÅŸle\n            keyword_list = [kw.strip().lower().replace(' ', '_') for kw in keywords.split(',')]\n            keyword_list = [kw for kw in keyword_list if kw and len(kw) > 1][:15]  # Maksimum 15 kelime\n            keywords = ','.join(keyword_list)\n        \n        cleaned['keywords'] = keywords\n        \n        return cleaned\n    \n    def _create_fallback_metadata(self, text_content: str) -> Dict[str, Any]:\n        \"\"\"Hata durumunda basit metadata oluÅŸturur\"\"\"\n        # Ä°Ã§eriÄŸin ilk birkaÃ§ kelimesinden baÅŸlÄ±k oluÅŸtur\n        words = text_content.strip().split()[:10]\n        title = ' '.join(words) if words else \"PDF BÃ¶lÃ¼mÃ¼\"\n        if len(title) > 100:\n            title = title[:97] + \"...\"\n        \n        # Basit anahtar kelimeler oluÅŸtur\n        common_words = []\n        for word in text_content.split():\n            word = word.strip().lower()\n            if len(word) > 3 and word.isalpha():\n                word = word.replace(' ', '_')\n                if word not in common_words:\n                    common_words.append(word)\n                if len(common_words) >= 5:\n                    break\n        \n        keywords = ','.join(common_words) if common_words else \"pdf_iÃ§erik,dokÃ¼man\"\n        \n        return {\n            'title': title,\n            'description': f\"Bu bÃ¶lÃ¼m yaklaÅŸÄ±k {len(text_content)} karakter iÃ§erik barÄ±ndÄ±rmaktadÄ±r. Ä°Ã§eriÄŸin detaylÄ± analizi yapÄ±lamadÄ±.\",\n            'keywords': keywords\n        }\n    \n    def suggest_content_based_sections(self, page_texts: list, total_pages: int) -> list:\n        \"\"\"Ä°Ã§erik bazlÄ± optimal bÃ¶lÃ¼mleme Ã¶nerileri oluÅŸturur\"\"\"\n        try:\n            # Her 3 sayfada bir Ã¶rnek al (Ã§ok uzun olmamasÄ± iÃ§in)\n            sample_pages = []\n            for i in range(0, total_pages, max(1, total_pages // 10)):  # Maksimum 10 Ã¶rnek\n                if i < len(page_texts):\n                    sample_pages.append({\n                        'page': i + 1,\n                        'text': page_texts[i][:500]  # Her sayfadan ilk 500 karakter\n                    })\n            \n            # Ã–rnekleri birleÅŸtir\n            samples_text = \"\\n\\n\".join([f\"SAYFA {s['page']}: {s['text']}\" for s in sample_pages])\n            \n            prompt = f\"\"\"\nBu bir {total_pages} sayfalÄ±k PDF dokÃ¼manÄ±nÄ±n iÃ§erik Ã¶rnekleridir. RAG (Retrieval Augmented Generation) sistemi iÃ§in bu PDF'i optimal bÃ¶lÃ¼mlere ayÄ±rmalÄ±yÄ±m.\n\nÄ°Ã‡ERÄ°K Ã–RNEKLERÄ°:\n{samples_text}\n\nGÃ–REV:\nBu PDF'i anlam bÃ¼tÃ¼nlÃ¼ÄŸÃ¼ olan, RAG iÃ§in optimal bÃ¶lÃ¼mlere ayÄ±r. Her bÃ¶lÃ¼m:\n- Tek bir ana konuyu veya iliÅŸkili konularÄ± kapsamalÄ±\n- Ã‡ok kÃ¼Ã§Ã¼k (1-2 sayfa) veya Ã§ok bÃ¼yÃ¼k (30+ sayfa) olmamalÄ±\n- MantÄ±klÄ± bir baÅŸlangÄ±Ã§ ve bitiÅŸ noktasÄ± olmalÄ±\n\nÃ‡IKTI FORMATI (sadece JSON array dÃ¶ndÃ¼r):\n[\n  {{\"start_page\": 1, \"end_page\": 5, \"reason\": \"GiriÅŸ ve genel kavramlar\"}},\n  {{\"start_page\": 6, \"end_page\": 12, \"reason\": \"Ana konu 1\"}},\n  {{\"start_page\": 13, \"end_page\": {total_pages}, \"reason\": \"Ana konu 2 ve sonuÃ§\"}}\n]\n\nÃ–NEMLÄ°:\n- TÃ¼m sayfalar kapsanmalÄ± (1'den {total_pages}'a kadar)\n- BÃ¶lÃ¼mler Ã¶rtÃ¼ÅŸmemeli\n- Sayfa numaralarÄ± ardÄ±ÅŸÄ±k olmalÄ±\n- Maksimum 15 bÃ¶lÃ¼m oluÅŸtur\n\"\"\"\n\n            response = self.client.chat.completions.create(\n                model=\"deepseek-chat\",\n                messages=[\n                    {\n                        \"role\": \"system\",\n                        \"content\": \"Sen bir dokÃ¼man analiz uzmanÄ±sÄ±n. PDF iÃ§eriklerini analiz ederek RAG sistemleri iÃ§in optimal bÃ¶lÃ¼mleme Ã¶nerileri sunuyorsun.\"\n                    },\n                    {\n                        \"role\": \"user\",\n                        \"content\": prompt\n                    }\n                ],\n                temperature=0.3,\n                max_tokens=2000\n            )\n            \n            result_text = response.choices[0].message.content\n            if not result_text:\n                raise ValueError(\"API'den boÅŸ yanÄ±t alÄ±ndÄ±\")\n            \n            # JSON array'i ayÄ±kla\n            json_match = re.search(r'\\[.*\\]', result_text.strip(), re.DOTALL)\n            if json_match:\n                sections = json.loads(json_match.group())\n                \n                # BÃ¶lÃ¼mleri doÄŸrula ve dÃ¼zelt\n                validated_sections = self._validate_sections(sections, total_pages)\n                return validated_sections\n            else:\n                raise ValueError(\"API yanÄ±tÄ±nda JSON array bulunamadÄ±\")\n                \n        except Exception as e:\n            print(f\"Ä°Ã§erik bazlÄ± bÃ¶lÃ¼mleme hatasÄ±: {str(e)}\")\n            # Fallback: Basit eÅŸit bÃ¶lÃ¼mleme\n            return self._create_fallback_sections(total_pages)\n    \n    def _validate_sections(self, sections: list, total_pages: int) -> list:\n        \"\"\"BÃ¶lÃ¼mlerin geÃ§erliliÄŸini kontrol eder ve dÃ¼zeltir\"\"\"\n        if not sections:\n            return self._create_fallback_sections(total_pages)\n        \n        validated = []\n        expected_start = 1\n        \n        for section in sections:\n            start = section.get('start_page', expected_start)\n            end = section.get('end_page', start)\n            \n            # SÄ±nÄ±rlarÄ± dÃ¼zelt\n            start = max(expected_start, min(start, total_pages))\n            end = max(start, min(end, total_pages))\n            \n            if start <= total_pages:\n                validated.append({\n                    'start_page': start,\n                    'end_page': end,\n                    'reason': section.get('reason', '')\n                })\n                expected_start = end + 1\n        \n        # EÄŸer tÃ¼m sayfalar kapsanmadÄ±ysa, son bÃ¶lÃ¼mÃ¼ geniÅŸlet\n        if validated and validated[-1]['end_page'] < total_pages:\n            validated[-1]['end_page'] = total_pages\n        \n        # HiÃ§ bÃ¶lÃ¼m yoksa fallback kullan\n        if not validated:\n            return self._create_fallback_sections(total_pages)\n        \n        return validated\n    \n    def _create_fallback_sections(self, total_pages: int) -> list:\n        \"\"\"Basit eÅŸit bÃ¶lÃ¼mleme oluÅŸturur\"\"\"\n        sections = []\n        pages_per_section = max(5, total_pages // 5)  # YaklaÅŸÄ±k 5 bÃ¶lÃ¼m\n        \n        current_page = 1\n        while current_page <= total_pages:\n            end_page = min(current_page + pages_per_section - 1, total_pages)\n            sections.append({\n                'start_page': current_page,\n                'end_page': end_page,\n                'reason': f'BÃ¶lÃ¼m {len(sections) + 1}'\n            })\n            current_page = end_page + 1\n        \n        return sections\n    \n    def test_connection(self) -> bool:\n        \"\"\"API baÄŸlantÄ±sÄ±nÄ± test eder\"\"\"\n        try:\n            response = self.client.chat.completions.create(\n                model=\"deepseek-chat\",\n                messages=[{\"role\": \"user\", \"content\": \"Merhaba, baÄŸlantÄ± testi.\"}],\n                max_tokens=10,\n                temperature=0\n            )\n            return True\n        except Exception as e:\n            print(f\"DeepSeek baÄŸlantÄ± hatasÄ±: {str(e)}\")\n            return False\n","size_bytes":11968},"create_simple_pdf.py":{"content":"from pypdf import PdfWriter\n\n# Create a simple blank PDF for testing\nwriter = PdfWriter()\n\n# Add a single blank page\nfrom pypdf.generic import PageObject\npage = PageObject.create_blank_page(width=612, height=792)  # Letter size\n\n# Add some pages\nfor i in range(5):\n    writer.add_page(page)\n\n# Save the PDF\nwith open(\"test_document.pdf\", \"wb\") as output_file:\n    writer.write(output_file)\n\nprint(\"Created test PDF: test_document.pdf with 5 pages\")\n","size_bytes":449},"utils.py":{"content":"import requests\nimport tempfile\nimport os\nfrom pathlib import Path\nfrom urllib.parse import urlparse\nimport uuid\n\ndef download_pdf_from_url(url: str, max_retries: int = 3) -> str:\n    \"\"\"URL'den PDF indirir ve geÃ§ici klasÃ¶re kaydeder\"\"\"\n    import time\n    \n    last_error = None\n    \n    for attempt in range(max_retries):\n        try:\n            # URL'yi doÄŸrula\n            parsed_url = urlparse(url)\n            if not parsed_url.scheme or not parsed_url.netloc:\n                raise ValueError(\"GeÃ§ersiz URL formatÄ±\")\n            \n            # HTTP headers\n            headers = {\n                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n                'Accept': 'application/pdf,*/*'\n            }\n            \n            # PDF'i indir (daha uzun timeout ve allow_redirects)\n            response = requests.get(url, headers=headers, timeout=60, allow_redirects=True)\n            response.raise_for_status()\n        \n            # Content-Type kontrolÃ¼\n            content_type = response.headers.get('content-type', '').lower()\n            if 'pdf' not in content_type and not url.lower().endswith('.pdf'):\n                # Ä°Ã§eriÄŸi kontrol et (PDF magic number)\n                if not response.content.startswith(b'%PDF-'):\n                    raise ValueError(\"Ä°ndirilen dosya PDF formatÄ±nda deÄŸil\")\n            \n            # GeÃ§ici dosya oluÅŸtur\n            temp_dir = tempfile.gettempdir()\n            filename = f\"downloaded_pdf_{uuid.uuid4().hex[:8]}.pdf\"\n            temp_path = os.path.join(temp_dir, filename)\n            \n            # DosyayÄ± kaydet\n            with open(temp_path, 'wb') as f:\n                f.write(response.content)\n            \n            # Dosya boyutunu kontrol et\n            file_size = os.path.getsize(temp_path)\n            if file_size < 1024:  # 1KB'dan kÃ¼Ã§Ã¼kse\n                os.remove(temp_path)\n                raise ValueError(\"Ä°ndirilen dosya Ã§ok kÃ¼Ã§Ã¼k (PDF olmayabilir)\")\n            \n            return temp_path\n            \n        except (requests.exceptions.RequestException, ValueError) as e:\n            last_error = e\n            if attempt < max_retries - 1:\n                # Son deneme deÄŸilse, kÄ±sa bir sÃ¼re bekle ve tekrar dene\n                wait_time = (attempt + 1) * 2  # 2, 4, 6 saniye\n                time.sleep(wait_time)\n            continue\n        except Exception as e:\n            # DiÄŸer hatalar iÃ§in hemen Ã§Ä±k\n            raise Exception(f\"PDF indirme hatasÄ±: {str(e)}\")\n    \n    # TÃ¼m denemeler baÅŸarÄ±sÄ±z olduysa\n    if last_error:\n        if isinstance(last_error, requests.exceptions.RequestException):\n            raise Exception(f\"URL'den indirme hatasÄ± ({max_retries} deneme sonucu): {str(last_error)}\")\n        else:\n            raise Exception(f\"PDF indirme hatasÄ± ({max_retries} deneme sonucu): {str(last_error)}\")\n    \n    raise Exception(\"PDF indirilemedi (bilinmeyen hata)\")\n\ndef create_output_directories() -> str:\n    \"\"\"Ã‡Ä±ktÄ± klasÃ¶rlerini oluÅŸturur\"\"\"\n    try:\n        # Ana Ã§Ä±ktÄ± klasÃ¶rÃ¼\n        base_dir = Path.cwd() / \"pdf_output\"\n        \n        # Benzersiz alt klasÃ¶r (timestamp ile)\n        import datetime\n        timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n        output_dir = base_dir / f\"sections_{timestamp}\"\n        \n        # KlasÃ¶rleri oluÅŸtur\n        output_dir.mkdir(parents=True, exist_ok=True)\n        \n        return str(output_dir)\n        \n    except Exception as e:\n        raise Exception(f\"Ã‡Ä±ktÄ± klasÃ¶rÃ¼ oluÅŸturma hatasÄ±: {str(e)}\")\n\ndef validate_pdf_file(file_path: str) -> bool:\n    \"\"\"PDF dosyasÄ±nÄ±n geÃ§erliliÄŸini kontrol eder\"\"\"\n    try:\n        if not os.path.exists(file_path):\n            return False\n        \n        # Dosya uzantÄ±sÄ± kontrolÃ¼\n        if not file_path.lower().endswith('.pdf'):\n            return False\n        \n        # Dosya boyutu kontrolÃ¼\n        file_size = os.path.getsize(file_path)\n        if file_size < 1024:  # 1KB'dan kÃ¼Ã§Ã¼k\n            return False\n        \n        # PDF magic number kontrolÃ¼\n        with open(file_path, 'rb') as f:\n            header = f.read(8)\n            if not header.startswith(b'%PDF-'):\n                return False\n        \n        return True\n        \n    except Exception:\n        return False\n\ndef cleanup_temp_files(file_paths: list):\n    \"\"\"GeÃ§ici dosyalarÄ± temizler\"\"\"\n    for file_path in file_paths:\n        try:\n            if os.path.exists(file_path) and 'temp' in file_path:\n                os.remove(file_path)\n        except Exception:\n            pass  # Sessizce devam et\n\ndef format_file_size(size_bytes: int) -> str:\n    \"\"\"Dosya boyutunu okunabilir formatta dÃ¶ndÃ¼rÃ¼r\"\"\"\n    if size_bytes < 1024:\n        return f\"{size_bytes} B\"\n    elif size_bytes < 1024 * 1024:\n        return f\"{size_bytes / 1024:.1f} KB\"\n    elif size_bytes < 1024 * 1024 * 1024:\n        return f\"{size_bytes / (1024 * 1024):.1f} MB\"\n    else:\n        return f\"{size_bytes / (1024 * 1024 * 1024):.1f} GB\"\n\ndef extract_filename_from_url(url: str) -> str:\n    \"\"\"URL'den dosya adÄ±nÄ± Ã§Ä±karÄ±r\"\"\"\n    try:\n        parsed_url = urlparse(url)\n        filename = os.path.basename(parsed_url.path)\n        if not filename or not filename.endswith('.pdf'):\n            filename = f\"document_{uuid.uuid4().hex[:8]}.pdf\"\n        return filename\n    except Exception:\n        return f\"document_{uuid.uuid4().hex[:8]}.pdf\"\n\ndef transliterate_turkish(text: str) -> str:\n    \"\"\"TÃ¼rkÃ§e karakterleri Ä°ngilizce karakterlere Ã§evirir\"\"\"\n    turkish_map = {\n        'Ã§': 'c', 'Ã‡': 'C',\n        'ÄŸ': 'g', 'Ä': 'G',\n        'Ä±': 'i', 'Ä°': 'I',\n        'Ã¶': 'o', 'Ã–': 'O',\n        'ÅŸ': 's', 'Å': 'S',\n        'Ã¼': 'u', 'Ãœ': 'U'\n    }\n    \n    result = text\n    for turkish_char, english_char in turkish_map.items():\n        result = result.replace(turkish_char, english_char)\n    \n    return result\n\ndef sanitize_filename(filename: str) -> str:\n    \"\"\"Dosya adÄ±nÄ± gÃ¼venli hale getirir\"\"\"\n    import re\n    # TÃ¼rkÃ§e karakterleri koru, sadece gÃ¼venli olmayan karakterleri temizle\n    filename = re.sub(r'[<>:\"/\\\\|?*]', '_', filename)\n    filename = filename.strip('. ')\n    if not filename:\n        filename = f\"file_{uuid.uuid4().hex[:8]}\"\n    return filename\n\ndef create_pdf_filename(base_name: str, section_num: int, start_page: int, end_page: int, title: str = \"\") -> str:\n    \"\"\"PDF bÃ¶lÃ¼m dosya adÄ± oluÅŸturur (TÃ¼rkÃ§e karaktersiz)\"\"\"\n    import re\n    \n    # BaÅŸlÄ±ÄŸÄ± kullan, yoksa base_name kullan\n    if title and title != \"Ä°Ã§erik Tespit Edilemedi\" and title != \"API Analiz HatasÄ±\":\n        # BaÅŸlÄ±ktan dosya adÄ± oluÅŸtur\n        filename = transliterate_turkish(title)\n        # Ã–zel karakterleri temizle\n        filename = re.sub(r'[^\\w\\s-]', '', filename)\n        # BoÅŸluklarÄ± alt Ã§izgiye Ã§evir\n        filename = re.sub(r'\\s+', '_', filename)\n        # Ã‡ok uzunsa kÄ±salt\n        if len(filename) > 80:\n            filename = filename[:80]\n    else:\n        # Base name'den oluÅŸtur\n        filename = transliterate_turkish(base_name)\n        filename = re.sub(r'[^\\w\\s-]', '', filename)\n        filename = re.sub(r'\\s+', '_', filename)\n        filename = f\"{filename}_Bolum_{section_num}\"\n    \n    # Sayfa numaralarÄ±nÄ± ekle\n    filename = f\"{section_num:02d}_{filename}_{start_page}-{end_page}.pdf\"\n    \n    # Temizle ve gÃ¼venli hale getir\n    filename = filename.strip('_')\n    \n    return filename\n","size_bytes":7505},"pyproject.toml":{"content":"[project]\nname = \"repl-nix-workspace\"\nversion = \"0.1.0\"\ndescription = \"Add your description here\"\nrequires-python = \">=3.11\"\ndependencies = [\n    \"openai>=2.6.0\",\n    \"pypdf>=6.1.3\",\n    \"requests>=2.32.5\",\n    \"streamlit>=1.50.0\",\n]\n","size_bytes":234}},"version":2}